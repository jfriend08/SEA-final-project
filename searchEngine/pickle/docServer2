(dp0
I128
(dp1
S'url'
p2
S'http://en.wikipedia.org/wiki/Semantic compression'
p3
sS'text'
p4
VIn [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build \u000aa textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. \u000aAs a result, the same ideas can be represented using a smaller set of words.\u000a\u000aSemantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document \u000acannot be reconstructed in a reverse process.\u000a\u000a==Semantic compression by generalization==\u000aSemantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:\u000a#	determining cumulated term frequencies to identify target lexicon,\u000a#	replacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.<ref>[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010</ref>\u000a\u000aStep 1 requires assembling word frequencies and \u000ainformation on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, \u000aa cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:\u000a<math>cum f(k_{i}) = f(k_{i}) + \u005csum_{j} cum f(k_{j})</math> where <math>k_{i}</math> is a hypernym of <math>k_{j}</math>.\u000aThen, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.\u000a\u000aIn the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence \u000aof a less frequent hyponym as its hypernym in output text.\u000a\u000a;Example\u000a\u000aThe below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.\u000a\u000a<blockquote>They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' \u000ain very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects \u000a'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the \u000a'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of \u000a'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.</blockquote>\u000a\u000aThe procedure outputs the following text:\u000a\u000a<blockquote>They are both '''facility''' building '''insect''', but '''insect''' and honey '''insects''' '''arrange''' their '''biological groups''' \u000ain very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects \u000a'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the \u000a'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of \u000a'''organic process''', and there are '''impinging difference of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.</blockquote>\u000a\u000a==Implicit semantic compression==\u000aA natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)\u000a.<ref>[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],\u000aCOLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982</ref>\u000a\u000a==Applications and advantages==\u000aIn the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less \u000a[[computational complexity]] and a positive influence on efficiency. \u000a\u000aSemantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).<ref>[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010</ref> This is due to more precise descriptors (reduced effect of language diversity \u2013 limited language redundancy, a step towards a controlled dictionary).\u000a\u000aAs in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).\u000a\u000a==See also==\u000a* [[Text simplification]]\u000a* [[Lexical substitution]]\u000a* [[Information theory]]\u000a* [[Quantities of information]]\u000a\u000a==References==\u000a<references/>\u000a\u000a==External links==\u000a* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a[[Category:Quantitative linguistics]]\u000a[[Category:Computational linguistics]]
p5
sS'title'
p6
S'Semantic compression'
p7
ssI2
(dp8
g2
S'http://en.wikipedia.org/wiki/Document retrieval'
p9
sg4
V'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.\u000a\u000aDocument retrieval is sometimes referred to as, or as a branch of, '''Text Retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.\u000a\u000a==Description==\u000aDocument retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.\u000a\u000aA document retrieval system has two main tasks:\u000a# Find relevant documents to user queries\u000a# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].\u000a\u000aInternet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.\u000a\u000a==Variations==\u000aThere are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.\u000a\u000a===Form based===\u000aForm based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.\u000a\u000a===Content based===\u000aThe content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.\u000a\u000aA ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.\u000a\u000a==Example: PubMed==\u000aThe [[PubMed]]<ref>{{cite journal |author=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319\u201323 |year=2001 |pmid=11825203 |pmc=2243528 }}\u000a</ref> form interface features the "related articles" search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.<ref>{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}</ref><ref>{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}</ref>\u000a\u000a== See also ==\u000a\u000a* [[Compound term processing]]\u000a* [[Document classification]]\u000a* [[Enterprise search]]\u000a* [[Full text search]]\u000a* [[Information retrieval]]\u000a* [[Latent semantic indexing]]\u000a* [[Search engine]]\u000a\u000a== References ==\u000a\u000a<references/>\u000a\u000a==Further reading==\u000a* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems (TOIS)|volume=2|issue=4|year=1984|pages=267\u2013288|doi=10.1145/2275.357411}}\u000a* {{cite journal|author=Justin Zobel, Alistair Moffat and Kotagiri Ramamohanarao|title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems (TODS)|volume=23|issue=4|year=1998|pages= 453\u2013490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}\u000a* {{cite journal|author=Ben Carterette and Fazli Can|title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613\u2013633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}\u000a\u000a== External links ==\u000a* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Electronic documents]]\u000a[[Category:Substring indices]]\u000a\u000a[[zh:\u6587\u672c\u4fe1\u606f\u68c0\u7d22]]
p10
sg6
S'Document retrieval'
p11
ssI131
(dp12
g2
S'http://en.wikipedia.org/wiki/Natural language user interface'
p13
sg4
V'''Natural Language User Interfaces''' (LUI or NLUI) are a type of [[User interface|computer human interface]] where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.\u000a\u000aIn [[interface design]] natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to [[natural language understanding|understanding]] wide varieties of ambiguous input.<ref>Hill, I. (1983). "Natural language versus computer language." In M. Sime and M. Coombs (Eds.) Designing for Human-Computer Communication. Academic Press.</ref>\u000aNatural language interfaces are an active area of study in the field of [[natural language processing]] and [[computational linguistics]]. An intuitive general Natural language interface is one of the active goals of the [[Semantic Web]].\u000a\u000aText interfaces are 'natural' to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional [[keyword search]] engine could be described as a 'shallow' Natural language user interface.\u000a\u000a==Overview==\u000aA natural language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which [[United States|U.S.]] state has the highest [[income tax]]?', conventional search engines ignore the question and instead search on the [[index term|keywords]] 'state', 'income' and 'tax'. Natural language search, on the other hand, attempts to use natural language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.\u000a\u000a==History==\u000a\u000aPrototype Nl interfaces had already appeared in the late sixties and early seventies.<ref name="edin">Natural Language Interfaces to Databases \u2013 An Introduction,\u000aI. Androutsopoulos,\u000aG.D. Ritchie,\u000aP. Thanisch,\u000aDepartment of Artificial Intelligence, University of Edinburgh</ref>\u000a\u000a*[[SHRDLU]], a natural language interface that manipulates blocks in a virtual "blocks world"\u000a*''Lunar'', a natural language interface to a database containing chemical analyses of Apollo-11 moon rocks by [http://parsecraft.com/ William A. Woods].\u000a*''Chat-80'' transformed English questions into [[Prolog]] expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem5.htm Chat-80 demo]</ref>\u000a*[[ELIZA]], written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem4.htm ELIZA demo]</ref>\u000a* ''Janus'' is also one of the few systems to support temporal questions.\u000a* ''Intellect'' from [[Trinzic]] (formed by the merger of AICorp and Aion).\u000a* BBN\u2019s ''Parlance'' built on experience from the development of the ''Rus''  and ''Irus'' systems.\u000a* [[IBM]] ''Languageaccess''\u000a* [[Q&A (software)|Q&A]] from [[Symantec]].\u000a* ''Datatalker'' from Natural Language Inc.\u000a* ''Loqui''  from [[Bim]].\u000a* ''English Wizard'' from [[Linguistic Technology Corporation]].\u000a* ''iAskWeb'' from Anserity Inc. fully implemented in [[Prolog]] was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001<ref>{{cite book | last = Galitsky\u000a | first = Boris\u000a | title = Natural Language Question Answering: technique of semantic headers\u000a | publisher = Advance Knowledge International\u000a | date = 2003\u000a | location = Adelaide, Australia\u000a | url = http://www.amazon.com/Natural-Language-Question-Answering-system/dp/0868039799\u000a | isbn = 0868039799\u000a  }}</ref>\u000a\u000a==Challenges==\u000aNatural language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the [[AI winter]] of the 1970s and 80s.\u000a\u000aA [http://arxiv.org/abs/cmp-lg/9503016 1995 paper] titled 'Natural Language Interfaces to Databases \u2013 An Introduction', describes some challenges:<ref name="edin"/>\u000a* ''Modifier attachment''\u000aThe request \u201cList all employees in the company with a driving licence\u201d is ambiguous unless you know companies can't have drivers licences.\u000a\u000a* ''Conjunction and disjunction''\u000a\u201cList all applicants who live in California and Arizona\u201d is ambiguous unless you know that a person can't live in two places at once.\u000a* ''[[Anaphora resolution]]''\u000a- resolve what a user means by 'he', 'she' or 'it', in a self-referential query.\u000a\u000aOther goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market.\u000a\u000aFinally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.\u000a\u000a==Uses and applications==\u000a\u000aThe natural language interface gives rise to technology used for many different applications. \u000a\u000aSome of the main uses are:\u000a\u000a* ''Dictation'', is the most common use for [[automated speech recognition]] (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.\u000a* ''Command and control'', ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like "Open Netscape" and "Start a new xterm" will do just that.\u000a* ''Telephony'', some PBX/[[Voice Mail]] systems allow callers to speak commands instead of pressing buttons to send specific tones.\u000a* ''Wearables'', because inputs are limited for wearable devices, speaking is a natural possibility.\u000a* ''Medical, disabilities'', many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's speech to text.\u000a* ''Embedded applications'', some new cellular phones include C&C speech recognition that allow utterances such as "call home". This may be a major factor in the future of automatic speech recognition and [[Linux]].\u000a\u000aBelow are named and defined some of the applications that use natural language recognition, and so have integrated utilities listed above.\u000a\u000a===Ubiquity===\u000a{{main|Ubiquity (Firefox)}}\u000aUbiquity, an [[add-on (Mozilla)|add-on]] for [[Mozilla Firefox]], is a collection of quick and easy natural-language-derived commands that act as [[mashup (web application hybrid)|mashups]] of web services, thus allowing users to get information and relate it to current and other webpages.\u000a\u000a===Wolfram Alpha===\u000a{{main|Wolfram Alpha}}\u000aWolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] would.<ref>{{cite news |url=http://www.guardian.co.uk/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=2009-03-09 |work=[[The Guardian]] |accessdate=2009-03-09}}</ref> It was announced in March 2009 by [[Stephen Wolfram]], and was released to the public on May 15, 2009.<ref name="launch date">{{cite web|url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |publisher=Wolfram Alpha Blog |date=2009-05-08 |accessdate=2009-10-20}}</ref>\u000a\u000a===Siri===\u000a{{main|Siri (software)}}\u000aSiri is a [[personal assistant]] application for the operating system [[iOS]]. The application uses [[natural language processing]] to answer questions and make recommendations. The iPhone app is the first public product by its makers, who are focused on [[artificial intelligence]] applications.\u000a\u000aSiri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.<ref>[http://www.apple.com/iphone/features/siri.html Siri webpage]</ref>\u000a\u000a===Others===\u000a* [[Anboto Group]] provides Web customer service and e-commerce technology based on semantics and natural language processing. The main offer of [http://www.anbotogroup.com/en/index.php Anboto Group] are the virtual sales agent and intelligent chat.\u000a* [[Ask.com]] - The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.\u000a* [[Braina]]<ref>[http://www.brainasoft.com/braina/ Braina]</ref> - Braina is a natural language interface for [[Windows OS]] that allows to type or speak English language sentences to perform a certain action or find information.\u000a* [http://www.cmantik.com/ CMANTIK] - CMANTIK is a semantic information search engine which is trying to answer user's questions by looking up relevant information in Wikipedia and some news sources.\u000a* C-Phrase<ref>[http://code.google.com/p/c-phrase/ C-Phrase]</ref> - is a web-based natural language front end to relational databases. C-Phrase runs under Linux, connects with PostgreSQL databases via ODBC and supports both select queries and updates. Currently there is only support for English. C-Phrase is hosted on [[Google Code]] site.\u000a* [http://devtools.korzh.com/easyquery/ EasyQuery] - is a component library (for .NET framework first of all) which allows you to implement natural language query builder in your application. Works both with relational databases or ORM solutions like Entity Framework.\u000a[[File:GNOME Do Classic.png|thumb|Screenshot of GNOME DO classic interface.]]\u000a* [[GNOME Do]] - Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).<ref>Ubuntu 10.04 Add/Remove Applications description for GNOME Do</ref>\u000a* [[Invention Machine]] Goldfire - powered by a semantic research engine that has the capability to transform unstructured documents from various electronic sources into an index that, when searched, delivers answers to research questions. Goldfire\u2019s Natural Language query interface enables the user to put a question in a free text format, which would be the same format as if the question were given to another person. And, once knowledge has been retrieved, Goldfire presents the results in a way that makes their meaning readily apparent.\u000a* [[hakia]] - hakia is an Internet search engine. The company has invented an alternative new infrastructure to indexing that uses SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics.\u000a* [[Lexxe]] - Lexxe is an Internet search engine that uses natural language processing for queries (semantic search). Searches can be made with keywords, phrases, and questions, such as "How old is Wikipedia?" When it comes to facts, Lexxe is quite effective, though needs much improvement in natural language analysis in the area of facts and in other areas.\u000a* [http://www.mnemoo.com/ Mnemoo] - Mnemoo is an answer engine that aimed to directly answer questions posed in plain text (Natural Language), which is accomplished using a database of facts and an inference engine.\u000a* [http://www.naturaldateandtime.com/ Natural Date and Time] - Natural language date and time zone engine. It allows you to ask questions about time, daylight saving information and to do time zone conversions via plain English questions such as 'What is the time in São Paulo when it is 6pm on the 2nd of June in Detroit'.\u000a* [http://www.linguasys.com/web_production/server-item/NLUI%20Server NLUI Server] - an enterprise-oriented multilingual application server by LinguaSys for natural language user interface scripts, supporting English, Spanish, Portuguese, German, Japanese, Chinese, Pashto, Thai, Russian, Vietnamese, Malay, with Arabic, French, and more languages in development.\u000a* [[Pikimal]] - Pikimal uses natural language tied to user preference to make search recommendations by template.\u000a* [[Powerset (company)|Powerset]] \u2014 On May 11, 2008, the company unveiled a tool for searching a fixed subset of [[Wikipedia]] using conversational phrases rather than keywords.<ref>{{cite news |url=http://bits.blogs.nytimes.com/2008/05/12/powerset-debuts-with-search-of-wikipedia/ |title=Powerset Debuts With Search of Wikipedia |publisher=The New York Times |first=Miguel |last=Helft |date=May 12, 2008}}</ref> On July 1, 2008, it was purchased by [[Microsoft]].<ref>{{cite web |url=http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archiveurl=http://web.archive.org/web/20090225064356/http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archivedate=February 25, 2009 |title=Microsoft to Acquire Powerset |publisher=Powerset Blog |first=Mark |last=Johnson |date=July 1, 2008}}</ref>\u000a* [[Q-go]] - The Q-go technology provides relevant answers to users in response to queries on a company\u2019s internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by [[RightNow Technologies]] in 2011\u000a* [[START (MIT project)]] - [http://start.csail.mit.edu/ START], Web-based question answering system. Unlike information retrieval systems such as search engines, START aims to supply users with "just the right information," instead of merely providing a list of hits. Currently, the system can answer millions of English questions about places, movies, people and dictionary definitions.\u000a* [http://swingly.com/ Swingly] - Swingly is an answer engine designed to find exact answers to factual questions. Just ask a question in plain English - and Swingly will find you the answer (or answers) you're looking for (according to their site).\u000a* [[Yebol]] - Yebol is a vertical "decision" search engine that had developed a knowledge-based, semantic search platform. Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually 'learns', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.<ref>Humphries, Matthew. [http://www.geek.com/articles/news/yebolcom-steps-into-the-search-market-20090731/ "Yebol.com steps into the search market"] ''Geek.com''. 31 July 2009.</ref>\u000a\u000a==See also==\u000a*[[Natural language programming]]\u000a**[[xTalk]], a family of English-like programming languages\u000a*[[Chatterbot]], a computer program that simulates human conversations\u000a*[[Noisy text]]\u000a*[[Question answering]]\u000a*[[Selection-based search]]\u000a*[[Semantic search]]\u000a*[[Semantic Web]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{Internet search}}\u000a{{Computable knowledge}}\u000a\u000a{{DEFAULTSORT:Natural language user interface}}\u000a[[Category:User interfaces]]\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]
p14
sg6
S'Natural language user interface'
p15
ssI5
(dp16
g2
S'http://en.wikipedia.org/wiki/Category:String similarity measures'
p17
sg4
S'{{Cat main|String metrics}}\n\n[[Category:Algorithms on strings|Similarity]]\n[[Category:Information retrieval]]\n[[Category:Metric geometry]]\n[[Category:Information theory]]\n[[Category:String (computer science)]]'
p18
sg6
S'Category:String similarity measures'
p19
ssI134
(dp20
g2
S'http://en.wikipedia.org/wiki/Category:Vector space model'
p21
sg4
S'[[Category:Information retrieval]]'
p22
sg6
S'Category:Vector space model'
p23
ssI8
(dp24
g2
S'http://en.wikipedia.org/wiki/National Centre for Text Mining'
p25
sg4
VThe '''National Centre for Text Mining''' (NaCTeM)\u000a<ref name="ariadne">{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}</ref> is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community. \u000a\u000aThe [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest - examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].\u000a\u000aThe Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[information extraction]], [[natural language processing]] and parallel and distributed data mining systems in biomedical and clinical applications.\u000a\u000a==Services==\u000a[http://www.nactem.ac.uk/software/termine/ '''TerMine'''] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them. <ref name="multi-word">{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117\u2013132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}</ref> \u000a\u000a[http://www.nactem.ac.uk/software/acromine/ '''AcroMine'''] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[Disambiguation|disambiguates]] them.<ref name="pmid17050571">{{cite journal| author=Okazaki N, Ananiadou S| title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 3089\u201395 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&tool=sumsearch.org/cite&retmode=ref&cmd=prlinks&id=17050571  }} </ref>\u000a\u000a[http://www-tsujii.is.s.u-tokyo.ac.jp/medie/ '''Medie'''] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts.\u000a\u000a[http://refine1-nactem.mc.man.ac.uk/facta/ ''' Facta+'''] is a MEDLINE search engine for finding associations between biomedical concepts.<ref name="pmid18772154">{{cite journal| author=Tsuruoka Y, Tsujii J, Ananiadou S| title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 2559\u201360 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }} </ref>\u000a\u000a[http://www.nactem.ac.uk/software/kleio/ '''KLEIO'''] is a faceted semantic information retrieval system based on MEDLINE.\u000a\u000a[https://www-tsujii.is.s.u-tokyo.ac.jp/info-pubmed/ '''Info-PubMed'''] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].\u000a\u000a==Resources==\u000a\u000a[http://www.nactem.ac.uk/biolexicon/ '''BioLexicon'''] a large-scale terminological resource for the biomedical domain\u000a\u000a[http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?page=GENIA+corpus '''GENIA'''] a collection of reference materials for the development of biomedical text mining systems\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* http://www.nactem.ac.uk\u000a\u000a[[Category:Computational linguistics]]\u000a[[Category:Computer science organizations]]\u000a[[Category:Information retrieval]]\u000a[[Category:Linguistics organizations]]\u000a[[Category:School of Computer Science, University of Manchester]]
p26
sg6
S'National Centre for Text Mining'
p27
ssI137
(dp28
g2
S'http://en.wikipedia.org/wiki/PolySpot'
p29
sg4
S"{{Infobox company\n| name = PolySpot\n| logo = [[File:PolySpot-Logo.jpg|300px]]\n| type = [[Privately held company|Private]]\n| foundation = [[Paris]] (2001)\n| location = [[Paris]], [[London]]\n| key_people = Guy Mounier, CEO\n| industry = [[Information technology]] <br/> [[Unified Information Access]] <br/> [[Search Engine]]\n| products = PolySpot Infowarehouse<br/>PolySpot Information At Work<br/>PolySpot Enterprise Search\n| slogan = Open Search Solutions\n| homepage = [http://www.polyspot.com/en/ www.polyspot.com]\n}}\n'''PolySpot''' is a subsidiary of CustomerMatrix, an [[enterprise search]] [[ISV|software company]].\n\nCreated in 2001, PolySpot has its headquarters in Paris, France. It also has offices in the United Kingdom.\n\nIn 2011, PolySpot raised EUR 2.5m from [[Newfund]].<ref>{{cite web|url=http://finance.yahoo.com/news/PolySpot-Raises-2-Million-prnews-4194799492.html| title=Yahoo News: PolySpot Raises 2 Million}}</ref> and True Global Ventures.\n\nIn 2013, CustomerMatrix (US) acquired PolySpot. In December 2013, PolySpot reduced its shareholders equity from 507,209 EUR to 206,120 EUR.\n\n== Functionalities ==\n\nPolySpot's infrastructure provide a unified information access to all data, through which users can instantly interact with all available information resources, both inside and outside the company, and regardless of whether or how the data are structured.\n\nPolySpot's indexing capabilities are based on the [[Apache Software Foundation|Apache]] [[Lucene]] [[Solr]] Java-based open-source projects.\n\n==References==\n{{Portal|Software}}\n{{Reflist|colwidth=30em}}\n\n==External links==\n*[http://www.polyspot.com/en/ Website]\n*[http://www.customermatrix.com/ CustomerMatrix Website]\n*[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=38687243 Company profile in BusinessWeek]\n*[http://www.arnoldit.com/search-wizards-speak/polyspot-2.html PolySpot in ArnoldIT]\n*[http://arnoldit.com/wordpress/2010/01/13/polyspot-lands-crdit-agricole-sa/ PolySpot lands Credit Agricole in ArnoldIT]\n\n{{DEFAULTSORT:Polyspot}}\n[[Category:Searching]]\n[[Category:Search engine software|*Enterprise search vendors]]\n[[Category:Internet search engines]]\n[[Category:Information retrieval]]"
p30
sg6
S'PolySpot'
p31
ssI11
(dp32
g2
S'http://en.wikipedia.org/wiki/ChemRefer'
p33
sg4
S'{{Orphan|date=February 2009}}\n{{Infobox website\n| name = Chemrefer\n| logo = [[Image:Chemrefer.png]]\n| screenshot = \n| caption = \n| url = http://www.chemrefer.com\n| commercial = Yes\n| type = [[Search engine]]\n| language = English\n| registration = Not Applicable\n| owner = ChemRefer Limited\n| author = William James Griffiths\n| launch date = 2006\n| current status = Offline\n| revenue = \n}}\n\'\'\'ChemRefer\'\'\' is a service that allows searching of freely-available and full-text chemical and pharmaceutical literature that is published by authoritative sources.<ref>{{citation|journal=Science Articles |title= Science News Forum|publisher= SciScoop |date=May 19, 2006|url= http://www.sciscoop.com/story/2006/5/19/95844/6293}}</ref>\n\nFeatures include basic and advanced search options, [[mouseover]] detailed view, an integrated chemical structure drawing and search tool, downloadable [[toolbar]], customized [[RSS]] feeds, and newsletter.\n\nChemRefer is primarily of use to readers who do not have subscriptions for accessing restricted chemical literature, and to publishers who offer either [[Open access (publishing)|open access]] or [[hybrid open access journal]]s and seek to attract further subscriptions by publicly releasing part of their archive.\n\n==See also==\n*[[Google Scholar]]\n*[[Windows Live Academic]]\n*[[BASE (search engine)|BASE]]\n*[[PubMed]]\n\n==References==\n{{reflist}}\n\n==External links==\n===Recommendations & reviews===\n*[http://www.rowland.harvard.edu/resources/library/lnn_archive/031706.php Cited as an "Internet Site of the Week"] by the library of the [[Rowland Institute for Science]] at [[Harvard University]]\n*[http://infoweb.nrl.navy.mil/index.cfm?i=156 Recommended in the list of chemical literature databases] by the library of the [[United States Naval Research Laboratory]]\n*[http://www.mta.ca/library/subject_chemistry.html Recommended in the list of chemical literature databases] by the library of [[Mount Allison University]]\n*[http://depth-first.com/articles/2007/01/15/chemrefer-free-direct-access-to-the-primary-literature Review of ChemRefer] at Depth-First chemoinformatics magazine\n*[http://recherche-technologie.wallonie.be/fr/particulier/menu/revue-athena/l-annuaire-de-liens/internet/moteurs-de-recherche/www-chemrefer-com.html?PROFIL=PART Recommended in the list of chemical literature databases] by the Technology Research Portal, Belgium\n*[http://www.certh.gr/0E9BF53C.en.aspx Recommended in the list of chemical literature databases] by the Centre for Research and Technology, Thessaloniki\n\n===Background===\n*[http://www.reactivereports.com/56/56_0.html Interview with William James Griffiths] at Reactive Reports chemistry magazine\n*[http://www.earlham.edu/~peters/fos/overview.htm Open access overview] by Professor Peter Suber, Earlham College\n\n[[Category:Scholarly search services]]\n[[Category:Chemistry literature]]\n[[Category:Information retrieval]]\n[[Category:Open access projects]]\n\n{{searchengine-website-stub}}'
p34
sg6
S'ChemRefer'
p35
ssI140
(dp36
g2
Vhttp://en.wikipedia.org/wiki/Sørensen\u2013Dice coefficient
p37
sg4
VThe '''Sørensen\u2013Dice index''', also known by other names (see Names, below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald Sørensen]]<ref>{{cite journal |last=Sørensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=1\u201334 |doi= }}</ref> and [[Lee Raymond Dice]],<ref>{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297\u2013302 |doi=10.2307/1932409 }}</ref> who published in 1948 and 1945 respectively.\u000a\u000a==Name==\u000aThe index is known by several other names, usually '''Sørensen index''' or '''Dice's coefficient'''. Both names also see "similarity coefficient", "index", and other such variations. Common alternate spellings for Sørensen are Sorenson, Soerenson index and Sörenson index, and all three can also be seen with the \u2013sen ending.\u000a\u000aOther names include:\u000a*[[Jan Czekanowski|Czekanowski]]'s binary (non-quantitative) index<ref name ="gallagher"/>\u000a\u000a==Quantitative version==\u000aThe expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:\u000a* Quantitative Sørensen\u2013Dice index<ref name ="gallagher"/>\u000a* Quantitative Sørensen index<ref name ="gallagher"/>\u000a* Quantitative Dice index<ref name ="gallagher"/>\u000a* [[Bray Curtis dissimilarity|Bray-Curtis similarity]] (1 minus the ''Bray-Curtis dissimilarity'')<ref name ="gallagher"/>\u000a* [[Jan Czekanowski|Czekanowski]]'s quantitative index<ref name ="gallagher"/>\u000a* Steinhaus index<ref name ="gallagher"/>\u000a* [[E. C. Pielou|Pielou]]'s percentage similarity<ref name ="gallagher"/>\u000a* 1 minus the [[Hellinger distance]]<ref>{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1948 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326\u2013349 |doi=10.2307/1942268 }}</ref>\u000a\u000a==Formula==\u000aSørensen's original formula was intended to be applied to presence/absence data, and is\u000a\u000a:<math> QS = \u005cfrac{2C}{A + B} = \u005cfrac{2 |A \u005ccap B|}{|A| + |B|}</math>\u000a\u000awhere ''A'' and ''B'' are the number of species in samples A and B, respectively, and ''C'' is the number of species shared by the two samples; QS is the quotient of similarity and ranges between 0 and 1.<ref>http://www.sekj.org/PDF/anbf40/anbf40-415.pdf</ref>\u000a\u000aIt can be viewed as a similarity measure over sets:\u000a\u000a:<math>s = \u005cfrac{2 | X \u005ccap Y |}{| X | + | Y |} </math>\u000a\u000aSimilarly to Jaccard, the set operations can be expressed in terms of vector operations over binary vectors ''A'' and ''B'':\u000a\u000a<math>s_v = \u005cfrac{2 | A \u005ccdot B |}{| A |^2 + | B |^2} </math>\u000a\u000awhich gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.\u000a\u000aFor sets ''X'' and ''Y'' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :<ref>{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979\u000a|title=Information Retrieval\u000a|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}</ref>\u000a\u000aWhen taken as a string similarity measure, the coefficient may be calculated for two strings, ''x'' and ''y'' using [[bigram]]s as follows:<ref>{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003\u000a|title=Cognates Can Improve Statistical Translation Models\u000a|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics\u000a|pages=46\u201348 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}</ref>\u000a\u000a:<math>s = \u005cfrac{2 n_t}{n_x + n_y}</math>\u000a\u000awhere ''n''<sub>''t''</sub> is the number of character bigrams found in both strings, ''n''<sub>''x''</sub> is the number of bigrams in string ''x'' and ''n''<sub>''y''</sub> is the number of bigrams in string ''y''. For example, to calculate the similarity between:\u000a\u000a:<code>night</code>\u000a:<code>nacht</code>\u000a\u000aWe would find the set of bigrams in each word:\u000a:{<code>ni</code>,<code>ig</code>,<code>gh</code>,<code>ht</code>}\u000a:{<code>na</code>,<code>ac</code>,<code>ch</code>,<code>ht</code>}\u000a\u000aEach set has four elements, and the intersection of these two sets has only one element: <code>ht</code>.\u000a\u000aInserting these numbers into the formula, we calculate, ''s''&nbsp;=&nbsp;(2&nbsp;·&nbsp;1)&nbsp;/&nbsp;(4&nbsp;+&nbsp;4)&nbsp;=&nbsp;0.25.\u000a\u000a==Difference from Jaccard ==\u000aThis coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn't satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.<ref name ="gallagher"/>\u000a\u000aThe function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function\u000a\u000a:<math>d = 1 -  \u005cfrac{2 | X \u005ccap Y |}{| X | + | Y |} </math>\u000a\u000ais not a proper distance metric as it does not possess the property of [[triangle inequality]].<ref name ="gallagher">Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&rep=rep1&type=pdf COMPAH Documentation], University of Massachusetts, Boston</ref> The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of ''any'' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.\u000a\u000a==Applications==\u000aThe Sørensen\u2013Dice coefficient is mainly useful for ecological community data (e.g. Looman & Campbell, 1960<ref>[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409\u2013416.]</ref>). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s<ref>[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123\u2013131.]</ref>). As compared to [[Euclidean distance]], Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.<ref>McCune, Bruce & Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.</ref>\u000a\u000a==See also==\u000a* [[Correlation]]\u000a* [[Czekanowski similarity index]]\u000a* [[Jaccard index]]\u000a* [[Hamming distance]]\u000a* [[Horn\u2019s index]]\u000a* [[Hurlbert\u2019s index]]\u000a* [[Kulczy\u0144ski similarity index]]\u000a* [[Pianka's index]]\u000a* [[MacArthur and Levin's index]]\u000a* [[Mantel test]]\u000a* [[Morisita's overlap index]]\u000a* [[Most frequent k characters]]\u000a* [[Overlap coefficient]]\u000a* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])\u000a* [[Simplified Morisita\u2019s index]]\u000a* [[Tversky index]]\u000a* [[Universal adaptive strategy theory (UAST)]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a{{Wikibooks|Algorithm implementation|Strings/Dice's coefficient|Dice's coefficient}}\u000a* Open Source [https://github.com/rockymadden/stringmetric/blob/master/core/src/main/scala/com/rockymadden/stringmetric/similarity/DiceSorensenMetric.scala Dice / Sorensen] [[Scala programming language|Scala]] implementation as part of the larger [http://rockymadden.com/stringmetric/ stringmetric project]\u000a\u000a{{DEFAULTSORT:Sorensen-Dice coefficient}}\u000a[[Category:Information retrieval]]\u000a[[Category:String similarity measures]]\u000a[[Category:Measure theory]]
p38
sg6
VSørensen\u2013Dice coefficient
p39
ssI14
(dp40
g2
S'http://en.wikipedia.org/wiki/Web search query'
p41
sg4
VA '''web search query''' is a query that a user enters into a web [[search engine]] to satisfy his or her [[information needs]]. Web search queries are distinctive in that they are often plain text or [[hypertext]] with optional search-directives (such as "and"/"or" with "-" to exclude). They vary greatly from standard [[query language]]s, which are governed by strict syntax rules as [[command language]]s with keyword or positional [[Parameter (computer science)|parameters]].\u000a\u000a== Types ==\u000aThere are three broad categories that cover most web search queries: informational, navigational, and transactional. These are often called "do, know, go."<ref>{{cite web|last=Gibbons|first=Kevin|title=Do, Know, Go: How to Create Content at Each Stage of the Buying Cycle|url=http://searchenginewatch.com/article/2235624/Do-Know-Go-How-to-Create-Content-at-Each-Stage-of-the-Buying-Cycle|publisher=Search Engine Watch|accessdate=24 May 2014}}</ref>\u000a\u000a* '''Informational queries''' \u2013 Queries that cover a broad topic (e.g., ''colorado'' or ''trucks'') for which there may be thousands of relevant results.\u000a\u000a* '''Navigational queries''' \u2013 Queries that seek a single website or web page of a single entity (e.g., ''youtube'' or ''delta air lines'').\u000a\u000a* '''Transactional queries''' \u2013 Queries that reflect the intent of the user to perform a particular action, like purchasing a car or downloading a screen saver.\u000a\u000aSearch engines often support a fourth type of query that is used far less frequently:\u000a\u000a* '''Connectivity queries''' \u2013 Queries that report on the connectivity of the indexed [[web graph]] (e.g., Which links point to this [[Uniform Resource Locator|URL]]?, and How many pages are indexed from this [[domain name]]?).<ref>{{cite web|last=Moore|first=Ross|title=Connectivity servers|url=http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html|publisher=Cambridge University Press|accessdate=24 May 2014}}</ref>\u000a\u000a== Characteristics ==\u000a\u000aMost commercial web search engines do not disclose their search logs, so information about what users are searching for on the Web is difficult to come by.<ref>Dawn Kawamoto and Elinor Mills (2006), [http://news.cnet.com/AOL-apologizes-for-release-of-user-search-data/2100-1030_3-6102793.html AOL apologizes for release of user search data]</ref> Nevertheless, a study in 2001<ref>{{cite journal|author = Amanda Spink, Dietmar Wolfram, Major B. J. Jansen, Tefko Saracevic | year = 2001 | title = Searching the web: The public and their queries | journal = Journal of the American Society for Information Science and Technology | volume = 52 | issue = 3 | pages = 226\u2013234 | doi = 10.1002/1097-4571(2000)9999:9999<::AID-ASI1591>3.3.CO;2-I }}</ref> analyzed the queries from the [[Excite]] search engine showed some interesting characteristics of web search:\u000a\u000a* The average length of a search query was 2.4 terms. \u000a* About half of the users entered a single query while a little less than a third of users entered three or more unique queries. \u000a* Close to half of the users examined only the first one or two pages of results (10 results per page).\u000a* Less than 5% of users used advanced search features (e.g., [[boolean operators]] like AND, OR, and NOT).\u000a* The top four most frequently used terms were , '' (empty search), and, of, ''and'' sex.\u000a\u000aA study of the same Excite query logs revealed that 19% of the queries contained a geographic term (e.g., place names, zip codes, geographic features, etc.).<ref>{{cite conference | author = Mark Sanderson and Janet Kohler | year = 2004 | title = Analyzing geographic queries | booktitle = Proceedings of the Workshop on Geographic Information (SIGIR '04) | url =http://supremacyseo.com/analyzing-geographic-queries }}</ref>\u000a\u000aA 2005 study of Yahoo's query logs revealed 33% of the queries from the same user were repeat queries and that 87% of the time the user would click on the same result.<ref>{{cite conference | author = Jaime Teevan, Eytan Adar, Rosie Jones, Michael Potts | year = 2005 | title = History repeats itself: Repeat Queries in Yahoo's query logs | booktitle = Proceedings of the 29th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR '06) | pages = 703\u2013704 | url =http://www.csail.mit.edu/~teevan/work/publications/posters/sigir06.pdf | doi=10.1145/1148170.1148326 }}</ref> This suggests that many users use repeat queries to revisit or re-find information. This analysis is confirmed by a Bing search engine blog post telling about 30% queries are navigational queries <ref>http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx</ref>\u000a\u000aIn addition, much research has shown that query term frequency distributions conform to the [[power law]], or ''long tail'' distribution curves. That is, a small portion of the terms observed in a large query log (e.g. > 100 million queries) are used most often, while the remaining terms are used less often individually.<ref name="baezayates1">{{cite journal | author = Ricardo Baeza-Yates | year = 2005 | title = Applications of Web Query Mining | booktitle = Lecture Notes in Computer Science | pages = 7\u201322 | volume = 3408 | publisher = Springer Berlin / Heidelberg | url = http://www.springerlink.com/content/kpphaktugag5mbv0/ | ISBN = 978-3-540-25295-5}}</ref> This example of the [[Pareto principle]] (or ''80\u201320 rule'') allows search engines to employ [[optimization techniques]] such as index or [[Partition (database)|database partitioning]], [[web cache|caching]] and pre-fetching.\u000a\u000aBut in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.<ref>{{cite journal | author = Mona Taghavi, Ahmed Patel, Nikita Schmidt, Christopher Wills, Yiqi Tew | year = 2011 | title = An analysis of web proxy logs with query distribution pattern approach for search engines | booktitle = Journal of Computer Standards & Interfaces | pages = 162\u2013170 | volume = 34 | issue = 1 |publisher = Elsevier  | url = http://www.sciencedirect.com/science/article/pii/S0920548911000808 | doi=10.1016/j.csi.2011.07.001}}</ref> Google has implemented the [[Google_Hummingbird|hummingbird]] update in August 2013 to handle longer search queries since more searches are conversational (ie "where is the nearest coffee shop?").<ref>{{cite web|last=Sullivan|first=Danny|title=FAQ: All About The New Google \u201cHummingbird\u201d Algorithm|url=http://searchengineland.com/google-hummingbird-172816|publisher=Search Engine Land|accessdate=24 May 2014}}</ref> \u000aFor longer queries, [[Natural language processing]] helps, since parse trees of queries can be matched with that of answers and their snippets.<ref>{{vcite journal |author=Galitsky B|title=Machine learning of syntactic parse trees for search and classification of text|journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=3 |date=2013 |pages=153-172|doi=10.1016/j.engappai.2012.09.017}}</ref> For multi-sentence queries where keywords statistics and [[Tf\u2013idf]] is not very helpful, [[Parse thicket]] technique comes into play to structurally represent complex questions and answers.<ref>{{vcite journal |author=Galitsky B, Ilvovsky D, Kuznetsov SO, Strok F|title=Finding Maximal Common Sub-parse Thickets\u000afor Multi-sentence Search |journal=Lecture Notes In Artificial Intelligence |volume = 8323 |date=2013 |http://www.aclweb.org/anthology/R13-1037\u000a}}</ref>\u000a\u000a== Structured queries ==\u000aWith search engines that support Boolean operators and parentheses, a technique traditionally used by librarians can be applied. A user who is looking for documents that cover several topics or ''facets'' may want to describe each of them by a [[logical disjunction|disjunction]] of characteristic words, such as <code>vehicles OR cars OR automobiles</code>. A ''faceted query'' is a [[logical conjunction|conjunction]] of such facets; e.g. a query such as <code>(electronic OR computerized OR DRE) AND (voting OR elections OR election OR balloting OR electoral)</code> is likely to find documents about electronic voting even if they omit one of the words "electronic" and "voting", or even both.<ref>{{Cite web\u000a|url=http://eprints.eemcs.utwente.nl/6918/01/TR-CTIT-06-57.pdf\u000a|title=Exploiting Query Structure and Document Structure to Improve Document Retrieval Effectiveness\u000a|author=Vojkan Mihajlovi\u0107, Djoerd Hiemstra, Henk Ernst Blok, Peter M.G. Apers\u000a|postscript=<!--None-->}}</ref>\u000a\u000a== See also ==\u000a* [[Information retrieval]]\u000a* [[Web search engine]]\u000a* [[Web query classification]]\u000a* [[Taxonomy for search engines]]\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a{{Internet search}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search]]
p42
sg6
S'Web search query'
p43
ssI143
(dp44
g2
S'http://en.wikipedia.org/wiki/Artificial Solutions'
p45
sg4
V{{Infobox company\u000a|name= Artificial Solutions\u000a|logo=[[Image:Artificial Solutions Logo.png]]\u000a|type=[[Private company]]\u000a|foundation=(2001)\u000a|founder=Johan Åhlund, Johan Gustavsson and Michael Söderström \u000a|location=[[Stockholm]], [[Sweden]]\u000a|locations=Offices worldwide with R&D centers in [[Barcelona]], [[Hamburg]], [[London]] and [[Stockholm]] \u000a|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], \u000a|products= Teneo platform\u000a|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]\u000a}}\u000a\u000a'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.<ref>{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}</ref> The company's natural language solutions have been deployed in a wide range of industries including finance,<ref>{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref> telecoms,<ref>{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtjänst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}</ref> the public sector,<ref>{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tjänst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}</ref> retail<ref>{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}</ref> and travel.<ref>{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}</ref>\u000a\u000a==History==\u000aArtificial Solutions was founded in Stockholm in 2001 by friends Johan Åhlund, Johan Gustavsson and Michael Söderström to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though Åhlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.<ref>{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=Löjlig affärside vinstlott för Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}</ref>\u000a\u000aThe company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.<ref>{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}</ref> Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.<ref>{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}</ref>\u000a\u000aIn 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.<ref>{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}</ref>\u000a[[Elbot]], Artificial Solutions\u2019 test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.<ref>[[Loebner Prize]]</ref><ref>{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &#124; England &#124; Berkshire &#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}</ref>\u000a\u000aWith a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.<ref>{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}</ref><ref>{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=Så effektiv er Ikeas chat-robot: Har været på 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}</ref>\u000aIn 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.<ref>{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}</ref>\u000aA new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.<ref>{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}</ref>\u000a\u000aIn February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].<ref>{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}</ref>\u000a\u000a==References==\u000a{{Reflist|30em}}\u000a\u000a==External links==\u000a*[http://www.hello-indigo.com Indigo]\u000a*[http://www.elbot.com Elbot]\u000a\u000a[[Category:Natural language processing software]]\u000a[[Category:Intelligent software assistants]]\u000a[[Category:User interfaces]]\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Natural language processing]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]
p46
sg6
S'Artificial Solutions'
p47
ssI17
(dp48
g2
S'http://en.wikipedia.org/wiki/Category:Electronic documents'
p49
sg4
S'[[Category:Documents]]\n[[Category:Digital media]]\n[[Category:Information retrieval]]\n[[Category:Electronic publishing]]'
p50
sg6
S'Category:Electronic documents'
p51
ssI146
(dp52
g2
S'http://en.wikipedia.org/wiki/Tag (metadata)'
p53
sg4
V{{hatnote|Not to be confused with [[Markup language]] or [[HTML element]] tags.}}\u000a[[File:Web 2.0 Map.svg|thumb|right|250px|A [[tag cloud]] with terms related to [[Web 2.0]]]]\u000a\u000aIn [[information system]]s, a '''tag''' is a non-hierarchical [[index term|keyword or term]] assigned to a piece of information (such as an [[Bookmark (World Wide Web)|Internet bookmark]], digital image, or [[computer file]]). This kind of [[metadata]] helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system.\u000a\u000aTagging was popularized by websites associated with [[Web 2.0]] and is an important feature of many Web 2.0 services. It is now also part of some desktop software.\u000a\u000a==History==\u000a\u000aLabeling and tagging are carried out to perform functions such as aiding in [[Classification (machine learning)|classification]], marking ownership, noting boundaries, and indicating [[online identity]]. They may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. In the organization of information and objects, the use of textual keywords as part of identification and classification long  predates computers. However, computer based searching made the use of keywords a rapid way of exploring records.\u000a\u000a[[File:A Description of the Equator and Some Otherlands, collaborative hypercinema portal Upload page.jpg|thumb|A Description of the Equator and Some Otherlands, collaborative hypercinema portal, produced by documenta X, 1997. User upload page associating user contributed media with the term ''Tag''.]] Online and Internet databases and early websites deployed them as a way for publishers to help users find content. In 1997, the collaborative portal "A Description of the Equator and Some Other Lands" produced by [[documenta]] X, Germany, coined the folksonomic term ''Tag'' for its co-authors and guest authors on its Upload page. In "The Equator" the term ''Tag'' for user-input was described as an ''abstract literal or keyword'' to aid the user. Turned out in Web 1.0 days, all "Otherlands" users defined singular ''Tags'', and did not share ''Tags'' at that point.\u000a\u000aIn 2003, the [[social bookmarking]] website [[Delicious (website)|Delicious]] provided a way for its users to add "tags" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag.<ref>[http://flickr.com/photos/joshu/765809051/in/set-72157600740166824/ Screenshot of tags on del.icio.us] in 2004 and [http://flickr.com/photos/joshu/765817375/in/set-72157600740166824/ Screenshot of a tag page on del.icio.us], also in 2004, both published by [[Joshua Schachter]] on July 9, 2007.</ref> [[Flickr]] allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable.<ref>[http://www.adaptivepath.com/ideas/essays/archives/000519.php "An Interview with Flickr's Eric Costello"] by Jesse James Garrett, published on August 4, 2005. Quote: "Tags were not in the initial version of Flickr. Stewart Butterfield...liked the way they worked on del.icio.us, the social bookmarking application. We added very simple tagging functionality, so you could tag your photos, and then look at all your photos with a particular tag, or any one person\u2019s photos with a particular tag."</ref> The success of Flickr and the influence of Delicious popularized the concept,<ref>An example is [http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html "Folksonomies - Cooperative Classification and Communication Through Shared Metadata"] by Adam Mathes, December 2004. It focuses on tagging in Delicious and Flickr.</ref> and other [[social software]] websites&nbsp;\u2013 such as [[YouTube]], [[Technorati]], and [[Last.fm]]&nbsp;\u2013 also implemented tagging. Other traditional and web applications have incorporated the concept such as "Labels" in [[Gmail]] and the ability to add and edit tags in [[iTunes]] or [[Winamp]].\u000a\u000aTagging has gained wide popularity due to the growth of social networking, photography sharing and bookmarking sites. These sites allow users to create and manage labels (or \u201ctags\u201d) that categorize content using simple keywords. The use of keywords as part of an identification and classification system long predates computers. In the early days of the web keywords meta tags were used by web page designers to tell search engines what the web page was about. Today's tagging takes the meta keywords concept and re-uses it. The users add the tags. The tags are clearly visible, and are themselves links to other items that share that keyword tag.\u000a\u000aKnowledge tags are an extension of [[Index term|keyword]] tags. They were first used by [[Jumper 2.0]], an [[open source]] [[Web 2.0]] software platform released by Jumper Networks on 29 September 2008.<ref>{{Citation|url=http://www.jumpernetworks.com/ NEWS-Jumper_Networks_Releases_Jumper_2.0_Platform.pdf|title=Jumper Networks Press Release for Jumper 2.0|publisher=Jumper Networks, Inc.|date=29 September 2008}}</ref> Jumper 2.0 was the first [[collaborative search engine]] platform to use a method of expanded tagging for [[knowledge capture]].\u000a\u000aWebsites that include tags often display collections of tags as [[tag cloud]]s. A user's tags are useful both to them and to the larger community of the website's users.\u000a\u000aTags may be a "bottom-up" type of classification, compared to [[hierarchy|hierarchies]], which are "top-down". In a traditional hierarchical system ([[Taxonomy (general)|taxonomy]]), the designer sets out a limited number of terms to use for classification, and there is one correct way to classify each item. In a tagging system, there are an unlimited number of ways to classify an item, and there is no "wrong" choice. Instead of belonging to one category, an item may have several different tags. Some researchers and applications have experimented with combining structured hierarchy and "flat" tagging to aid in information retrieval.<ref>[http://infolab.stanford.edu/~heymann/taghierarchy.html Tag Hierarchies], research notes by Paul Heymann.</ref>\u000a\u000a==Examples==\u000a=== Within a Blog ===\u000aMany [[blog]] systems allow authors to add free-form tags to a post, along with (or instead of) placing the post into categories. For example, a post may display that it has been tagged with ''baseball'' and ''tickets''. Each of those tags is usually a [[web link]] leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.\u000a\u000a===For an event===\u000aAn official tag is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a [[controlled vocabulary]].\u000a\u000a===In research===\u000aA researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, free form [[categorization|classification]] allows the author to manage what would otherwise be unwieldy amounts of information. Commercial, as well as some free computer applications are readily available to do this.\u000a\u000a==Special types==\u000a===Triple tags===\u000a{{see also|Microformat}}\u000aA '''triple tag''' or '''machine tag''' uses a special [[syntax]] to define extra [[semantic]] information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a [[namespace]], a [[wikt:predicate|predicate]], and a value. For example, "geo:long=50.123456" is a tag for the geographical [[longitude]] coordinate whose value is 50.123456. This triple structure is similar to the [[Resource Description Framework]] model for information.\u000a\u000aThe triple tag format was first devised for geolicious<ref>[http://brainoff.com/weblog/2004/11/05/124 geo.lici.us : geotagging hosted services] by Mikel Maron, November 5, 2004.</ref> in November 2004, to map [[Delicious (website)|Delicious]] bookmarks, and gained wider acceptance after its adoption by [http://stamen.com/projects/mappr Mappr] and GeoBloggers<ref>[http://web.archive.org/web/20071011024028/http://geobloggers.com/archives/2006/01/11/advanced-tagging-and-tripletags/ Advanced Tagging and TripleTags] by Reverend Dan Catt, ''Geobloggers'', January 11, 2006.</ref> to map [[Flickr]] photos. In January 2007, [[Aaron Straup Cope]] at [[Flickr]] introduced the term ''machine tag'' as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.<ref>[http://www.flickr.com/groups/api/discuss/72157594497877875/ Machine tags], a post by Aaron Straup Cope in the Flickr API group, January 24, 2007.</ref>\u000a\u000aSpecialized metadata for geographical identification is known as ''[[geotagging]]''; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using [[binomial nomenclature]].<ref>[http://www.flickr.com/groups/encyclopedia_of_life/rules/ Encyclopedia of Life use of machine tag], The Encyclopedia of Life project rules including the required use of a taxonomy machine tag, September 19, 2009.</ref>\u000a\u000a===Hashtags===\u000a{{main|Hashtag}}\u000aA hashtag is a kind of metadata tag marked by the prefix <code>#</code>, sometimes known as a "hash" symbol. This form of tagging is used on [[microblogging]] and [[social networking service]]s such as [[Twitter]], [[Facebook]], [[Google+]], [[VK (social network)|VK]] and [[Instagram]].\u000a\u000a===Knowledge tags===\u000aA knowledge tag is a type of [[metadata|meta-information]] that describes or defines some aspect of an information resource (such as a [[document]], [[digital image]], [[database table|relational table]], or [[web page]]). Knowledge tags are more than traditional non-hierarchical [[index term|keywords or terms]]. They are a type of [[metadata]] that captures knowledge in the form of descriptions, categorizations, classifications, [[semantics]], comments, notes, annotations, [[hyperdata]], [[hyperlinks]], or references that are collected in tag profiles. These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository. Knowledge tags are a [[knowledge management]] discipline that leverages [[Enterprise 2.0]] methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. It generally allows greater flexibility than other [[knowledge management]] classification systems.\u000a\u000aCapturing knowledge in tags takes many different forms, there is factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies).<ref>\u000a{{Citation\u000a | last=Wiig | first=K. M.\u000a | year= 1997\u000a | title=Knowledge Management: An Introduction and Perspective\u000a | journal=Journal of Knowledge Management\u000a | volume=1 | issue=1\u000a | pages=6\u201314\u000a | url=http://www.mendeley.com/c/67997727/Wiig-1997-Knowledge-Management-An-Introduction-and-Perspective/\u000a | doi=10.1108/13673279710800682\u000a}}\u000a</ref> These forms of [[knowledge]] often exist outside the data itself and are derived from personal experience, insight, or expertise. \u000a\u000aKnowledge tags, in fact, manifest themselves in any number of ways \u2013 conceptual knowledge tags describe procedures, lessons learned, and facts that are related to the information resource. [[Tacit knowledge]] tags, manifests itself through skills, habits or learning by doing and represent experience or organizational intelligence. Anecdotal knowledge, is a memory of a particular case or event that may not surface without context.<ref>\u000a{{citation\u000a | last=Getting | first=Brian\u000a | year= 2007\u000a | title=What Are \u201cTags\u201d And What Is \u201cTagging?\u000a | publisher=Practical eCommerce\u000a | url=http://www.practicalecommerce.com/articles/589\u000a}}\u000a</ref> \u000a\u000aKnowledge can best be defined as information possessed in the mind of an individual: it is personalized or subjective information related to facts, procedures, concepts, interpretations, ideas, observations and judgments (which may or may not be unique, useful, accurate, or structurable). Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information.<ref>{{citation\u000a| author=Cambria, Erik and Hussain, Amir | title=Sentic album: Content-, concept-, and context-based online personal photo management system | journal=Cognitive Computation | volume=4 | issue=4 | pages=477-496 | year=2012 | doi=10.1007/s12559-012-9145-4}}</ref> Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turn-over, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.<ref>\u000a{{Citation\u000a | last=Alavi | first=Maryam\u000a | last2=Leidner\u000a | year= 1999\u000a | title=Knowledge Management Systems: Issues, Challenges, and Benefits\u000a | journal=Communications of the Association for Information Systems\u000a | volume=1 | issue=7\u000a | url=http://www.belkcollege.uncc.edu/jpfoley/Readings/artic07.pdf\u000a}}\u000a</ref>\u000a\u000a== Advantages and disadvantages ==\u000a{{procon|date=November 2012}}\u000a\u000aIn a typical tagging system, there is no explicit information about the meaning or [[semantics]] of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them.<ref name="Smith2008">Smith, Gene (2008). Tagging: People-Powered Metadata for the Social Web. Berkeley, CA: New Riders. ISBN 0-321-52917-0</ref> The flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.\u000a\u000aWhen users can freely choose tags (creating a [[folksonomy]], as opposed to selecting terms from a [[controlled vocabulary]]), the resulting metadata can include [[homonym]]s (the same tags used with different meanings) and [[synonym]]s (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject.<ref>Golder, Scott A. Huberman, Bernardo A. (2005).\u000a"[http://arxiv.org/abs/cs.DL/0508082 The Structure of Collaborative Tagging Systems]." Information Dynamics Lab, HP Labs. Visited November 24, 2005.</ref> For example, the tag "orange" may refer to the [[Orange (fruit)|fruit]] or the [[Orange (colour)|color]], and items related to a version of the [[Linux kernel]] may be tagged "Linux", "kernel", "Penguin", "software", or a variety of other terms. Users can also choose tags that are different [[inflection]]s of words (such as singular and plural),<ref>[http://keithdevens.com/weblog/archive/2004/Dec/24/SvP.tags Singular vs. plural tags in a tag-based categorization system] by Keith Devens, December 24, 2004.</ref> which can contribute to navigation difficulties if the system does not include [[stemming]] of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of "tag terms" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies collectively develop a partial set of tagging conventions.\u000a\u000a===Complex system dynamics===\u000a\u000aDespite the apparent lack of control, research has shown that a simple form of shared vocabularies emerges in social bookmarking systems. Collaborative tagging exhibits a form of [[complex system]]s dynamics,<ref name="WWW07-ref">Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proceedings of the 16th International Conference on the World Wide Web (WWW'07), Banff, Canada, pp. 211-220, ACM Press, 2007. Downloadable on [http://www2007.org/papers/paper635.pdf the conference's website]</ref> (or [[Self-organization|self organizing]] dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags that describe different resources (e.g., websites) converges over time to stable [[power law]] distributions.<ref name="WWW07-ref"/> Once such stable distributions form, simple vocabularies can be extracted by examining the [[correlation]]s that form between different tags.  This informal collaborative system of tag creation and management has been called a [[folksonomy]].\u000a\u000a===Spamming===\u000a\u000aTagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a [[YouTube]] video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items.<ref>[http://heymann.stanford.edu/tagspam.html Tag Spam], research notes by Paul Heymann.</ref> The number of tags allowed may also be limited to reduce spam.\u000a\u000a==Syntax==\u000aSome tagging systems provide a single [[text box]] to enter tags, so to be able to [[tokenize]] the string, a [[Wiktionary:separator|separator]] must be used. Two popular separators are the [[Space (punctuation)|space character]] and the [[comma]]. To enable the use of separators in the tags, a system may allow for higher-level separators (such as [[quotation mark]]s) or [[escape character]]s. Systems can avoid the use of separators by allowing only one tag to be added to each input [[Web widget|widget]] at a time, although this makes adding multiple tags more time-consuming.\u000a\u000aA syntax for use within [[HTML]] is to use the '''rel-tag''' [[microformat]] which uses the [[Rel attribute|''rel'' attribute]] with value "tag" (i.e., <code>rel="tag"</code>) to indicate that the linked-to page acts as a tag for the current context.<ref>[http://microformats.org/wiki/rel-tag rel tag microformat specification], Microformats Wiki, January 10, 2005.</ref>\u000a\u000a==See also==\u000a{{colbegin||27em}}\u000a* [[Collective intelligence]]\u000a* [[Concept map]]\u000a* [[Enterprise 2.0]]\u000a* [[Enterprise bookmarking]]\u000a* [[Explicit knowledge]]\u000a* [[Faceted classification]]\u000a* [[Folksonomy]]\u000a* [[Information ecology]]\u000a* [[Knowledge representation]]\u000a* [[Knowledge transfer]]\u000a* [[Metaknowledge]]\u000a* [[Ontology (information science)]]\u000a* [[Organisational memory]]\u000a* [[Semantic web]]\u000a* [[Tag cloud]]\u000a* [[Web 2.0]]\u000a{{colend}}\u000a'''Others'''\u000a{{colbegin||27em}}\u000a* [[Collective unconscious]]\u000a* [[Human-computer interaction]]\u000a* [[Social network aggregation]]\u000a* [[Enterprise social software]]\u000a* [[Expert system]]\u000a* [[Knowledge]]\u000a* [[Knowledge base]]\u000a* [[Knowledge worker]]\u000a* [[Management information system]]\u000a* [[Microformats]]\u000a* [[Social network]]\u000a* [[Social software]]\u000a* [[Sociology of knowledge]]\u000a* [[Tacit Knowledge]]\u000a{{colend}}\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a'''General'''\u000a{{refbegin}}\u000a*{{Citation\u000a | surname1=Nonaka | given1=Ikujiro\u000a | year=1994\u000a | title= A dynamic theory of organizational knowledge creation\u000a | journal= ORGANIZATION SCIENCE/ Vol. 5, No. 1, February 1994\u000a | pages=14\u201337\u000a | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992\u000a}}\u000a*{{Citation\u000a | surname1=Wigg | given1=Karl M  \u000a | year=1993  \u000a | title= Knowledge Management Foundations: Thinking About Thinking: How People and Organizations Create, Represent and Use Knowledge \u000a | journal= Arlington: Schema Press  \u000a | pages=153\u000a | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992 \u000a}} \u000a*{{Citation\u000a | surname1=Alavi | given1=Maryam\u000a | surname2=Leidner | given2=Dorothy E.\u000a | year=1999\u000a | title=Knowledge management systems: issues, challenges, and benefits\u000a | journal=Communications of the AIS\u000a | volume=1| issue=2 | url=http://portal.acm.org/citation.cfm?id=374117\u000a}}\u000a*{{Citation\u000a | surname1=Sandy | given1=Kemsley\u000a | year=2009\u000a | title=Models, Social Tagging and Knowledge Management #BPM2009 #BPMS2\u201909\u000a | journal=BPM, Enterprise 2.0 and technology trends in business\u000a | url=http://www.column2.com/2009/09/models-social-tagging-and-knowledge-management-bpm2009-bpms209/\u000a}}\u000a{{refend}}\u000a\u000a==External links==\u000a* [http://www.inc.com/tech-blog/twitter-hashtag-techniques-for-businesses.html Hashtag Techniques for Businesses], Curt Finch. Inc Magazine. May 26, 2011.\u000a* [http://www.tbray.org/tmp/tag-urn.html A Uniform Resource Name (URN) Namespace for Tag Metadata].  Tim Bray.  Internet draft, expired August 5, 2007.\u000a\u000a{{Web syndication}}\u000a\u000a{{DEFAULTSORT:Tag (Metadata)}}\u000a[[Category:Collective intelligence]]\u000a[[Category:Computer jargon]]\u000a[[Category:Information retrieval]]\u000a[[Category:Knowledge representation]]\u000a[[Category:Metadata]]\u000a[[Category:Reference]]\u000a[[Category:Web 2.0]]
p54
sg6
S'Tag (metadata)'
p55
ssI20
(dp56
g2
S'http://en.wikipedia.org/wiki/Semantic technology'
p57
sg4
V{{no footnotes|date=March 2013}}\u000a[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]\u000aIn [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. \u000a\u000aThis enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.\u000a\u000aWith traditional [[information technology]], on the other hand, meanings and relationships must be predefined and \u201chard wired\u201d into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.\u000a\u000aOff-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.\u000a\u000aSemantic technologies are \u201cmeaning-centered.\u201d They include tools for:\u000a\u000a* autorecognition of topics and concepts, \u000a* information and meaning extraction, and\u000a* categorization. \u000a\u000aGiven a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.\u000a\u000aSemantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.\u000a\u000a== See also ==\u000a* [[Business Intelligence 2.0]] (BI 2.0)\u000a* [[Metadata]]\u000a* [[Ontology (computer science)]]\u000a* [[Semantic targeting]]\u000a* [[Semantic web]]\u000a\u000a==References==\u000a\u000a* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004\u000a* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003 \u2014 Proc. of the 12th international conference on World Wide Web'', pp 700\u2013709. [[ACM Press]], 2003.\u000a* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.\u000a* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.\u000a* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.\u000a* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, \u000a* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September\u000a2004.\u000a* P. Hitzler, M. Krötzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&Hall/CRC, 2009, ISBN 978-1-4200-9050-5\u000a\u000a== External links ==\u000a* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]\u000a* [http://www.semanticarts.com Semantic Technology and the Enterprise]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Semantics]]
p58
sg6
S'Semantic technology'
p59
ssI149
(dp60
g2
S'http://en.wikipedia.org/wiki/Special Interest Group on Information Retrieval'
p61
sg4
S'{{Infobox organization\n|name           = ACM Special Interest Group on Information Retrieval\n|image          = sig-information-retrieval-logo.png\n|size           = 140px\n|alt            = ACM SIGIR\n|parent_organization = [[Association for Computing Machinery]]\n|website        = {{URL|sigir.org}}\n}}\n\n\'\'\'SIGIR\'\'\' is the [[Association for Computing Machinery]]\'s Special Interest Group on [[Information Retrieval]]. The scope of the group\'s specialty is the theory and application of computers to the acquisition, organization, storage, retrieval and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.\n\n== Conferences ==\nThe annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval.  SIGIR also sponsors the annual [[Joint Conference on Digital Libraries]] (JCDL) in association with [[SIGWEB]], the [[Conference on Information and Knowledge Management]], and the [[International Conference on Web Search and Data Mining]] (WSDM) in association with [[SIGKDD]], [[SIGMOD]], and [[SIGWEB]].\n\n=== SIGIR Conference Locations ===\n{| class="wikitable" border="1"\n|-\n!  Number\n!  Year\n!  Location\n|-\n|  22\n|  1999\n|  [[Berkeley, California]]\n|-\n|  23\n|  2000\n|  [[Athens]]\n|-\n|  24\n|  2001\n|  [[New Orleans]]\n|-\n|  25\n|  2002\n|  [[Tampere]]\n|-\n|  26\n|  2003\n|  [[Toronto]]\n|-\n|  27\n|  2004\n|  [[Sheffield]]\n|-\n|  28\n|  2005\n|  [[Salvador, Bahia]]\n|-\n|  29\n|  2006\n|  [[Seattle]]\n|-\n|  30\n|  2007\n|  [[Amsterdam]]\n|-\n|  31\n|  2008\n|  [[Singapore]]\n|-\n|  32\n|  2009\n|  [[Boston]]\n|-\n|  33\n|  2010\n|  [[Geneva]]\n|-\n|  34\n|  2011\n|  [[Beijing]]\n|-\n|  35\n|  2012\n|  [[Portland, Oregon]]\n|-\n|  36\n|  2013\n|  [[Dublin]]\n|-\n|  37\n|  2014\n|  [[Gold Coast, Queensland]]\n|-\n|  38\n|  2015\n|  [[Santiago]]\n|-\n|  39\n|  2016\n|  [[Pisa]]\n|-\n|  40\n|  2017\n|  [[Tokyo]]\n|}\n\n== Awards ==\nThe group gives out several awards to contributions to the field of information retrieval. The most important award is the [[Gerard Salton Award]] (named after the computer scientist [[Gerard Salton]]), which is awarded every three years to an individual who has made "significant, sustained and continuing contributions to research in information retrieval". Additionally, SIGIR presents a Best Paper Award <ref>{{cite web | url=http://sigir.org/awards/awards.html#bestpaper | title=SIGIR Conference Best Paper Awards | accessdate=2012-08-29 }}</ref> to recognize the highest quality paper at each conference.\n\n==See also==\n* [[Conference on Information and Knowledge Management]]\n\n==External links==\n* [http://www.sigir.org/ SIGIR]\n\n==References==\n\n{{Reflist}}\n{{Authority control}}\n\n[[Category:Association for Computing Machinery Special Interest Groups]]\n[[Category:Information retrieval]]'
p62
sg6
S'Special Interest Group on Information Retrieval'
p63
ssI23
(dp64
g2
S'http://en.wikipedia.org/wiki/Dynatext'
p65
sg4
S"{{primary sources|date=October 2011}}\n'''DynaText''' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.\n\nDynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.\n\nDynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996.\n\nDynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as the origin of the notion of [[well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].\n\n[[Inso]] corporation went out of business in 2002. \n\n==References==\n*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]]\n\n[[Category:Information retrieval]]\n\n{{markup-languages-stub}}"
p66
sg6
S'Dynatext'
p67
ssI152
(dp68
g2
S'http://en.wikipedia.org/wiki/URL redirection'
p69
sg4
V{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}\u000a\u000a{{refimprove|date=July 2014}}\u000a'''URL redirection''', also called '''URL forwarding''', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, '''domain redirection''' or '''domain forwarding''' is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org].\u000aURL redirection can be used for [[URL shortening]], to prevent [[link rot|broken links]] when web pages are moved, to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]], to guide navigation into and out of a website, for privacy protection, and for less innocuous purposes such as [[phishing]] attacks.\u000a\u000a== Purposes ==\u000aThere are several reasons to use URL redirection :\u000a\u000a=== Similar domain names ===\u000aA user might mis-type a URL\u2014for example, "example.com" and "exmaple.com". Organizations often register these "mis-spelled" domains and re-direct them to the "correct" location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to "reserve" other [[top-level domain]]s (TLD) with the same name, or make it easier for a true ".edu" or ".net" to redirect to a more recognizable ".com" domain.\u000a\u000a=== Moving pages to a new domain ===\u000aWeb pages may be redirected to a new domain for three reasons:\u000a* a site might desire, or need, to change its domain name;\u000a* an author might move his or her individual pages to a new domain;\u000a* two web sites might merge.\u000a\u000aWith URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers.\u000a\u000aThe same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a "moved permanently" redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.\u000a\u000a=== Logging outgoing links ===\u000aThe access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor's browser has no need to communicate with the original server when the visitor clicks on an outgoing link.\u000a\u000aThis information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website's domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website's server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.<ref>\u000a{{cite journal\u000a  | title = Google revives redirect snoopery\u000a  | journal = blog.anta.net\u000a  | date = 2009-01-29\u000a  | url = http://blog.anta.net/2009/01/29/509/\u000a  | issn = 1797-1993\u000a  | archiveurl=http://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/\u000a  | archivedate=2011-08-17\u000a}}</ref>\u000a\u000aThe same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.\u000a\u000a=== Short aliases for long URLs ===\u000a{{Main|URL shortening}}\u000a\u000aWeb applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.\u000a\u000a=== Meaningful, persistent aliases for long or changing URLs ===\u000a{{See also|Permalink|PURL|Link rot}}\u000a\u000aSometimes the URL of a page changes even though the content stays the same. Therefore URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.\u000a\u000a=== Post/Redirect/Get ===\u000a{{Main|Post/Redirect/Get}}\u000a\u000aPost/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).\u000a\u000a=== Manipulating search engines ===\u000aRedirect techniques are used to fool search engines.  For example, one page could show popular search terms to search engines but redirect the visitors to a different target page.  There are also cases where redirects have been used to "steal" the page rank of one popular page and use it for a different page, They will also redirect using searches with search engines as searches, usually involving the 302 [[List of HTTP status codes|HTTP status code]] of "moved temporarily."<ref>{{cite web|url=http://www.pandia.com/sw-2004/40-hijack.html |title=Google's serious hijack problem \u2013 Spammers hijack web site listings in Google |date=September 13, 2004 |publisher=Pandia.com |archiveurl=http://web.archive.org/web/20130605153457/http://www.pandia.com/sw-2004/40-hijack.html |archivedate=2013-06-05}}</ref><ref>[http://www.loriswebs.com/hijacking_web_pages.html "Stop Scrapers From Hijacking your Web Pages"]. Lori's Web Design.com. Retrieved 2013-12-18.</ref>\u000a\u000aSearch engine providers have noticed the problem and are working on appropriate actions.{{Citation needed|date=August 2009}}\u000a\u000aAs a result, today, such manipulations usually result in less rather than more site exposure.\u000a\u000a=== Manipulating visitors ===\u000aURL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.{{Citation needed|date=January 2010}} Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.\u000a\u000a=== Removing <code>referer</code> information ===\u000aWhen a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, <code><nowiki>http://company.com/plans-for-the-next-release-of-our-product</nowiki></code>), it is not desirable for the <code>referer</code> URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example <code><nowiki>http://externalsite.com/page</nowiki></code> into <code><nowiki>http://redirect.company.com/http://externalsite.com/page</nowiki></code>. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.\u000a\u000a== Techniques ==\u000aSeveral different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.\u000a\u000a=== Manual redirect ===\u000aThe simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:\u000a\u000a<source lang="html4strict">\u000aPlease follow <a href="http://www.example.com/">this link</a>.\u000a</source>\u000a\u000aThis method is often used as a fall-back&nbsp;\u2014 if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.\u000a\u000a=== HTTP status codes 3xx ===\u000aIn the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a '''redirect''' is a response with a [[List of HTTP status codes|status code]] beginning with ''3'' that causes a browser to display a different page.  The different codes describe the reason for the redirect, which allows for the correct subsequent action (such as changing links in the case of code 301, a permanent change of address).\u000a\u000aHTTP/1.1 defines [http://tools.ietf.org/html/rfc7231#section-6.4 several status codes] for redirection:\u000a* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)\u000a* [[HTTP 301|301 moved permanently]]\u000a* [[HTTP 302|302 found]] (originally "temporary redirect" in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)\u000a* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)\u000a* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)\u000a\u000aAll of these status codes require that the URL of the redirect target be given in the Location: header of the HTTP response.  The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.\u000a\u000a(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).\u000a\u000aAn [[HTTP]] response with the 301 "moved permanently" redirect looks like this:\u000a\u000a<source lang="html4strict">\u000aHTTP/1.1 301 Moved Permanently\u000aLocation: http://www.example.org/\u000aContent-Type: text/html\u000aContent-Length: 174\u000a\u000a<html>\u000a<head>\u000a<title>Moved</title>\u000a</head>\u000a<body>\u000a<h1>Moved</h1>\u000a<p>This page has moved to <a href="http://www.example.org/">http://www.example.org/</a>.</p>\u000a</body>\u000a</html>\u000a</source>\u000a\u000a==== Using server-side scripting for redirection ====\u000aWeb authors producing HTML content can't usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling "non-parsed-headers").  Many web servers will generate a 3xx status code if a script outputs a "Location:" header line.  For example, in [[PHP]], one can use the "header" function:\u000a\u000a<source lang="php">\u000aheader('HTTP/1.1 301 Moved Permanently');\u000aheader('Location: http://www.example.com/');\u000aexit();\u000a</source>\u000a\u000a(More headers may be required to prevent caching<ref name="php-301-robust-solution">{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=http://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}</ref>).\u000a\u000aThe programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using <code>response.buffer=true</code> and <code>response.redirect <nowiki>"http://www.example.com/"</nowiki></code>\u000a\u000aHTTP/1.1 [http://tools.ietf.org/html/rfc7231#section-7.1.2 allows for] either a relative URI reference or an absolute URI reference. If the URI reference is relative the client computes the required absolute URI reference according to [http://tools.ietf.org/html/rfc3986#section-5 the rules defined in RFC 3986].\u000a\u000a==== Apache mod_rewrite ====\u000aThe [[Apache HTTP Server]]'s [http://httpd.apache.org/docs/current/mod/mod_alias.html mod_alias] extension can be used to redirect certain requests.  Typical configuration directives look like:\u000a\u000a<source lang="apache">\u000aRedirect permanent /oldpage.html http://www.example.com/newpage.html\u000aRedirect 301 /oldpage.html http://www.example.com/newpage.html\u000a</source>\u000a</blockquote>\u000a\u000aFor more flexible URL rewriting and redirection, Apache [http://httpd.apache.org/docs/current/mod/mod_rewrite.html mod_rewrite] can be used.  E.g. to redirect a requests to a canonical domain name:\u000a<source lang="apache">\u000aRewriteEngine on\u000aRewriteCond %{HTTP_HOST} ^([^.:]+\u005c.)*oldsite\u005c.example\u005c.com\u005c.?(:[0-9]*)?$ [NC]\u000aRewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]\u000a</source>\u000a\u000aSuch configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a <code>.htaccess</code> file.\u000a\u000a==== nginx rewrite ====\u000a[[Nginx]] has an integrated http rewrite module,<ref>{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}</ref> which can be used to perform advanced URL processing and even web-page generation (with the <tt>return</tt> directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.<ref>{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}</ref><ref>{{cite web |url=http://mdoc.su/ |title=mdoc.su \u2014 Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>\u000a\u000aFor example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 <tt>/DragonFlyBSD/HAMMER.5</tt>] were to come along, it would first be redirected internally to <tt>/d/HAMMER.5</tt> with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:<ref>{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>\u000a<source lang="pcre">\u000a	location /DragonFly {\u000a		rewrite	^/DragonFly(BSD)?([,/].*)?$	/d$2	last;\u000a	}\u000a	location /d {\u000a		set	$db	"http://leaf.dragonflybsd.org/cgi/web-man?command=";\u000a		set	$ds	"&section=";\u000a		rewrite	^/./([^/]+)\u005c.([1-9])$		$db$1$ds$2	redirect;\u000a	}\u000a</source>\u000a\u000a=== Refresh Meta tag and HTTP refresh header ===\u000a[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.  See\u000a* [http://www.w3schools.com/tags/tag_meta.asp HTML <meta> tag]\u000a* [http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]\u000a\u000aA timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.<ref>[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ "Google and Yahoo accept undelayed meta refreshs as 301 redirects"]. Sebastian's Pamphlets. 3 September 2007.</ref>\u000a\u000aThis is an example of a simple HTML document that uses this technique:\u000a<source lang="html4strict">\u000a<html>\u000a<head>\u000a<meta http-equiv="Refresh" content="0; url=http://www.example.com/" />\u000a</head>\u000a<body>\u000a<p>Please follow <a href="http://www.example.com/">this link</a>.</p>\u000a</body>\u000a</html>\u000a</source>\u000a\u000aThis technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the "head" section of the HTML file.  The number "0" in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the "body" section is for users whose browsers do not support this feature.\u000a\u000aThe same effect can be achieved with an HTTP <code>refresh</code> header:\u000a<source lang="html4strict">\u000aHTTP/1.1 200 ok\u000aRefresh: 0; url=http://www.example.com/\u000aContent-type: text/html\u000aContent-length: 78\u000a\u000aPlease follow <a href="http://www.example.com/">this link</a>.\u000a</source>\u000a\u000aThis response is easier to generate by CGI programs because one does not need to change the default status code.\u000a\u000aHere is a simple CGI program that effects this redirect:\u000a<source lang="perl">\u000a#!/usr/bin/perl\u000aprint "Refresh: 0; url=http://www.example.com/\u005cr\u005cn";\u000aprint "Content-type: text/html\u005cr\u005cn";\u000aprint "\u005cr\u005cn";\u000aprint "Please follow <a href=\u005c"http://www.example.com/\u005c">this link</a>!"\u000a</source>\u000a\u000aNote: Usually, the HTTP server adds the status line and the Content-length header automatically.\u000a\u000aThe [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C's [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don't break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].\u000a\u000a=== JavaScript redirects ===\u000a[[JavaScript]] can cause a redirect by setting the <code>window.location</code> attribute, e.g.:\u000a<syntaxhighlight lang="ecmascript">\u000awindow.location='http://www.example.com/'\u000a</syntaxhighlight>\u000aNormally JavaScript pushes the redirector site's [[URL]] to the browser's history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.<ref>{{cite web|url=http://online-marketing-technologies.com/tools/javascript-redirection-generator.html|title=Advanced JavaScript Redirections|publisher=Online Marketing Technologies}}</ref>\u000a<syntaxhighlight lang="ecmascript">\u000awindow.location.replace('http://www.example.com/')\u000a</syntaxhighlight>\u000aHowever, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.\u000a\u000a=== Frame redirects ===\u000aA slightly different effect can be achieved by creating a single HTML [[Iframe|frame]] that contains the target page:\u000a<source lang="html4strict">\u000a<frameset rows="100%">\u000a  <frame src="http://www.example.com/">\u000a  <noframes>\u000a    <body>Please follow <a href="http://www.example.com/">link</a>.</body>\u000a  </noframes>\u000a</frameset>\u000a</source>\u000a\u000aOne main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar.\u000a\u000aThis ''cloaking'' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].<ref>Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf "Anti-Phishing Technology"] (PDF). Radix Labs.</ref>\u000a\u000aThe same effect can be done with an inline frame:\u000a<source lang="html4strict">\u000a<iframe height="100%" width="100%" src="http://www.example.com/">\u000aPlease follow <a href="http://www.example.com/">link</a>.\u000a</iframe>\u000a</source>\u000a\u000a=== Redirect chains ===\u000aOne redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia'''.com'''/wiki/URL_redirection] (note the domain name) is first redirected to [[:www:URL redirection|http://www.wikipedia'''.org'''/wiki/URL redirection]] and then to the correct URL: http://en.wikipedia.org/wiki/URL_redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by ''rewriting'' the URL as much as possible on the server before returning it to the browser as a redirect.\u000a\u000a=== Redirect loops ===\u000aSometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.\u000a\u000a[http://tools.ietf.org/html/rfc7231#section-6.4 HTTP/1.1] states:\u000a<blockquote>\u000aA client ''SHOULD'' detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).\u000a\u000aNote: An earlier version of this specification recommended a maximum of five redirections ([RFC2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.\u000a</blockquote>\u000aNote that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -> http://www.example.com/2 -> http://www.example.com/3 ...\u000a\u000a== Services ==\u000aThere exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.\u000a\u000a=== URL redirection services ===\u000aA '''redirect service''' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]].\u000a\u000aHyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.\u000a\u000aRecently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs.\u000a\u000aA major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.\u000a\u000a==== History ====\u000aThe first redirect services took advantage of [[top-level domains]] (TLD) such as "[[.to]]" (Tonga), "[[.at]]" (Austria) and "[[.is]]" (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including "r.im", "go.to", "i.am", "come.to" and "start.at".  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.<ref>{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}</ref> As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined.\u000a\u000aWith the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.\u000a\u000a=== Referrer masking ===\u000aRedirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.)\u000a\u000aThis type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .\u000a\u000aHere is a simplistic example of such a service, written in [[PHP]].\u000a<source lang="php">\u000a<?php\u000a$url = htmlspecialchars($_GET['url']);\u000aheader( 'Refresh: 0; url=http://'.$url );\u000a?>\u000a<!-- Fallback using meta refresh. -->\u000a<html>\u000a <head>\u000a  <title>Redirecting...</title>\u000a  <meta http-equiv="refresh" content="0;url=http://<?php echo $url; ?>">\u000a </head>\u000a <body>\u000a Attempting to redirect to <a href="http://<?php echo $url; ?>">http://<?php echo $url; ?></a>.\u000a </body>\u000a</html>\u000a</source>\u000a\u000aThe above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server's resources.\u000a\u000a==Security Issues==\u000aURL redirection can be abused by attackers for [[Phishing]] attacks, such as [[Open Redirect]] and [[Covert Redirect]].\u000a\u000a"An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation."<ref name="Open_Redirect">{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}</ref>\u000a\u000a"Covert Redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation."<ref name="Covert_Redirect">{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}</ref> It is disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.<ref name="CNET">{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}</ref>\u000a\u000a== See also ==\u000a* [[Link rot]]\u000a* [[Canonical meta tag]]\u000a* [[Domain masking]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]\u000a* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)\u000a* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification\u000a* [http://www.dancatts.com/articles/htaccess-301-redirects-for-moved-pages.php 301 Redirects for moved pages using .htaccess]\u000a* [http://911-need-code-help.blogspot.com/2011/03/redirecting-visitors-to-preferred.html Redirecting your visitors to your preferred domain] using 301 permanent redirects&nbsp;\u2014 rationale and mod_rewrite/PHP/ASP.NET implementations\u000a\u000a{{Spamming}}\u000a\u000a{{Use dmy dates|date=November 2010}}\u000a\u000a{{DEFAULTSORT:Url Redirection}}\u000a[[Category:Uniform resource locator]]\u000a[[Category:Black hat search engine optimization]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]
p70
sg6
S'URL redirection'
p71
ssI26
(dp72
g2
S'http://en.wikipedia.org/wiki/ZyLAB Technologies'
p73
sg4
V{{advert|date=April 2012}}\u000a{{Infobox company |\u000a  name   = ZyLAB |\u000a  logo   = <!--  Commented out because image was deleted: [[Image:zylab logo.jpg|center]] --> |\u000a  slogan = "eDiscovery & Information Risk Management" |\u000a  type   = Private |\u000a  foundation     = 1983 |\u000a  location       = [[McLean, Virginia]]<br>[[Amsterdam]] |\u000a  key_people     = [[Pieter Varkevisser]], president & CEO<br>[[Dr. Johannes C. Scholtes]], chairman & chief strategy officer | Mary Mack, Enterprise Technology Counsel\u000a  num_employees  = 140 |\u000a  industry       = [[Software]], eDiscovery and Information Risk Management, Records Management, Email Archiving, SharePoint Archiving |\u000a  products       = ZyLAB Information Management Platform and various bundles for eDiscovery, email & SharePoint archiving, text-analytics, visualization, contract management, and workflow. |\u000a\u000a  homepage       = [http://www.zylab.com/ www.zylab.com]\u000a}}\u000a\u000a'''ZyLAB''' is a developer of software for [[Electronic discovery|e-discovery]], information risk management, email management, records, contract, and document management, knowledge management, and workflow. The company is headquartered in [[McLean, Virginia]] and in [[Amsterdam]], [[Netherlands]]. ZyLAB\u2019s most important products are ZyLAB eDiscovery & Production System, the ZyLAB Information Management Platform and bundles that build systems for deployments.\u000a\u000a== History ==\u000aIn 1983 ZyLAB was the first company providing a [[Full text search|full-text]] search program for electronic files stored in the file system of [[IBM PC compatible|IBM-compatible PCs]]. The program was called ZyINDEX. The first version of ZyINDEX was written in [[Pascal (programming language)|Pascal]] and worked on [[MS-DOS]]. Subsequent programs were written in [[C (programming language)|C]], [[C++]] and [[C Sharp (programming language)|C#]] and work on a variety of Microsoft operating systems.\u000a\u000aIn 1991, ZyLAB integrated ZyINDEX with an optical character recognition ([[Optical character recognition|OCR]]) program, Calera Wordscan, which was a spin-off from [[Raymond Kurzweil]]\u2019s first OCR implementation. This integration was called ZyIMAGE. ZyIMAGE was the first PC program to include a [[Fuzzy string searching|fuzzy string search]] algorithm to overcome scanning and OCR errors.\u000a\u000aIn 1998, the company developed support to full-text search email, including attachments.\u000a\u000aIn 2000, ZyLAB embraced the new [[XML]] standard and created a full content management and records management system based on the XML standard and build a full solution for e-discovery, historical archives, records management, document management, email archiving, contract management, and professional back-office solutions.\u000a\u000aIn 2003, the company invested in expanding the ZyIMAGE product suite with advanced [[text analytics]], [[text mining]], [[data visualization]], [[computational linguistics]], and [[Machine translation|automatic translation]].\u000a\u000a2005: ZyIMAGE Information Access Platform was released, an integrated solution to address information access problems.\u000a\u000aPlatforms for ZyIMAGE e-Discovery and legal production, historical archiving, compliance, back-office records management and [[COMINT#COMINT|COMINT]] were launched in 2007.\u000a\u000a2010: ZyLAB Information Management Platform was released, an integrated solution to address e-Discovery and information management problems.\u000a\u000a==Customers==\u000aInitial customers of ZyINDEX were organizations such as the [[FBI]] and other law enforcement agencies to investigate electronic data from seized PCs, the [[United States Navy|U.S. Navy]] for on-board manuals, and law firms around the world for [[Electronic discovery|e-Discovery]]. Over the years, ZyLAB received grants from the European Union (DG13).\u000a\u000aOther well-known ZyLAB customers were [[O. J. Simpson murder case|O.J. Simpson's defense team]], war crime tribunals such as the [[trial of Slobodan Milo\u0161evi\u0107]], the [[Special Court for Sierra Leone]], the [[Extraordinary Chambers in the Courts of Cambodia|UN-AKRT-ECCC Cambodia Khmer Rouge trials]] and the [[International Criminal Tribunal for Rwanda|Rwanda tribunal]]. In 2007, the U.S. [[Executive Office of the President of the United States|Executive Office of the President]] selected ZyLAB for email archiving, basically for its open XML structures, which is endorsed by organizations such as the [[National Archives and Records Administration]]. ZyLAB\u2019s software was used for many other high-profile investigations such as the [[Oklahoma City bombing]].\u000a\u000aPublic websites also use the ZyLAB Webserver.\u000a\u000a[[Gartner]] positioned ZyLAB in the "Leaders" quadrant in its 2007, 2008 and 2009 Magic Quadrant for Information Access Solutions, gave it a strong positive rating in its 2007, 2008 and 2009 e-Discovery Marketscope and a Positive Rating in its 2007 and 2008 Records Management MarketScope.\u000a\u000aZyLAB\u2019s chief strategy officer, Dr. Johannes C. Scholtes, is professor in [[text mining]] at [[Maastricht University|the University of Maastricht]] faculty of Humanities and Sciences and director in the board of AIIM.\u000a\u000a==System overview and compatibility==\u000aAccording to the company\u2019s website it delivers systems for deployments, product bundles and the core components is the ZyLAB Information Management platform include:\u000a\u000aSystems:\u000a*ZyLAB eDiscovery and Production\u000a*ZyLAB Compliance and Litigation readiness\u000a*ZyLAB Law Enforcement and Investigations\u000a*ZyLAB Communications Intelligence\u000a*ZyLAB Digital Print and Media Archiving\u000a*ZyLAB Enterprise Information Management\u000a\u000aBundles:\u000a*E-Mail Archiving Bundle\u000a*Microsoft SharePoint Bundle\u000a*Analytics Bundle\u000a*eDiscovery EDRM Processing bundle\u000a*DoD and Sox Compliant RMA Bundle\u000a*TIFF Archiving and Production Bundle\u000a*WebPublishing Bundle\u000a*Commercial Publishing Bundle\u000a*Business Process Automation Bundle\u000a*Development and Integrators Bundle\u000a*Scanning Bundle\u000a*Digital Copier Bundle\u000a*Professional Text Mining\u000a*Machine translation\u000a\u000a===Supported configurations===\u000a*'''Server OS''': Windows 2003, Windows 2008\u000a*'''Databases''': XML, MS SQL Server 2005, MS SQL Server 2008, Oracle 10g, Oracle 11g, mySQL\u000a*'''Web Servers''': IIS\u000a*'''Client OS''': Windows XP, Windows Vista, Windows 7\u000a*'''Clustering''': Support for Active/Passive Failover.\u000a*'''Authentication''': Active Directory, LDAP, XML, NTFS, IBM Tripoli.\u000a*'''Virtualization''': VMware Infrastructure, VMware Workstation, VMware Server, VMware Fusion.\u000a\u000a===Languages supported===\u000a*'''Unicode'''. Support for documents in all languages.\u000a*'''Internationalization'''. ZyLAB offers translated products for English, German, French, Dutch, Spanish, Italian, Danish, Swedish, Norwegian, Finnish, Portuguese, Arabic and [[Persian language|Persian]]. In addition to these languages, over 400 languages are supported by ZyLAB's recognition and full-text indexing technology, including all Western-European, Eastern European, Baltic, African, Asian and South American languages. ZyLAB's technical ability for broad language and character recognition enhances the accuracy of stored information searches and helps diminish the costs incurred by incorrect searches or text correction.\u000a\u000a==Zy-IMAGE-nation Annual Conference==\u000aThe annual Zy-IMAGE-nation Conference is sponsored by ZyLAB. During this conference, seminars and interactive sessions from leading professionals about the advanced technologies and procedural enhancements that are driving new levels of operational efficiency in private and public sectors. The focus of the conference is on technologies that provide integrated capabilities for managing the accumulated knowledge of an organization, especially records and e-mail, as well as other business-critical processes. Related topics to be covered include best practices for e-discovery preparation and implementation, records management, email archiving, and knowledge management.\u000a\u000a==See also==\u000a* [[Electronic discovery|e-Discovery]]\u000a* [[Optical character recognition|Optical Character Recognition (OCR)]]\u000a* [[Document Imaging]]\u000a* [[E-mail archiving|E-mail Archiving]]\u000a* [[Knowledge Management]]\u000a* [[Document management system|Document Management (System)]]\u000a* [[Enterprise content management|Enterprise Content Management]]\u000a* [[Records management|Records Management]]\u000a* [[Contract management|Contract Management]]\u000a* [[Workflow]]\u000a* [[Text mining|Text Mining]]\u000a* [[Text analytics|Text Analytics]]\u000a* [[Machine translation|Automatic Machine Translation]]\u000a* [[Data visualization|Data Visualization]]\u000a\u000a==References==\u000a{{Reflist}}\u000a*[http://www.pcmag.com/encyclopedia_term/0,,t=zyindex&i=55248,00.asp Definition of ZyINDEX] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia\u000a*[http://www.pcmag.com/encyclopedia_term/0,2542,t=ZyIMAGE&i=55247,00.asp Definition of ZyIMAGE] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia\u000a*[http://www.informationweek.com/777/knowledge.htm Review] of ZyImage 3.0 in ''[[InformationWeek]]''\u000a*[http://www.accessmylibrary.com/coms2/summary_0286-9201794_ITM Mac version of ZyINDEX made its debut on Comdex]\u000a*[http://query.nytimes.com/gst/fullpage.html?res=940DE6DA1730F93AA35751C0A96E948260 Review] of ZyINDEX in the ''[[New York Times]]''\u000a*[http://www.computerwoche.de/heftarchiv/1988/26/1155611/ Review] of ZyINDEX on ''Computerwoche.de'' (article in German)\u000a*[http://www.computerwoche.de/index.cfm?pid=2123&pk=1096333 Review] of ZyIMAGE's webserver on ''Computerwoche.de'' (article in German)\u000a*[http://nl.newsbank.com/nl-search/we/Archives?p_product=MH&s_site=miami&p_multi=MH&p_theme=realcities&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EB367D56736E685&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date: Review] of ZyINDEX in the ''[[Miami Herald]]''\u000a*[http://www.usdoj.gov/oig/special/0203/chapter3.htm ZyINDEX used in the Investigation of the Belated Production of Documents in the Oklahoma City Bombing Case]\u000a*[http://www.fcw.com/print/6_31/news/70014-1.html Review] of ZyIMAGE on ''Federal Computer Week (FCW.com)''\u000a*Zylab retrieval engine optimized for CD-ROM; Zylab, Progressive Technologies merge," Seybold Report on Desktop Publishing. vol. 8, No. 10, Jun. 6, 1994, p. 40.\u000a*Knibbe, "ZyImage 2 boosts, OCR, batch duties," InfoWorld, vol. 15, Issue 51, Dec. 20, 1993, p.&nbsp;20.\u000a*Knibbe, "ZyImage 3.0 will facilitate distribution on CD-ROMs; Boasts integration with WordScan OCR software," InfoWorld, vol. 16, No. 38, Sep. 19, 1994, p.&nbsp;22.\u000a*Marshall, "Text retrieval alternatives: 10 more ways to pinpoint important information," Infoworld, vol. 14, No. 12, Mar. 23, 1992, pp.&nbsp;88\u201389.\u000a*Marshall, "ZyImage adds scanning access to ZyIndex," InfoWorld, vol. 16, No. 15, Apr. 11, 1994, pp.&nbsp;73, 76, and 77.\u000a*Marshall, "ZyImage is ZyIndex plus a scan interface integrated," InfoWorld. vol. 15, Issue 10, Mar. 8, 1993, p.&nbsp;100.\u000a*Marshall et al., "ZyIndex for Windows, Version 5.0," InfoWorld, v. 15, n. 21, May 1993, pp.&nbsp;127, 129, 133 and 137.\u000a*Simon, "ZyImage: A Winning Combination of OCR And Text Indexing," PC Magazine. vol. 12, No. 6, Mar. 30, 1993, p.&nbsp;56.\u000a*Rooney, "Text-retrieval veterans prepare Windows attack," PC Week, v. 9, n. 24, Jun. 1992, p.&nbsp;46.\u000a*Rooney, "ZyLab partners with Calera: firms roll out document-image system," PC Week, vol. 10, No. 3, Jan. 25, 1993, p.&nbsp;22.\u000a*Torgan, "ZyImage: Document Imaging and Retrieval System," PC Magazine. vol. 12, No. 3, Feb. 9, 1993, p.&nbsp;62.\u000a\u000a===Gartner reports===\u000a*Introduction to Investigative Case Management Products (18 April 2007)\u000a*Hype Cycle for Legal and Regulatory Information Governance, 2007 (16 July 2007)\u000a*MarketScope for Contract Management, 2007 (16 July 2007)\u000a*Choosing an E-Discovery Solution in 2007 and 2008 (18 July 2007)\u000a*Magic Quadrant for Information Access Technology, 2007 (5 September 2007)\u000a*Magic Quadrant for Information Access Technology, 2008\u000a*Magic Quadrant for Information Access Technology, 2009\u000a*The Expanding Enterprise E-Discovery Marketplace (12 November 2007)\u000a*MarketScope for E-Discovery and Litigation Support Vendors, 2007 (14 December 2007)\u000a*MarketScope for E-Discovery Product Vendors, 2008\u000a*MarketScope for E-Discovery Product Vendors, 2009\u000a*MarketScope for Records Management (20 May 2008)\u000a*Hype Cycle for Content Management, 2008 (8 July 2008)\u000a*Using the Electronic Discovery Reference Model to Identify, Collect and Preserve Digital Evidence (11 July 2008)\u000a*Using the Electronic Discovery Reference Model to Process, Review and Analyze Digital Evidence (11 July 2008)\u000a*Hype Cycle for Governance, Risk and Compliance Technologies, 2009 (17 July 2009)\u000a\u000a==External links==\u000a*[http://www.zylab.com/ ZyLAB official website]\u000a*[http://www.edrm.net/ The Electronic Discovery Reference Model (EDRM)]\u000a*[http://www.aiim.org/ AIIM]\u000a\u000a[[Category:Companies established in 1983]]\u000a[[Category:Software companies of the United States]]\u000a[[Category:Information retrieval]]
p74
sg6
S'ZyLAB Technologies'
p75
ssI155
(dp76
g2
S'http://en.wikipedia.org/wiki/Hashtag'
p77
sg4
V[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the usage of a #timetoact hashtag at a 2014 conference]]\u000a\u000aA '''hashtag''' is a word or an unspaced phrase prefixed with the [[Number sign|hash character (or number sign), <code>#</code>]], to form a label.<ref>http://www.merriam-webster.com/dictionary/hashtag</ref> It is a type of [[Tag (metadata)|metadata tag]]. Words or phrases in messages on [[microblogging]] and [[social networking service]]s such as [[Facebook]], [[Google+]], [[Instagram]], [[Twitter]], or [[VK (social network)|VK]] may be tagged by entering # before them,<ref>{{cite web|url=http://support.twitter.com/articles/49309# |title=Using hashtags on Twitter|publisher=support.twitter.com |accessdate=2013-11-25}}</ref> either as they appear in a sentence, e.g., "New artists announced for #SXSW2014 Music Festival"<ref>{{cite web|url=https://dev.twitter.com/media/hashtags |title=Best Practices for Hashtags &#124; Twitter Developers |publisher=Dev.twitter.com |date=2011-07-19 |accessdate=2013-11-12}}</ref> or appended to it. The term hashtag can also refer to the hash symbol itself when used in the context of a hashtag.<ref>{{cite web |title=Oxford English Dictionary - Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June de 2014}}</ref>\u000a\u000aA hashtag allows grouping of similarly tagged messages, and also allows an electronic search to return all messages that contain it.\u000a\u000aDue to its widespread use, 'hashtag' was added to the ''[[Oxford English Dictionary]]'' in June 2014.<ref>{{cite web |title='Hashtag' added to the OED \u2013 but # isn't a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=13 June 2014}}</ref><ref>{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June de 2014}}</ref>\u000a\u000a==Origin==\u000aThe [[number sign]] was often used in [[information technology]] to highlight a special meaning. In 1970 for example, the number sign was used to denote ''immediate'' [[address mode]] in the assembly language of the [[PDP-11]]<ref>{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=2011-08-03 |accessdate=2014-08-25}}</ref> when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used ''#'' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].<ref>{{cite book|title=[[The C Programming Language]]|authors=B.W.Kernighan &  d.Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}</ref> Since before the invention of the hashtag, the number sign has been called the "hash symbol" in some countries outside of North America.<ref>{{cite book|last1=Bourke|first1=Jane|title=Communication Techonology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=http://books.google.co.uk/books?id=gPNBTmxzpIIC&lpg=PA19&dq=hash%20key%20telephone&pg=PA19#v=onepage&q=hash&f=false|accessdate=7 November 2014|isbn=9781863975858}}</ref><ref>{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=9780195157048|pages=33, 260|url=http://books.google.co.uk/books?id=dUTdk93cq9UC&lpg=PA260&dq=hash%20telephone&pg=PA260#v=onepage&q=hash%20mark&f=false}}</ref>\u000a\u000aThe number sign then appeared and was used within [[Internet Relay Chat|IRC]] networks to label groups and topics.<ref>"Channel Scope". Section 2.2. RFC 2811</ref> Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] '&').<ref>{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=3 June 2014}}</ref>\u000a\u000aThe use of the number sign in IRC inspired<ref>{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=2014-08-29}}</ref> [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.<ref>{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&pagewanted=all | title=Twitter\u2019s Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}</ref> He posted the first hashtag on Twitter: \u000a{{quote |1=how do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = ("factoryjoe"), August 23, 2007<ref>{{cite web|url = https://twitter.com/#!/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina ("factoryjoe")|date = August 23, 2007<!-- 3:25 PM-->}}</ref> |width  = 50% |align  = center }}\u000aInternationally, the hashtag became a practice of writing style for Twitter posts during the [[2009\u20132010 Iranian election protests]], as both English- and [[Persian language|Persian]]-language hashtags became useful for Twitter users inside and outside Iran.{{cite web|url=http://www.dw.de/%D8%AD%DA%A9%D8%A7%DB%8C%D8%AA-%D9%87%D8%B4%D8%AA%DA%AF%DB%8C-%DA%A9%D9%87-%D8%A7%DB%8C%D8%B1%D8%A7%D9%86%DB%8C%D8%A7%D9%86-%D8%A2%D8%BA%D8%A7%D8%B2-%DA%A9%D8%B1%D8%AF%D9%86%D8%AF/g-18012627|title = dw |date= 2009}}\u000a\u000aThe first use of the term "hash tag" was in a blog post by Stowe Boyd, "Hash Tags = Twitter Groupings,"<ref>{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=2013-09-19}}</ref> on 26 August 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society's New Words Committee.\u000a\u000aBeginning July 2, 2009,{{citation needed|date=November 2013}} Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced "[[Twitter#Trending_topics|Trending Topics]]" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.<ref>{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter's Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=2014-12-03}}</ref>\u000a\u000a==Style==\u000aOn microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. "It is #sunny today").\u000a\u000aThe quantity of hashtags used in a post or tweet is just as important as the type of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the "absolute maximum", and any contribution exceeding this risks \u201craising the ire of the community.\u201d<ref>{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=22 February 2014}}</ref>\u000a\u000aAs well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.<ref>{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=22 February 2014}}</ref>{{failed verification|date=August 2014}}\u000a \u000a[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on ''[[Late Night with Jimmy Fallon]]'' in September 2013.<ref>{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=http://www.youtube.com/watch?v=57dzaMaouXA |title="#Hashtag" with Jimmy Fallon & Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=2013-09-24 |accessdate=2014-08-25}}</ref>\u000a\u000a==Function==\u000a[[File:Seguir hashtags.png|300px|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag "#science".]]\u000aHashtags are mostly used as unmoderated ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can "trend" and attract more individual users to discussion using the hashtag. On Twitter, when a hashtag becomes extremely popular, it will appear in the "Trending Topics" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users, and neither can they be "retired" from public usage, meaning that hashtags can be used in theoretical perpetuity depending upon the longevity of the word or set of characters in a written language. They also do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes as espoused by those who make use of them.\u000a\u000aHashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using "#cakefestival" rather than simply "#cake". However, this can also make it difficult for topics to become "trending topics" because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.\u000a\u000aHashtags also function as beacons in order for users to find and "follow" (subscribe) or "list" (organize into public contact lists) other users of similar interest.\u000a\u000aHashtags can be used on the social network [[Instagram]], by posting pictures and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic like #photography #iPhone #iphoneography and therefore do not fulfil a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.<ref>{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = 7 November 2013|publisher=BBC.co.uk |accessdate=2013-11-25}}</ref> The censorship and ban against certain hashtags has a consequential role in the way that particular subaltern communities are built and maintained on Instagram. Despite Instagram\u2019s content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.<ref>\u000aOlszanowski, M. (2014). "Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship". Visual Communication Quarterly, 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F</ref> \u000a\u000a\u000aHashtags are also used informally to express context around a given message, with no intent to actually categorize the message for later searching, sharing, or other reasons.  This can help express humor, excitement, sadness or other contextual cues, for example "It's Monday!! #excited #sarcasm"\u000a\u000a==Use outside of social networking websites==\u000aThe feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]; in the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.<ref>{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker's Open Forums|author = Gabriel Snyder|publisher = Gawker|date = Oct 15, 2009<!-- 3:25 PM-->}}</ref><ref>{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting "chaos"|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = Oct 15, 2009 <!-- 8 a.m. -->}}</ref> Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the "river" of Twitter posts which can result from search terms or hashtags.{{citation needed|date=September 2014}}\u000a\u000a==Websites that support hashtags==\u000a{{Cleanup-list|section|date=May 2014}}\u000a{{columns-list|2|\u000a<!-- PLEASE RESPECT ALPHABETICAL ORDER -->\u000a* [[App.net]]\u000a* [[Diaspora (software)|Diaspora software]] and [[Diaspora (social network)|social network]]\u000a* [[DeviantART]]\u000a* [[Facebook]]\u000a* [[Flickr]]\u000a* [[FriendFeed]]\u000a* [[Gawker Media]] websites\u000a* [[GNU Social]]\u000a* [[Google+]]\u000a* [[Instagram]]\u000a* [[Kickstarter]]\u000a* [[Orkut]]<ref>{{cite web|url = http://en.blog.orkut.com/2012/02/hashtags-in-orkut-communities.html|title = Hashtags in Orkut communities|date = February 6, 2012 <!-- , 6:11 PM --> |publisher = Orkut|author = Marco Wisniewski}}</ref>\u000a* [[Sina Weibo]]\u000a* [[SoundCloud]]\u000a* [[Tout (company)|Tout]]\u000a* [[tsu]]\u000a* [[Tumblr]]\u000a* [[Twitter]]\u000a* [[Vine (software)|Vine]]\u000a* [[VK (social network)|VK]]\u000a}}\u000a\u000a==Usage==\u000a\u000a===Mass broadcast media===\u000a\u000aSince 2010, television series on various television channels promote themselves through "branded" hashtag [[digital on-screen graphic|bugs]].<ref>{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = Apr 21, 2011<!-- 3:25 PM-->|author = Michael Schneider|publisher = TV Guide}}</ref> This is used as a means of promoting a [[backchannel]] of online side-discussion before, during and after an episode broadcast. Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement<ref>{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald's Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}</ref> (for example, a motion picture trailer).\u000a\u000aWhile personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or "branded" hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to "trend" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It's Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, "temporary" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.<ref>{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter's Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}</ref> Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].<ref>{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |publisher=Ibtimes.com |date=2013-03-22 |accessdate=2013-09-19}}</ref>\u000a\u000aThe increased usage of hashtags as brand promotion devices has been compared to the promotion of branded "[[Index term|keywords]]" by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of commercials and series episodes.<ref>{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitter\u2019s Hashtag Pages Could Be The New AOL Keywords \u2014 But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}</ref>\u000a\u000a===Purchasing===\u000a\u000aSince February 2013 there is a collaboration between the social networking site Twitter and [[American Express]] that makes it possible to buy discounted goods online by tweeting a special hashtag.<ref>{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = 12 February 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = 2013-11-25}}</ref> American Express members can sync their card with Twitter and use the offers by tweeting and look for a response in a tweet with the confirmation from American Express.<ref>{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=2013-11-25}}</ref>\u000a\u000a===Event promotion===\u000a\u000a[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]\u000aOrganized real-world events have also made use of hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other on both Twitter and, in many cases, in real life during events.\u000a\u000aCompanies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.\u000a\u000aPolitical protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.\u000a\u000a===Consumer complaints===\u000aHashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term "bashtag" has been created to describe situations in which a corporate social media hashtag is used to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald's]] created the #McDStories hashtag so customers could share positive experiences about the restaurant chain. The marketing effort was cancelled after just two hours when McDonald's received numerous complaint tweets rather than the positive stories they were expecting.<ref>{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = 17 May 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = 2012-06-12}}</ref>\u000a\u000a===Sentiment analysis===\u000aThe use of hashtags also reveals things about the sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]]<ref>{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}</ref>\u2014a difficult [[Artificial Intelligence|AI]] problem.{{citation needed|date=September 2014}}\u000a\u000a==In popular culture==\u000aDuring the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]] [[Jack Layton]] referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]'s crime policies as "a hashtag fail" (presumably "#fail").<ref>{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton's debatable 'hashtag' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = Apr 13, 2011 <!-- , 6:00 AM EDT --> }}</ref><ref>{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = Apr 13, 2011<!-- 3:25 PM-->|publisher = CBC News}}</ref>\u000a\u000aThe term "hashtag [[Hip hop music|rap]]", coined by [[Kanye West]],<ref>{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West's Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}</ref> was developed in the 2010s to describe a style of rapping which, according to Rizoh of ''[[Houston Press]]'', uses "three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme".<ref>{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = Jul 7, 2011 <!-- at 9:00 AM --> }}</ref> Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]] and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]]<ref>{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 <!-- AT 11:43 AM --> |publisher = Tucson Weekly}}</ref> and various music writers.<ref>{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010's lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}</ref>\u000a\u000aOn September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a ''[[The New York Times|New York Times]]'' front page article regarding Twitter's [[initial public offering]].<ref>\u000a{{cite web \u000a| title = Twitter / nickbilton: My first byline on A1 of the ... \u000a| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1 \u000a| accessdate = 2013-09-14 \u000a }}\u000a</ref>\u000a\u000a"Hashtag [[Heel (professional wrestling)|heel]]" is a moniker used by [[WWE]] wrestler [[Dolph Ziggler]].\u000a\u000a[[Bird's Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called "Mashtags".<ref>{{cite web|title=Birds Eye launches Mashtags - social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}</ref>\u000a\u000aIn May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].<ref name="Nytimes">{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&_r=0 }}</ref>\u000a\u000aIn September 2014, in response to the "[[blame the victim]]" public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fiancée Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.<ref>{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}</ref><ref>{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard & Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}</ref>\u000a\u000a===Adaptations===\u000aIn 2010, Twitter introduced "hashflags" during the 2010 World Cup in South Africa.<ref>{{cite web|author=June 11, 2010 8:05 am |url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=2010-06-11 |accessdate=2014-08-25}}</ref> They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil.<ref>{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=2014-06-10 |accessdate=2014-08-25}}</ref><ref>{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=2014-06-10 |accessdate=2014-08-25}}</ref> When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.\u000a\u000aIn July 2012, Twitter adapted the hashtag style to make company [[ticker symbol]]s preceded by the [[dollar sign]] clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the "cashtag".<ref>{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils 'cashtags' to track stock symbols - Jul. 31, 2012 |publisher=Money.cnn.com |date=2012-07-31 |accessdate=2013-11-12}}</ref><ref>{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ 'cashtag' links official, following # and @ |publisher=The Verge |date=2012-07-30 |accessdate=2013-11-12}}</ref> This is intended to allow users to search posts discussing companies and their stocks.\u000a\u000aIn August 2012, British journalist Tom Meltzer reported in ''[[The Guardian]]'' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the "finger hashtag", in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.<ref>{{cite web |url=http://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say 'hashtag' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=1 August 2012 |accessdate=March 20, 2014}}</ref> The emerging gesture was reported about in ''[[Wired (magazine)|Wired]]'' by [[Nimrod Kamer]],<ref>{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}</ref> and during 2013 it was seen on TV used by [[Jimmy Fallon]], and on ''[[The Colbert Report]]'' among other places.<ref>{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtags\u2014and I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}</ref>\u000a\u000a==References==\u000a{{Reflist|colwidth=30em}}\u000a{{commons category|Hashtags}}\u000a\u000a{{Microblogging}}\u000a{{Online social networking}}\u000a{{Web syndication}}\u000a\u000a[[Category:Hashtags| ]]\u000a[[Category:Collective intelligence]]\u000a[[Category:Computer jargon]]\u000a[[Category:Information retrieval]]\u000a[[Category:Knowledge representation]]\u000a[[Category:Metadata]]\u000a[[Category:Reference]]\u000a[[Category:Web 2.0]]\u000a[[Category:Social media]]\u000a[[Category:2010s slang]]
p78
sg6
S'Hashtag'
p79
ssI29
(dp80
g2
S'http://en.wikipedia.org/wiki/Information needs'
p81
sg4
V'''Information need''' is an individual or group's desire to locate and obtain [[information]] to satisfy a conscious or unconscious [[need]]. The \u2018information\u2019 and \u2018need\u2019 in \u2018information need\u2019 are an inseparable interconnection. Needs and interests call forth information. The objectives of studying information needs are:\u000a# The explanation of observed phenomena of information use or expressed need;\u000a# The prediction of instances of information uses;\u000a# The control and thereby improvement of the utilization of information manipulation of essentials conditions.\u000a\u000aInformation needs are related to, but distinct from [[information requirements]].  An example is that a need is hunger; the requirement is food.\u000a\u000a== Background ==\u000a\u000aThe concept of information needs was coined by an American information gernalist [http://www.libsci.sc.edu/BOB/ISP/taylor2.htm Robert S. Taylor] in his article [http://doi.wiley.com/10.1002/asi.5090130405 "The Process of Asking Questions"] published in American Documentation (Now is Journal of the American Society of Information Science and Technology).\u000a\u000aIn this paper, Taylor attempted to describe how an inquirer obtains an answer from an [[information system]], by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system.\u000a\u000aAccording to Taylor, information need has four levels:\u000a# The conscious and unconscious need for information not existing in the remembered experience of the investigator. In terms of the query range, this level might be called the \u201cideal question\u201d \u2014 the question which would bring from the ideal system exactly what the inquirer, if he could state his need. It is the actual, but unexpressed, need for information\u000a# The conscious mental description of an ill-defined area of in decision. In this level, the inquirer might talk to someone else in the field to get an answer.\u000a# A researcher forms a rational statement of his question. This statement is a rational and unambiguous description of the inquirer\u2019s doubts.\u000a# The question as presented to the information system.\u000a\u000aThere are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback).\u000a\u000aHerbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles:\u000a# When approached from the point of view of the scientist or technologists, these are studies of scientists\u2019 communication behaviour;\u000a# When approached from the point of view of any communication medium, they are use studies;\u000a# When approached from the science communication system, they are studies in the flow of information among scientists and technologists.\u000a\u000aWilliam J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are:\u000a# The scientist within his culture.\u000a# The scientist within a political system.\u000a# The scientist within a membership group.\u000a# The scientist within a reference group.\u000a# The scientist within an invisible college.\u000a# The scientist within a formal organization.\u000a# The scientist within a work team.\u000a# The scientist within his own head.\u000a# The scientist within a legal/economical system.\u000a# The scientist within a formal.\u000a\u000a==See also==\u000a* [[Information retrieval]]\u000a\u000a==References==\u000a* Menzel, Herbert. \u201cInformation Needs and Uses in Science and Technology.\u201d Annual Review of Information Science and Technology, Vol. 1, Interscience Publishers 1966, pp 41-69.\u000a* Paisley, William J. \u201cInformation Needs and Uses.\u201d Annual Review of Information Science and Technology, Vol.3, Encyclopædia Britannica, Inc. Chicago 1968, pp.1-30.\u000a* Taylor, Robert S. \u201cThe Process of Asking Questions\u201d American Documentation, Vol.13, No. 4, October 1962, pp.391-396, DOI: 10.1002/asi.5090130405.\u000a* Wilson, T.D. \u201cOn User Studies and Information Needs.\u201d Journal of Documentation, Vol. 37, No. 1, 1981, pp.3-15\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]
p82
sg6
S'Information needs'
p83
ssI158
(dp84
g2
S'http://en.wikipedia.org/wiki/Category:Deep Web'
p85
sg4
S'{{cat main|Deep Web}}\n\n\n[[Category:Information retrieval]]\n[[Category:World Wide Web]]'
p86
sg6
S'Category:Deep Web'
p87
ssI32
(dp88
g2
S'http://en.wikipedia.org/wiki/IR evaluation'
p89
sg4
S"{{multiple issues|\n{{Orphan|date=February 2009}}\n{{Unreferenced|date=March 2008}}\n}}\n\n== IR Evaluation ==\nThe evaluation of information retrieval system is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].\n\n*'''Precision''' is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:\n\n:<math> \\mbox{precision}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{retrieved documents}\\}|} </math>\n\n*'''Recall''' is the fraction of the documents relevant to the query that are successfully retrieved:\n\n:<math> \\mbox{recall}=\\frac{|\\{\\mbox{relevant documents}\\}\\cap\\{\\mbox{retrieved documents}\\}|}{|\\{\\mbox{relevant documents}\\}|} </math>\n\n*'''F-measure''' is the harmonic mean of precision and recall:\n\n:<math> \\mbox{F-measure}= 2 * \\frac{\\{\\mbox{precision}\\}*\\{\\mbox{recall}\\}}{\\{\\mbox{precision}\\}+\\{\\mbox{recall}\\}} </math>\n\nFor modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.\n\nF-measure tends to be a better single metric when compared to precision and recall because both of them give different information that can complement each other when combined. If one of them excels more than the other, this metric will reflect it.\n\nVirtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for '''ranked retrieval''' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.\n\n==See also==\n* [[Information retrieval]]\n* [[Precision and recall]]\n* [[Web search engine]]\n\n==Further reading==\n* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.\n*Stefan B&uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\n\n[[Category:Information retrieval]]\n[[Category:Searching]]"
p90
sg6
S'IR evaluation'
p91
ssI161
(dp92
g2
S'http://en.wikipedia.org/wiki/Web search engine'
p93
sg4
V{{Redirect|Search engine}}\u000a{{selfref|For a tutorial on using search engines for researching Wikipedia articles, see [[Wikipedia:Search engine test]].}}\u000a[[File:Internet Key Layers.png|thumb|400px|right|Finding information on the World Wide Web had been a difficult and frustrating task, but became much more usable with breakthroughs in search engine technology in the late 1990s.]]\u000aA '''web search engine''' is a software system that is designed to search for information on the [[World Wide Web]].  The search results are generally presented in a line of results often referred to as [[search engine results pages]] (SERPs). The information may be a mix of [[web page]]s, images, and other types of files. Some search engines also [[data mining|mine data]] available in [[database]]s or [[web directory|open directories]].  Unlike [[web directories]], which are maintained only by human editors, search engines also maintain [[real-time computing|real-time]] information by running an [[algorithm]] on a [[web crawler]].\u000a\u000a== History ==\u000a{{further|Timeline of web search engines}}\u000a<!-- Keep this list limited to notable engines (i.e. those that already have Wikipedia articles) to avoid link spam -->\u000a{| class="bordered infobox"\u000a|-\u000a! colspan="3" | Timeline ([[List of search engines|full list]]) <!--Note:  "Launch" refers only to web availability of original crawl-based web search engine results.-->\u000a|-\u000a!Year\u000a!Engine\u000a!Current status\u000a|-\u000a| rowspan="4" |1993\u000a||[[W3Catalog]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Aliweb]]\u000a|{{Site inactive}}\u000a|-\u000a||[[JumpStation]]\u000a|{{Site inactive}}\u000a|-\u000a||[[World-Wide Web Worm|WWW Worm]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="4" |1994\u000a||[[WebCrawler]]\u000a|{{Site active}}, Aggregator\u000a|-\u000a||[[Go.com]]\u000a|{{Site active}}, Yahoo Search\u000a|-\u000a||[[Lycos]]\u000a|{{Site active}}\u000a|-\u000a||[[Infoseek]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="6" |1995\u000a||[[AltaVista]]\u000a|{{Site inactive}}, redirected to Yahoo!\u000a|-\u000a|[[Daum Communications|Daum]]\u000a|{{Site active}}\u000a|-\u000a||[[Magellan (search engine)|Magellan]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Excite]]\u000a|{{Site active}}\u000a|-\u000a||[[SAPO (company)|SAPO]]\u000a|{{Site active}}\u000a|-\u000a||[[Yahoo!]]\u000a|{{Site active}}, Launched as a directory\u000a|-\u000a| rowspan="4" |1996\u000a||[[Dogpile]]\u000a|{{Site active}}, Aggregator\u000a|-\u000a||[[Inktomi (company)|Inktomi]]\u000a|{{Site inactive}}, acquired by Yahoo!\u000a|-\u000a||[[HotBot]]\u000a|{{Site active}}  (lycos.com)\u000a|-\u000a||[[Ask.com|Ask Jeeves]]\u000a|{{Site active}}  (rebranded ask.com)\u000a|-\u000a| rowspan="2" |1997\u000a||[[Northern Light Group|Northern Light]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Yandex]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="4" |1998\u000a||[[Google Search|Google]]\u000a|{{Site active}}\u000a|-\u000a||[[Ixquick]]\u000a|{{Site active}}  also as Startpage\u000a|-\u000a||[[MSN Search]]\u000a|{{Site active}}  as Bing\u000a|-\u000a||[[empas]]\u000a|{{Site inactive}}  (merged with NATE)\u000a|-\u000a| rowspan="5" |1999\u000a||[[AlltheWeb]]\u000a|{{Site inactive}}  (URL redirected to Yahoo!)\u000a|-\u000a||[[GenieKnows]]\u000a|{{Site active}}, rebranded Yellowee.com\u000a|-\u000a||[[Naver]]\u000a|{{Site active}}\u000a|-\u000a||[[Teoma]]\u000a|{{Site inactive}}, redirects to Ask.com\u000a|-\u000a||[[Vivisimo]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="3" |2000\u000a||[[Baidu]]\u000a|{{Site active}}\u000a|-\u000a||[[Exalead]]\u000a|{{Site active}}\u000a|-\u000a||[[Gigablast]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="2" |2003\u000a||[[Info.com]]\u000a|{{Site active}}\u000a|-\u000a||[[Scroogle]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="3" |2004\u000a||[[Yahoo! Search]]\u000a|{{Site active}}, Launched own web search<br />(see Yahoo! Directory, 1995)\u000a|-\u000a||[[A9.com]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Sogou.com|Sogou]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="3" |2005\u000a||[[AOL Search]]\u000a|{{Site active}}\u000a|-\u000a||[[GoodSearch]]\u000a|{{Site active}}\u000a|-\u000a||[[SearchMe]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="6" |2006\u000a||[[Soso (search engine)]]\u000a|{{Site active}}\u000a|-\u000a||[[Quaero]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Ask.com]]\u000a|{{Site active}}\u000a|-\u000a||[[Live Search]]\u000a|{{Site active}} as Bing, Launched as<br />rebranded MSN Search\u000a|-\u000a||[[ChaCha (search engine)|ChaCha]]\u000a|{{Site active}}\u000a|-\u000a||[[Guruji.com]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="4" |2007\u000a||[[wikiseek]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Sproose]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Wikia Search]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Blackle.com]]\u000a|{{Site active}}, Google Search\u000a|-\u000a| rowspan="7" |2008\u000a||[[Powerset (company)|Powerset]]\u000a|{{Site inactive}} (redirects to Bing)\u000a|-\u000a||[[Picollator]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Viewzi]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Boogami]]\u000a|{{Site inactive}}\u000a|-\u000a||[[LeapFish]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Forestle]]\u000a|{{Site inactive}} (redirects to Ecosia)\u000a|-\u000a||[[DuckDuckGo]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="5" |2009\u000a||[[Bing]]\u000a|{{Site active}}, Launched as<br />rebranded Live Search\u000a|-\u000a||[[Yebol]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Mugurdy]]\u000a|{{Site inactive}}  due to a lack of funding\u000a|-\u000a||[[Goby Inc.|Scout (Goby)]]\u000a|{{Site active}}\u000a|-\u000a||[[Nate (web portal)|NATE]]\u000a|{{Site active}}\u000a|-\u000a| rowspan="3" |2010\u000a||[[Blekko]]\u000a|{{Site active}}\u000a|-\u000a||[[Cuil]]\u000a|{{Site inactive}}\u000a|-\u000a||[[Yandex]]\u000a|{{Site active}}, Launched global<br />(English) search\u000a|-\u000a||2011\u000a||[[YaCy]]\u000a|{{Site active}}, [[Peer-to-peer|P2P]] web search engine\u000a|-\u000a| rowspan="1" |2012\u000a||[[Volunia]]\u000a|{{Site inactive}}\u000a|-\u000a| rowspan="1" |2013\u000a||[[Halalgoogling]]\u000a|{{Site active}}, Islamic / Halal<br />filter Search\u000a|}\u000a\u000aDuring early development of the web, there was a list of [[webserver]]s edited by [[Tim Berners-Lee]] and hosted on the [[CERN]] webserver. One historical snapshot of the list in 1992 remains,<ref>{{cite web|url=http://www.w3.org/History/19921103-hypertext/hypertext/DataSources/WWW/Servers.html |title=World-Wide Web Servers |publisher=W3.org |accessdate=2012-05-14}}</ref> but as more and more webservers went online the central list could no longer keep up. On the [[National Center for Supercomputing Applications|NCSA]]  site, new servers were announced under the title "What's New!"<ref>{{cite web|url=http://home.mcom.com/home/whatsnew/whats_new_0294.html |title=What's New! February 1994 |publisher=Home.mcom.com |accessdate=2012-05-14}}</ref>\u000a\u000aThe first tool used for searching on the [[Internet]] was [[Archie search engine|Archie]].<ref name=LeidenUnivSE>\u000a     "Internet History - Search Engines" (from [[Search Engine Watch]]),\u000a     Universiteit Leiden, Netherlands, September 2001, web:\u000a     [http://www.internethistory.leidenuniv.nl/index.php3?c=7 LeidenU-Archie].\u000a</ref>\u000aThe name stands for "archive" without the "v".  It was created in 1990 by [[Alan Emtage]], Bill Heelan and J. Peter Deutsch, computer science students at [[McGill University]]  in [[Montreal]]. The program downloaded the directory listings of all the files located on public anonymous FTP ([[File Transfer Protocol]]) sites, creating a searchable database of file names; however, Archie did not index the contents of these sites since the amount of data was so limited it could be readily searched manually.\u000a\u000aThe rise of [[Gopher (protocol)|Gopher]] (created in 1991 by [[Mark McCahill]]  at the [[University of Minnesota]]) led to two new search programs, [[Veronica (computer)|Veronica]]  and [[Jughead (computer)|Jughead]]. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (''V''ery ''E''asy ''R''odent-''O''riented ''N''et-wide ''I''ndex to ''C''omputerized ''A''rchives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (''J''onzy's ''U''niversal ''G''opher ''H''ierarchy ''E''xcavation ''A''nd ''D''isplay) was a tool for obtaining menu information from specific Gopher servers.  While the name of the search engine "Archie" was not a reference to the [[Archie Comics|Archie comic book]] series, "[[Veronica Lodge|Veronica]]" and "[[Jughead Jones|Jughead]]" are characters in the series, thus referencing their predecessor.\u000a\u000aIn the summer of 1993, no search engine existed for the web, though numerous specialized catalogues were maintained by hand. [[Oscar Nierstrasz]] at the [[University of Geneva]] wrote a series of [[Perl]] scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for [[W3Catalog]], the web's first primitive search engine, released on September 2, 1993.<ref name="Announcement html">{{cite web |url= http://groups.google.com/group/comp.infosystems.www/browse_thread/thread/2176526a36dc8bd3/2718fd17812937ac?hl=en&lnk=gst&q=Oscar+Nierstrasz#2718fd17812937ac|title=Searchable Catalog of WWW Resources (experimental)|author=[[Oscar Nierstrasz]]|date=2 September 1993}}</ref>\u000a\u000aIn June 1993, Matthew Gray, then at [[Massachusetts Institute of Technology|MIT]], produced what was probably the first [[web robot]], the [[Perl]]-based [[World Wide Web Wanderer]], and used it to generate an index called 'Wandex'.  The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995.  The web's second search engine [[Aliweb]] appeared in November 1993.  Aliweb did not use a [[web robot]], but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format.\u000a\u000a[[JumpStation]] (created in December 1993<ref>{{cite web|url=http://archive.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/old-whats-new/whats-new-1293.html |archiveurl=//web.archive.org/web/20010620073530/http://archive.ncsa.uiuc.edu/SDG/Software/Mosaic/Docs/old-whats-new/whats-new-1293.html |archivedate=2001-06-20 |title=Archive of NCSA what's new in December 1993 page |publisher=Web.archive.org |date=2001-06-20 |accessdate=2012-05-14}}</ref> by [[Jonathon Fletcher]]) used a [[web crawler|web robot]] to find web pages and to build its index, and used a [[web form]] as the interface to its query program.  It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below.  Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered.\u000a\u000aOne of the first "all text" crawler-based search engines was [[WebCrawler]], which came out in 1994.  Unlike its predecessors, it allowed users to search for any word in any webpage, which has become the standard for all major search engines since. It was also the first one widely known by the public.  Also in 1994, [[Lycos]] (which started at [[Carnegie Mellon University]]) was launched and became a major commercial endeavor.\u000a\u000aSoon after, many search engines appeared and vied for popularity. These included [[Magellan (search engine)|Magellan]], [[Excite]], [[Infoseek]], [[Inktomi (company)|Inktomi]], [[Northern Light Group|Northern Light]], and [[AltaVista]]. [[Yahoo!]] was among the most popular ways for people to find web pages of interest, but its search function operated on its [[web directory]], rather than its full-text copies of web pages. Information seekers could also browse the directory instead of doing a keyword-based search.\u000a\u000aGoogle adopted the idea of selling search terms in 1998, from a small search engine company named [[goto.com]]. This move had a significant effect on the SE business, which went from struggling to one of the most profitable businesses in the internet.<ref>http://www.udacity.com/view#Course/cs101/CourseRev/apr2012/Unit/616074/Nugget/671097</ref>\u000a\u000aIn 1996, [[Netscape]] was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page.  The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.<ref>{{cite web|title=Yahoo! And Netscape Ink International Distribution Deal|url=http://files.shareholder.com/downloads/YHOO/701084386x0x27155/9a3b5ed8-9e84-4cba-a1e5-77a3dc606566/YHOO_News_1997_7_8_General.pdf|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref><ref>{{Cite journal |date=1 April 1996|title=Browser Deals Push Netscape Stock Up 7.8% |publisher=Los Angeles Times |url=http://articles.latimes.com/1996-04-01/business/fi-53780_1_netscape-home |postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>\u000a\u000aSearch engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.<ref>{{cite journal |last=Gandal |first=Neil |authorlink= |year=2001 |title=The dynamics of competition in the internet search engine market |journal=International Journal of Industrial Organization |volume=19 |issue=7 |pages=1103\u20131117 |doi=10.1016/S0167-7187(01)00065-0  |url= |accessdate=|quote= }}</ref> Several companies entered the market spectacularly, receiving record gains during their [[initial public offering]]s. Some have taken down their public search engine, and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the [[dot-com bubble]], a speculation-driven market boom that peaked in 1999 and ended in 2001.\u000a\u000aAround 2000, [[Google Search|Google's search engine]] rose to prominence.<ref>{{cite web|url=http://www.google.com/about/company/history/ |title=Our History in depth |publisher=W3.org |accessdate=2012-10-31}}</ref>  The company achieved better results for many searches with an innovation called [[PageRank]], as was explained in the paper ''Anatomy of a Search Engine'' written by [[Sergey Brin]] and [[Larry Page]], the later founders of Google.<ref>{{cite web|url=http://ilpubs.stanford.edu:8090/361/1/1998-8.pdf|title=The Anatomy of a Large-Scale Hypertextual Web Search Engine|last1=Brin|first1=Sergey|last2=Page|first2=Larry}}</ref> This [[iterative algorithm]] ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a [[web portal]]. In fact, Google search engine became so popular that spoof engines emerged such as [[Mystery Seeker]].\u000a\u000aBy 2000, [[Yahoo!]] was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and [[Overture]] (which owned [[AlltheWeb]] and AltaVista) in 2003.  Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions.\u000a\u000a[[Microsoft]] first launched MSN Search in the fall of 1998 using search results from Inktomi.  In early 1999 the site began to display listings from [[Looksmart]], blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista were instead.  In 2004, [[Microsoft]] began a transition to its own search technology, powered by its own [[web crawler]] (called [[msnbot]]).\u000a\u000aMicrosoft's rebranded search engine, [[Bing]], was launched on June 1, 2009.  On July 29, 2009, Yahoo! and Microsoft finalized a deal in which [[Yahoo! Search]] would be powered by Microsoft Bing technology.\u000a\u000a== How web search engines work ==\u000a{{Original research|section|date=October 2013\u000a}}\u000a{{Refimprove|date=July 2013}}\u000aA search engine operates in the following order:\u000a# [[Web crawling]]\u000a# [[Index (search engine)|Indexing]]\u000a# [[Web search query|Searching]]<ref name=Jawadekar2011>{{citation |year=2011 |author=Jawadekar, Waman S |title=Knowledge Management: Text & Cases |url=http://books.google.com/books?id=XmGx4J9daUMC&printsec=frontcover&dq=knowledge+management:+text&hl=en&sa=X&ei=ou6uUP-cNqWTiAe2oICoAw&sqi=2&ved=0CDIQ6AEwAA |chapter=8. Knowledge Management: Tools and Technology |chapter-url=http://books.google.com/books?id=XmGx4J9daUMC&pg=PA278&dq=%22search+engine+operates%22&hl=en&sa=X&ei=a-muUJ6UC4aeiAfI24GYAw&sqi=2&ved=0CDgQ6AEwBA |page=278 |place=New Delhi |publisher=Tata McGraw-Hill Education Private Ltd |isbn=978-0-07-07-0086-4 |accessdate=November 23, 2012 }}</ref>\u000a\u000aWeb search engines work by storing information about many web pages, which they retrieve from the [[HTML]] markup of the pages. These pages are retrieved by a [[Web crawler]] (sometimes also known as a spider) \u2014 an automated Web crawler which follows every link on the site. The site owner can exclude specific pages by using [[robots.txt]].\u000a\u000aThe search engine then analyzes the contents of each page to determine how it should be [[Search engine indexing|indexed]] (for example, words can be extracted from the titles, page content, headings, or special fields called [[meta tags]]). Data about web pages are stored in an index database for use in later queries. A query from a user can be a single word. The index helps find information relating to the query as quickly as possible.<ref name=Jawadekar2011/> Some search engines, such as [[Google]], store all or part of the source page (referred to as a [[web cache|cache]]) as well as information about the web pages, whereas others, such as [[AltaVista]], store every word of every page they find.{{Citation needed|date=November 2012}} This cached page always holds the actual search text since it is the one that was actually indexed, so it can be very useful when the content of the current page has been updated and the search terms are no longer in it.<ref name=Jawadekar2011/> This problem might be considered a mild form of [[linkrot]], and Google's handling of it increases [[usability]] by satisfying [[user expectations]] that the search terms will be on the returned webpage. This satisfies the [[principle of least astonishment]], since the user normally expects that the search terms will be on the returned pages. Increased search relevance makes these cached pages very useful as they may contain data that may no longer be available elsewhere.{{Citation needed|date=November 2012}}\u000a[[File:WebCrawlerArchitecture.svg|thumb|High-level architecture of a standard Web crawler]]\u000aWhen a user enters a [[web search query|query]] into a search engine (typically by using [[Keyword (Internet search)|keywords]]), the engine examines its [[inverted index|index]] and provides a listing of best-matching web pages according to its criteria, usually with a short summary containing the document's title and sometimes parts of the text. The index is built from the information stored with the data and the method by which the information is indexed.<ref name=Jawadekar2011/> From 2007 the Google.com search engine has allowed one to search by date by clicking "Show search tools" in the leftmost column of the initial search results page, and then selecting the desired date range.{{Citation needed|date=November 2012}} Most search engines support the use of the [[boolean operators]] AND, OR and NOT to further specify the [[web search query|search query]]. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered.  Some search engines provide an advanced feature called [[Proximity search (text)|proximity search]], which allows users to define the distance between keywords.<ref name=Jawadekar2011/>  There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases you search for. As well, natural language queries allow the user to type a question in the same form one would ask it to a human. A site like this would be ask.com.{{Citation needed|date=November 2012}}\u000a\u000aThe usefulness of a search engine depends on the [[relevance (information retrieval)|relevance]] of the '''result set''' it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to [[rank order|rank]] the results to provide the "best" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.<ref name=Jawadekar2011/> The methods also change over time as Internet usage changes and new techniques evolve.  There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an "[[inverted index]]" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work.\u000a\u000aMost Web search engines are commercial ventures supported by [[advertising]] revenue and thus some of them allow advertisers to [[paid inclusion|have their listings ranked higher]] in search results for a fee. Search engines that do not accept money for their search results make money by running [[contextual advertising|search related ads]] alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.<ref>{{cite web|title=FAQ|url=http://www.rankstar.de/hilfe.html|publisher=RankStar|accessdate=19 June 2013}}</ref>\u000a\u000a== Market share ==\u000a\u000a[[Google Search|Google]] is the world's most popular search engine, with a marketshare of 66.44 percent as of December, 2014.<ref name="NMS">{{cite web|url=http://marketshare.hitslink.com/search-engine-market-share.aspx?qprid=4&qpcustomd=0&qpcustom=|title=Desktop Search Engine Market Share|publisher=NetMarketShare|accessdate=2014-06-04}}</ref> [[Baidu]] comes in at second place.<ref name="NMS" />\u000a\u000aThe world's most popular search engines are:<ref>{{cite web|title=FAQ|url=https://www.netmarketshare.com/search-engine-market-share.aspx?qprid=4&qpcustomd=0|publisher=NetMarketShare|accessdate=23 November 2014}}</ref>\u000a\u000a{| class="wikitable sortable"\u000a! Search engine !! colspan="2" |Market share in December 2014\u000a|-\u000a| [[Google Search|Google]] || style="text-align:right;"|{{bartable|66.44|%|2}}\u000a|-\u000a| [[Baidu]]  || style="text-align:right;"|{{bartable| 11.15|%|2}}\u000a|-\u000a| [[Bing]]   || style="text-align:right;"|{{bartable| 10.29|%|2}}\u000a|-\u000a| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 9.31|%|2}}\u000a|-\u000a| [[AOL]]    || style="text-align:right;"|{{bartable| 0.53|%|2}}\u000a|-\u000a| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.21|%|2}}\u000a|-\u000a| [[Lycos]]   || style="text-align:right;"|{{bartable| 0.01|%|2}}\u000a|}\u000a\u000a{| class="wikitable sortable"\u000a! Search engine !! colspan="2" |Market share in October 2014\u000a|-\u000a| [[Google Search|Google]] || style="text-align:right;"|{{bartable|58.01|%|2}}\u000a|-\u000a| [[Baidu]]  || style="text-align:right;"|{{bartable| 29.06|%|2}}\u000a|-\u000a| [[Bing]]   || style="text-align:right;"|{{bartable| 8.01|%|2}}\u000a|-\u000a| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 4.01|%|2}}\u000a|-\u000a| [[AOL]]    || style="text-align:right;"|{{bartable| 0.21|%|2}}\u000a|-\u000a| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.10|%|2}}\u000a|-\u000a| [[Excite]]   || style="text-align:right;"|{{bartable| 0.00|%|2}}\u000a|}\u000a\u000a{| class="wikitable sortable"\u000a! Search engine !! colspan="2" |Market share in July 2014<ref name="NMS" />\u000a|-\u000a| [[Google Search|Google]] || style="text-align:right;"|{{bartable|68.69|%|2}}\u000a|-\u000a| [[Baidu]]  || style="text-align:right;"|{{bartable| 17.17|%|2}}\u000a|-\u000a| [[Yahoo!]]  || style="text-align:right;"|{{bartable| 6.74|%|2}}\u000a|-\u000a| [[Bing]]   || style="text-align:right;"|{{bartable| 6.22|%|2}}\u000a|-\u000a| [[Excite]]   || style="text-align:right;"|{{bartable| 0.22|%|2}}\u000a|-\u000a| [[Ask.com|Ask]]    || style="text-align:right;"|{{bartable| 0.13|%|2}}\u000a|-\u000a| [[AOL]]    || style="text-align:right;"|{{bartable| 0.13|%|2}}\u000a|}\u000a\u000a=== East Asia and Russia ===\u000a\u000aEast Asian countries and Russia constitute a few places where Google is not the most popular search engine.\u000a\u000a[[Yandex]] commands a marketshare of 61.9 per cent in Russia, compared to Google's 28.3 percent.<ref>{{cite web|url=http://www.liveinternet.ru/stat/ru/searches.html?slice=ru;period=week|title=Live Internet - Site Statistics|publisher=Live Internet|accessdate=2014-06-04}}</ref> In China, Baidu is the most popular search engine.<ref>{{cite news|url=http://www.theguardian.com/world/2014/jun/03/chinese-technology-companies-huawei-dominate-world|title=The Chinese technology companies poised to dominate the world|publisher=The Guardian|author=Arthur, Charles|date=2014-06-03|accessdate=2014-06-04}}</ref>  South Korea's homegrown search portal, [[Naver]], is used for 70 per cent online searches in the country.<ref>{{cite web|url=http://blogs.wsj.com/korearealtime/2014/05/21/how-naver-hurts-companies-productivity/|title=How Naver Hurts Companies\u2019 Productivity|publisher=The Wall Street Journal|date=2014-05-21|accessdate=2014-06-04}}</ref> [[Yahoo! Japan]] and [[Yahoo! Search|Yahoo! Taiwan]] are the most popular avenues for internet search in Japan and Taiwan, respectively.<ref>{{cite web|url=http://geography.oii.ox.ac.uk/?page=age-of-internet-empires|title=Age of Internet Empires|publisher=Oxford Internet Institute|accessdate=2014-06-04}}</ref>\u000a\u000a== Search engine bias ==\u000aAlthough search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide.<ref>Segev, El (2010). Google and the Digital Divide: The Biases of Online Knowledge, Oxford: Chandos Publishing.</ref><ref name=vaughan-thelwall>{{cite journal|last=Vaughan|first=Liwen|author2=Mike Thelwall |title=Search engine coverage bias: evidence and possible causes|journal=Information Processing & Management|year=2004|volume=40|issue=4|pages=693\u2013707|doi=10.1016/S0306-4573(03)00063-3}}</ref> These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its [[organic search]] results), and political processes (e.g., the removal of search results to comply with local laws).<ref>Berkman Center for Internet & Society (2002), [http://cyber.law.harvard.edu/filtering/china/google-replacements/ \u201cReplacement of Google with Alternative Search Systems in China: Documentation and Screen Shots\u201d], Harvard Law School.</ref> For example, Google will not surface certain Neo-Nazi websites in France and Germany, where Holocaust denial is illegal.\u000a\u000aBiases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more "popular" results.<ref>{{cite journal|last=Introna|first=Lucas|author2=[[Helen Nissenbaum]] |title=Shaping the Web: Why the Politics of Search Engines Matters|journal=The Information Society: An International Journal|year=2000|volume=16|issue=3|doi=10.1080/01972240050133634}}</ref> Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.<ref name=vaughan-thelwall />\u000a\u000a[[Google Bombing]] is one example of an attempt to manipulate search results for political, social or commercial reasons.\u000a\u000a== Customized results and filter bubbles ==\u000a\u000aMany search engines such as Google and Bing provide customized results based on the user's activity history. This leads to an effect that has been called a [[filter bubble]]. The term describes a phenomenon in which websites use [[algorithm]]s to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint, effectively isolating the user in a bubble that tends to exclude contrary information. Prime examples are Google's personalized search results and [[Facebook]]'s personalized news stream. According to [[Eli Pariser]], who coined the term, users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Pariser related an example in which one user searched Google for "BP" and got investment news about [[British Petroleum]] while another searcher got information about the [[Deepwater Horizon oil spill]] and that the two search results pages were "strikingly different".<ref name=twsT43>{{cite news\u000a |first1= Lynn | last1= Parramore\u000a |title= The Filter Bubble\u000a |work= The Atlantic\u000a |quote= Since Dec. 4, 2009, Google has been personalized for everyone. So when I had two friends this spring Google "BP," one of them got a set of links that was about investment opportunities in BP. The other one got information about the oil spill....\u000a |date=  10 October 2010\u000a |url= http://www.theatlantic.com/daily-dish/archive/2010/10/the-filter-bubble/181427/\u000a |accessdate= 2011-04-20\u000a}}</ref><ref name=twsO11>{{cite news\u000a |first= Jacob | last= Weisberg\u000a |title= Bubble Trouble: Is Web personalization turning us into solipsistic twits?\u000a |work= Slate\u000a |date= 10 June 2011\u000a |url= http://www.slate.com/id/2296633/\u000a |accessdate= 2011-08-15\u000a}}</ref><ref name=twsO14>{{cite news\u000a |first= Doug | last= Gross\u000a |title= What the Internet is hiding from you\u000a |publisher= ''CNN''\u000a |quote= I had friends Google BP when the oil spill was happening. These are two women who were quite similar in a lot of ways. One got a lot of results about the environmental consequences of what was happening and the spill. The other one just got investment information and nothing about the spill at all.\u000a |date= May 19, 2011\u000a |url= http://edition.cnn.com/2011/TECH/web/05/19/online.privacy.pariser/\u000a |accessdate= 2011-08-15\u000a}}</ref> The bubble effect may have negative implications for civic discourse, according to Pariser.<ref>{{cite journal| last1= Zhang | first1= Yuan Cao | first2= Diarmuid Ó |last2= Séaghdha | first3= Daniele | last3= Quercia | first4 =Tamas | last4 = Jambor |title=Auralist: Introducing Serendipity into Music Recommendation|journal=ACM WSDM |date=February 2012|url=http://www-typo3.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_11_21.pdf}}</ref>\u000a\u000aSince this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or "bubbling" users.\u000a\u000a== Faith-based search engines ==\u000a\u000aThe global growth of the Internet and popularity of electronic contents in the [[Arab]] and [[Muslim]] World during the last decade has encouraged faith adherents, notably in [[Middle East|the Middle East]] and [[Indian subcontinent|Asian sub-continent]], to "dream" of their own faith-based i.e. "[[Islamic]]" search engines or filtered search portals filters that would enable users to avoid accessing forbidden websites such as pornography and would only allow them to access sites that are compatible to the Islamic faith. Shortly before the Muslim only month of [[Ramadan]], [[Halalgoogling]] which collects results from other search engines like [[Google]] and [[Bing]] was introduced to the world July 2013 to presents the [[halal]] results to its users,<ref>{{cite web|url=http://news.msn.com/science-technology/new-islam-approved-search-engine-for-muslims |title=New Islam-approved search engine for Muslims |publisher=News.msn.com |date= |accessdate=2013-07-11}}</ref> nearly two years after I\u2019mHalal, another search engine initially (launched on September 2011) to serve Middle East Internet had to close its search service due to what its owner blamed on lack of funding.<ref>[http://blog.imhalal.com/ I\u2019mHalal - Islamic compliant search project launched September 2009 and shut down late 2011]</ref>\u000a\u000aWhile lack of investment and slow pace in technologies in the Muslim World as the main consumers or targeted end users has hindered progress and thwarted success of serious Islamic search engine, the spectacular failure of heavily invested Muslim lifestyle web projects like [[Muxlim]], which received millions of dollars from investors like Rite Internet Ventures, has - according to I\u2019mHalal shutdown notice - made almost laughable the idea that the next [[Facebook]] or [[Google]] can only come from [[Middle East|the Middle East]] if you support your bright youth.<ref>[http://imhalal.com/ I'mHalal Blog]</ref> Yet Muslim internet experts have been determining for years what is or is not allowed according to [[Shariah|the "Law of Islam"]] and have been categorizing websites and such into being either "[[halal]]" or "[[haram]]". All the existing and past Islamic search engines are merely custom search indexed or monetized by web major search giants like [[Google]], [[Yahoo]] and [[Bing]] with only certain filtering systems applied to ensure that their users can't access Haram sites, which include such sites as nudity, gay, gambling or anything that is deemed to be anti-Islamic.<ref>[http://blog.imhalal.com/ I'mHalal Blog]</ref>\u000a\u000aAnother religiously-oriented search engine is Jewogle, which is the Jewish version of Google and yet another is SeekFind.org, which is a Christian website that includes filters preventing users from seeing anything on the internet that attacks or degrades their faith.<ref>[http://allchristiannews.com/halalgoogling-muslims-get-their-own-sin-free-google-should-christians-have-christian-google/ AllChristianNews]</ref>\u000a\u000a== See also ==\u000a*[[Most popular Internet search engines]]\u000a* [[Comparison of web search engines]]\u000a* [[List of search engines]]\u000a* Answer engine ([[question answering]]) <!-- examples necessary here until article comprehensible to normal reader-->\u000a** [[Quora]]\u000a** [[True Knowledge]]\u000a** [[Wolfram Alpha]]\u000a* [[Google effect]]\u000a* [[Internet Search Engines and Libraries]]\u000a* [[Semantic Web]]\u000a* [[Spell checker]]\u000a* [[Web development tools]]\u000a\u000a== References ==\u000a{{Reflist|33em}}\u000a\u000a== Further reading ==\u000a* For a more detailed history of early search engines, see [http://searchenginewatch.com/showPage.html?page=3071951 Search Engine Birthdays] (from [[Search Engine Watch]]), Chris Sherman, September 2003.\u000a* {{cite journal | quotes =| author =Steve Lawrence; C. Lee Giles | year =1999| title =Accessibility of information on the web | journal =[[Nature (journal)|Nature]] | volume =400 | issue =6740| doi =10.1038/21987 | pmid =10428673 | pages =107\u20139 }}\u000a* Bing Liu (2007), ''[http://www.cs.uic.edu/~liub/WebMiningBook.html Web Data Mining: Exploring Hyperlinks, Contents and Usage Data].'' Springer,ISBN 3-540-37881-2\u000a* Bar-Ilan, J. (2004). The use of Web search engines in information science research. ARIST, 38, 231-288.\u000a* {{cite book | first =Mark | last =Levene | year =2005 | title =An Introduction to Search Engines and Web Navigation | publisher =Pearson | location =| isbn =}}\u000a* {{cite book | first =Randolph | last =Hock | year =2007 | title =The Extreme Searcher's Handbook}}ISBN 978-0-910965-76-7\u000a* {{cite journal | quotes =| author =Javed Mostafa |date= February 2005 | title =Seeking Better Web Searches | journal =[[Scientific American]] | volume =| issue =| pages =| publisher =| pmid =| doi =| bibcode =| url =http://www.sciam.com/article.cfm?articleID=0006304A-37F4-11E8-B7F483414B7F0000 | language =}}<sup class="noprint Inline-Template"><span title="&nbsp;since September 2010" style="white-space: nowrap;">&#91;''&#93;</span></sup>\u000a* {{cite journal |last=Ross |first=Nancy |authorlink=|author2=Wolfram, Dietmar  |year=2000 |title=End user searching on the Internet: An analysis of term pair topics submitted to the Excite search engine |journal=Journal of the American Society for Information Science |volume=51 |issue=10 |pages=949\u2013958 |doi=10.1002/1097-4571(2000)51:10<949::AID-ASI70>3.0.CO;2-5|url=|accessdate=|quote=}}\u000a* {{cite journal |last=Xie |first=M. |authorlink=|year=1998 |title=Quality dimensions of Internet search engines |journal=Journal of Information Science |volume=24 |issue=5 |pages=365\u2013372 |doi=10.1177/016555159802400509 |url=|accessdate=|quote=|display-authors=1 |last2=Wang |first2=H. |last3=Goh |first3=T. N. }}\u000a*{{cite book|title=Information Retrieval: Implementing and Evaluating Search Engines|url= http://www.ir.uwaterloo.ca/book/ | year=2010|publisher=MIT Press|author8=Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack}}\u000a\u000a== External links ==\u000a{{commons category|Internet search engines}}\u000a* {{Dmoz|Computers/Internet/Searching/Search_Engines/|Search Engines}}\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Web Search Engine}}\u000a[[Category:Internet search engines| ]]\u000a[[Category:History of the Internet]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet terminology]]
p94
sg6
S'Web search engine'
p95
ssI35
(dp96
g2
S'http://en.wikipedia.org/wiki/Relevance (information retrieval)'
p97
sg4
V{{Other uses|Relevance}}\u000a\u000aIn [[information science]] and [[information retrieval]], '''relevance''' denotes how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.\u000a\u000a== History ==\u000a\u000aThe concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.\u000a\u000aThe formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term "relevant" to characterize articles relevant to a subject (cf., [[Bradford's law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.<ref>Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810\u2010832.</ref>\u000a\u000aSince 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".\u000a\u000aRecently, Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> showed a connection between the [[Binary Independence Model|relevance probability]] and the [[vocabulary mismatch]] problem in retrieval, which could lead to at least 50-300% gains in retrieval accuracy.<ref>Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>\u000a\u000a== Evaluation ==\u000a\u000aThe information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.\u000a\u000aIn order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a ''relevance level'' to each retrieved result, a process known as ''relevance assessment''. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system's output.\u000a\u000aIn contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance. These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).\u000a\u000a== Clustering and relevance ==\u000a\u000aThe [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.<ref name=diazthesis>F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.</ref>    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:\u000a* cluster-based information retrieval<ref name=croftcbir>W. B. Croft, \u201cA model of cluster searching based on classification,\u201d Information Systems, vol. 5, pp. 189\u2013195, 1980.</ref><ref name=griffithscbir>A. Griffiths, H. C. Luckhurst, and P. Willett, \u201cUsing interdocument similarity information in document retrieval systems,\u201d Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3\u201311, 1986.</ref>\u000a* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.<ref name=lmcbir>X. Liu and W. B. Croft, \u201cCluster-based retrieval using language models,\u201d in SIGIR \u201904: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186\u2013193, ACM Press, 2004.</ref>    It is important to ensure that clusters \u2013 either in isolation or combination \u2013 successfully model the set of possible relevant documents.\u000a\u000aA second interpretation, most notably advanced by Ellen Voorhees,<ref name=voorheescbir>E. M. Voorhees, \u201cThe cluster hypothesis revisited,\u201d in SIGIR \u201985: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188\u2013196, ACM Press, 1985.</ref>    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,\u000a* multiple cluster retrieval<ref name=griffithscbir/><ref name=voorheescbir/>\u000a* spreading activation<ref name=preece>S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.</ref> and relevance propagation<ref name=relprop>T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, \u201cA study of relevance propagation for web search,\u201d in SIGIR \u201905: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408\u2013415, ACM Press, 2005.</ref> methods\u000a* local document expansion<ref name=docexpansion>A. Singhal and F. Pereira, \u201cDocument expansion for speech retrieval,\u201d in SIGIR \u201999: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34\u201341, ACM Press, 1999.</ref>\u000a* score regularization<ref name=diazreg>F. Diaz, \u201cRegularizing query-based retrieval scores,\u201d Information Retrieval, vol. 10, pp. 531\u2013562, December 2007.</ref>\u000aLocal methods require an accurate and appropriate document similarity measure.\u000a\u000a==Epistemological issues==\u000a{{Section OR|date=May 2014}}\u000aAre users best at evaluating the relevance of a given document, or is it better to use experts?\u000aMost research about relevance in information retrieval in recent years have implicitly assumed that the users' evaluation of the output a given system should be used to increase "relevance" output. An alternative strategy would be to use journal [[impact factor]] to rank output and thus base relevance on expert evaluations. Other strategies, such as including diversity of the search results, may be used as well. The important thing to recognize is, however, that relevance is fundamentally a question of [[epistemology]], not [[psychology]]. (Peoples' psychology reflects certain epistemological influences).\u000a\u000a==References==\u000a {{reflist}}\u000a\u000a==Additional reading==\u000a*Hjørland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.\u000a\u000a*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9\u000a\u000a*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])\u000a\u000a*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])\u000a\u000a*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])\u000a\u000a[[Category:Information retrieval]]
p98
sg6
S'Relevance (information retrieval)'
p99
ssI164
(dp100
g2
S'http://en.wikipedia.org/wiki/Topic-based vector space model'
p101
sg4
S"The '''Topic-based Vector Space Model (TVSM)'''<ref>{{cite | url=http://www.kuropka.net/files/TVSM.pdf | title=Topic-based Vector Space Model | author=Dominik Kuropka | coauthors=Jorg Becker | year=2003}}</ref> (literature: [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id=]) extends the [[vector space model]] of [[information retrieval]] by removing the constraint that the term-vectors be orthogonal. The assumption of orthogonal terms is incorrect regarding natural languages which causes problems with synonyms and strong related terms. This facilitates the use of stopword lists, stemming and thesaurus in TVSM.\nIn contrast to the [[generalized vector space model]] the TVSM does not depend on concurrence-based similarities between terms. \n\n==Definitions==\nThe basic premise of TVSM is the existence of a ''d'' dimensional space ''R'' with only positive axis intercepts, i.e. ''R in R<sup>+</sup>'' and ''d in N<sup>+</sup>''. Each dimension of ''R'' represents a fundamental topic. A term vector ''t'' has a specific weight for a certain ''R''. To calculate these weights assumptions are made taking into account the document contents. Ideally important terms will have a high weight and stopwords and irrelevants terms to the topic will have a low weight. The TVSM document model is obtained as a sum of term vectors representing terms in the document. The similarity between two documents ''Di'' and ''Dj'' is defined as the scalar product of document vectors.\n\n==Enhanced Topic-based Vector Space Model==\nThe enhancement of the Enhanced Topic-based Vector Space Model (eTVSM)<ref>{{cite | url= http://kuropka.net/files/HPI_Evaluation_of_eTVSM.pdf | author=Dominik Kuropka | coauthors=Artem Polyvyanyy | title=A Quantitative Evaluation of the Enhanced Topic-Based Vector Space Model | year=2007}}</ref> (literature: [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id=]) is a proposal on how to derive term vectors from an [[Ontology_(information_science) | Ontology]]. Using a synonym Ontology created from [[WordNet]] Kuropka shows good results for document similarity. If a trivial Ontology is used the results are similar to Vector Space model.\n\n==Implementations==\n* [http://sourceforge.net/projects/etvsm/ Implementation of eTVSM in python]\n\n== References ==\n{{reflist}}\n\n[[Category:Vector space model]]"
p102
sg6
S'Topic-based vector space model'
p103
ssI38
(dp104
g2
S'http://en.wikipedia.org/wiki/Globrix'
p105
sg4
V'''Globrix''' was a UK [[real estate]] [[Web search engine|search engine]] that was launched in January 2008. It was launched as a joint venture with [[News International]], publishers of ''[[The Sunday Times]]'', ''[[The Sun (newspaper)|The Sun]]'', ''[[The Times]]'', ''[[The News of the World]]'' and ''[[Thelondonpaper]]''.<ref>[http://www.nma.co.uk/news/news-international-invests-in-property-site-globrix/35492.article News International invests in property site, Globrix - NMA article]</ref>\u000a\u000a[[Estate agent]]s and [[letting agent]]s could list their properties for free. This competed with traditional paid-listings sites such as [[Rightmove]] (originally a joint venture between four of the UK's largest property agents, now a [[public limited company]]), [[Zoopla|Propertyfinder]] (also partly backed by News International) and [[Primelocation]] (owned by [[Daily Mail and General Trust]]). Unlike most property websites, Globrix directed users to agent websites rather than hosting the property details and capturing the lead on Globrix itself. Globrix gathered its property listings in three different ways; crawling agent websites, taking data feeds and by agents manually uploading via the Globrix extranet. Because Globrix was 'free to list', Globrix was able to gain substantial market coverage and claimed to list more properties than any other UK property website. Unlike websites like [[Gumtree]] and [[Oodle]], private sellers and landlords were not allowed to list their properties on the site.\u000a\u000aThe website charged property professionals and property related services companies for geo-targeted [[Web banner|banner ads]]. There were also premium services available to estate and letting agents (such as [[Search Engine Optimization]] consultancy, branded email alerts and increased traffic) and [[Google Ads]] were displayed in unsold advertising positions on the right hand side of search results.\u000a\u000a==Functionality==\u000a\u000aThe basic property search functionality is kept simple with just one text box on the homepage. Users can search for property by location (e.g. city, town, full postcode, partial postcode or, unusually for property portals, street name), places of interest (e.g. schools, stations, landmarks) or by key features (e.g. swimming pool, garden, double glazing, helipad).\u000a\u000aSearch results can then be refined further by changing the price parameters, number of bedrooms and bathrooms, property type (e.g. detached, bungalow, flat), outside space, nearby stations and schools and property features (e.g. wooden floors, sea view). Registered users are able to search by additional parameters such as price change.\u000a\u000aAs an alternative to the regular 'list view' of property results, users can also opt to see the search results plotted on [[Bing Maps]] (previously they used [[Google map]]) to allow users to look for property by location. (Some users are unimpressed with the lack of precision of the inferior Bing offering, which often manages to put the marker in a field, compared to the accuracy and ease of use of Googlemaps).  Users are able to drag and zoom the map, with relevant properties automatically placed in view. It is also possible for users to draw a catchment area directly onto the map of where they would like to search.\u000a\u000a==Data==\u000a\u000aGlobrix data was sometimes used by the national media to illustrate stories on house prices,<ref>House prices drop £100,000 in two weeks in race to sell before Christmas - Daily Mail [http://www.dailymail.co.uk/news/article-1089563/House-prices-drop-100-000-WEEKS-race-sell-Christmas.html]</ref> the economy, area trends, consumer confidence<ref>[http://news.bbc.co.uk/1/hi/business/7737507.stm House sales rise as prices fall - BBC News]</ref> and the property market.<ref>[http://www.telegraph.co.uk/finance/personalfinance/borrowing/mortgages/3268208/Housing-market-stagnates-as-buyers-disappear.html Housing market stagnates as buyers disappear - Daily Telegraph]</ref>\u000a\u000a==Awards==\u000a\u000aIn 2008, Globrix was awarded 'Best Property Portal UK' which is awarded by one of the group's own newspapers, the [[The Daily Mail]].<ref>[http://www.residentialpropertyawards.net/index.php/International/Winners/Winners-of-2008.html Daily Mail Property Awards 2008]</ref> Globrix also won 'Estate Agency Service Firm of the Year' at The Negotiator Awards.<ref>[http://negotiator-magazine.co.uk/events/awards/categories-and-finalists/agency-service-firm-of-the-year/ The Negotiator Awards 2008]</ref>\u000a\u000a==Founders==\u000a\u000aGlobrix was founded by Dan Lee and Ian Parry, both ex employees of UK-based search company [[Autonomy Corporation|Autonomy]] and the Norwegian search company [[Fast Search & Transfer|FAST]].\u000a\u000a==Merged with Zoopla==\u000a\u000aIn December 2012 Globrix merged with [[Zoopla]].<ref name="Estate Agent Today">{{cite web|title=Zoopla acquires Globrix as it steps up battle against Rightmove|url=http://www.estateagenttoday.co.uk/news_features/Zoopla-acquires-Globrix-as-it-steps-up-battle-against-Rightmove|work=Estate Agent Today|accessdate=8 September 2013}}</ref>\u000a\u000a== References ==\u000a<references/>\u000a\u000a==External links==\u000a* [http://www.globrix.com/ Globrix homepage]\u000a* [http://www.ft.com/cms/s/0/bc401968-824e-11dc-8a8f-0000779fd2ac.html News International invests in search engine] - Financial Times\u000a* [http://www.independent.co.uk/news/business/analysis-and-features/home-search-sites-have-a-new-kid-on-the-block-786342.html Home search sites have a new kid on the block] - The Independent\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Online real estate companies]]
p106
sg6
S'Globrix'
p107
ssI167
(dp108
g2
S'http://en.wikipedia.org/wiki/Category:Search algorithms'
p109
sg4
S'{{Commons category|Search algorithms}}\n{{Cat main|Search algorithms}}\n\n[[Category:Algorithms]]\n[[Category:Searching]]'
p110
sg6
S'Category:Search algorithms'
p111
ssI41
(dp112
g2
S'http://en.wikipedia.org/wiki/Scientific data archiving'
p113
sg4
V'''Scientific data archiving''' is  the [[Computer_data_storage#Volatility|long-term storage]] of [[scientific data]] and methods. The various scientific journals have differing policies regarding how much of their data and methods scientists are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as   research in some areas depends on large datasets which cannot easily be replicated independently.\u000a\u000a[[Data archiving]] is more important in some fields than others.  In a few fields, all of the data necessary to replicate the work is already available in the journal article.  In drug development, a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data.\u000a\u000aThe requirement of data archiving is a recent development in the [[history of science]].  It was made possible by advances in [[information technology]] allowing large amounts of data to be stored and accessed from central locations.  For example, the [[American Geophysical Union]] (AGU) adopted their first policy on data archiving in 1993, about three years after the beginning of the [[WWW]].<ref>\u201dPolicy on Referencing Data in and Archiving Data for AGU Publications\u201d [http://www.agu.org/pubs/authors/policies/data_policy.shtml]</ref> This policy mandates that datasets cited in AGU papers must be archived by a recognised data center; it permits the creation of "data papers"; and it establishes AGU's role in maintaining data archives. But it makes no requirements on paper authors to archive their data.\u000a\u000aPrior to organized data archiving, researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author.  The science community expects authors to [[Data sharing (Science)|share supplemental data]].  This process was recognized as wasteful of time and energy and obtained mixed results.  Information could become lost or corrupted over the years.  In some cases, authors simply refuse to provide the information.\u000a\u000aThe need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation.<ref>"The Case for Due Diligence When Empirical Research is Used in Policy Formation" by Bruce McCullough and Ross McKitrick. [http://economics.ca/2006/papers/0685.pdf]</ref><ref>[http://gking.harvard.edu/replication.shtml "Data Sharing and Replication" a website by Gary King]</ref>\u000a\u000a==Selected policies by journals==\u000a\u000a===The American Naturalist===\u000a{{quote|[[The American Naturalist]]'' requires authors to deposit the data associated with accepted papers in a public archive. For gene sequence data and phylogenetic trees, deposition in [[GenBank]] or [[TreeBASE]], respectively, is required. There are many possible archives that may suit a particular data set, including the [[Dryad (repository)|Dryad]] repository for ecological and evolutionary biology data. All accession numbers for GenBank, TreeBASE, and Dryad must be included in accepted manuscripts before they go to Production. If the data is deposited somewhere else, please provide a link. If the data is culled from published literature, please deposit the collated data in Dryad for the convenience of your readers. Any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out.|JSTOR<ref>[http://www.jstor.org/page/journal/amernatu/forAuthor.html#data Supporting Data and Material]</ref>}}\u000a\u000a===Journal of Heredity===\u000a{{quote|The primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise, and should be preserved in usable form for decades in the future. For this reason, ''Journal of Heredity'' requires that newly reported nucleotide or amino acid sequences, and structural coordinates, be submitted to appropriate public databases (e.g., GenBank; the [[EMBL Nucleotide Sequence Database]]; DNA Database of Japan; the [[Protein Data Bank]] ; and [[Swiss-Prot]]). Accession numbers must be included in the final version of the manuscript. For other forms of data (e.g., microsatellite genotypes, linkage maps, images), the Journal endorses the principles of the Joint Data Archiving Policy (JDAP) in encouraging all authors to archive primary datasets in an appropriate public archive, such as Dryad, TreeBASE, or the Knowledge Network for Biocomplexity. Authors are encouraged to make data publicly available at time of publication or, if the technology of the archive allows, opt to embargo access to the data for a period up to a year after publication.\u000a\u000aThe American Genetic Association also recognizes the vast investment of individual researchers in generating and curating large datasets. Consequently, we recommend that this investment be respected in secondary analyses or meta-analyses in a gracious collaborative spirit.|oxfordjournals.org<ref>[http://www.oxfordjournals.org/our_journals/jhered/for_authors/msprep_submission.html#4.%20DATA%20ARCHIVING%20POLICY Data archiving policy]</ref>}}\u000a\u000a===Molecular Ecology===\u000a{{quote|[[Molecular Ecology]] expects that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, [[Gene Expression Omnibus]], TreeBASE, Dryad, the [[Knowledge Network for Biocomplexity]], your own institutional or funder repository, or as Supporting Information on the Molecular Ecology web site. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.|Wiley<ref>[http://www.wiley.com/bw/submit.asp?ref=0962-1083&site=1 Policy on data archiving]</ref>}}\u000a\u000a===Nature===\u000a{{quote|Such material must be hosted on an accredited independent site (URL and accession numbers to be provided by the author), or sent to the ''Nature'' journal at submission, either uploaded via the journal's online submission service, or if the files are too large or in an unsuitable format for this purpose, on CD/DVD (five copies). Such material cannot solely be hosted on an author's personal or institutional web site.<ref>[http://www.nature.com/authors/editorial_policies/availability.html "Availability of Data and Materials: The Policy of Nature Magazine]</ref>\u000a\u000a''Nature'' requires the reviewer to determine if all of the supplementary data and methods have been archived.  The policy advises reviewers to consider several questions, including: "Should the authors be asked to provide supplementary methods or data to accompany the paper online? (Such data might include source code for modelling studies, detailed experimental protocols or mathematical derivations.)|[[Nature (journal)|Nature]]<ref>{{cite web|title=Guide to Publication Policies of the Nature Journals|date=March 14, 2007|url=http://www.nature.com/authors/gta.pdf}}</ref>}}\u000a\u000a===''Science''===\u000a{{quote|''Science'' supports the efforts of databases that aggregate published data for the use of the scientific community. Therefore, before publication, large data sets (including microarray data, protein or DNA sequences, and atomic coordinates or electron microscopy maps for macromolecular structures) must be deposited in an approved database and an accession number provided for inclusion in the published paper.<ref>[http://www.sciencemag.org/about/authors/prep/gen_info.dtl#datadep "General Policies of Science Magazine"]</ref>\u000a\u000a"Materials and methods" \u2013 ''Science'' now requests that, in general, authors place the bulk of their description of materials and methods online as supporting material, providing only as much methods description in the print manuscript as is necessary to follow the logic of the text. (Obviously, this restriction will not apply if the paper is fundamentally a study of a new method or technique.)|[[Science (journal)|Science]]<ref>[http://www.sciencemag.org/about/authors/prep/prep_online.dtl \u201dPreparing Your Supporting Online Material\u201d]</ref>}}\u000a\u000a== Royal Society Publishing==\u000a{{quote|As a condition of acceptance authors agree to honour any reasonable request by other researchers for materials, methods, or data necessary to verify the conclusion of the article. Supplementary data up to 10Mb is placed on the Society's website free of charge and is publicly accessible. Large datasets must be deposited in a recognised public domain database by the author prior to submission. The accession number should be provided for inclusion in the published article.|{{citation needed |date=September 2013}}}}\u000a\u000a==Policies by funding agencies==\u000aIn the United States, the [[National Science Foundation]] (NSF) has tightened requirements on data archiving.   Researchers seeking funding from NSF are now required to file a [[data management plan]] as a two-page supplement to the grant application.<ref>[http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-every-grant-applicant.html \u201dNSF to Ask Every Grant Applicant for Data Management Plan\u201d]</ref>\u000a\u000aThe NSF [[Datanet]] initiative has resulted in funding of the '''Data Observation Network for Earth''' ([[DataONE]]) project, which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide. DataONE's stated goal is to preserve and provide access to multi-scale, multi-discipline, and multi-national data. The community of users for DataONE includes scientists, ecosystem managers, policy makers, students, educators, and the public.\u000a\u000a==Data archives==\u000aThe following list refers to scientific data archives. See [[Data archive]] for social science archives. \u000a* [[CISL Research Data Archive]]\u000a* [[Dryad (repository)|Dryad]]\u000a* [[ESO/ST-ECF Science Archive Facility]]\u000a* [http://www.ncdc.noaa.gov/paleo/treering.html International Tree-Ring Data Bank]\u000a* [http://www.icpsr.umich.edu Inter-university Consortium for Political and Social Research]\u000a* [http://knb.ecoinformatics.org Knowledge Network for Biocomplexity]\u000a* [[National Archive of Computerized Data on Aging]]\u000a* National Archive of Criminal Justice Data [http://www.icpsr.umich.edu/nacjd]\u000a* [[National Climatic Data Center]]\u000a* [[National Geophysical Data Center]]\u000a* [[National Snow and Ice Data Center]]\u000a* [[National Oceanographic Data Center]]\u000a* [http://daac.ornl.gov Oak Ridge National Laboratory Distributed Active Archive Center]\u000a* [[PANGAEA (data library)|Pangaea - Data Publisher for Earth & Environmental Science]]\u000a* [[World Data Center]]\u000a* [[DataONE]]\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==External links==\u000a* [[Registry of Research Data Repositories]] ''re3data.org'' [http://service.re3data.org/search/results?term=]\u000a* Statistical checklist required by ''Nature'' [http://www.nature.com/nature/authors/gta/Statistical_checklist.doc]\u000a* Policies of ''Proceedings of the National Academy of Sciences (U.S.)'' [http://www.pnas.org/misc/iforc.shtml#policies]\u000a* The US National Committee for CODATA [http://www7.nationalacademies.org/usnc-codata/Archiving.html]\u000a* The Role of Data and Program Code Archives in the Future of Economic Research  [http://research.stlouisfed.org/wp/2005/2005-014.pdf]\u000a* Data sharing and replication \u2013 Gary King website [http://gking.harvard.edu/replication.shtml]\u000a* The Case for Due Diligence When Empirical Research is Used in Policy Formation by McCullough and McKitrick [http://economics.ca/2006/papers/0685.pdf]\u000a* Thoughts on Refereed Journal Publication by Chuck Doswell [http://www.cimms.ou.edu/~doswell/pubreviews.html]\u000a* \u201cHow to encourage the right behaviour\u201d An opinion piece published in ''Nature'',  March, 2002.[http://www.nature.com/nature/journal/v416/n6876/full/416001b.html]\u000a* [[NASA Astrophysics Data System]] [http://cdsads.u-strasbg.fr/]\u000a* [[Panton Principles]] for Open Data in Science, at Citizendium [http://en.citizendium.org/wiki/Panton_Principles]\u000a* [[Inter-university Consortium for Political and Social Research]] [http://www.icpsr.umich.edu]\u000a[[Category:Information retrieval]]\u000a[[Category:Knowledge representation]]
p114
sg6
S'Scientific data archiving'
p115
ssI170
(dp116
g2
S'http://en.wikipedia.org/wiki/Reverse DNS lookup'
p117
sg4
V'''{{Redirect|Reverse DNS}}\u000a\u000aIn [[computer networking]], '''reverse DNS lookup''' or '''reverse DNS resolution''' (rDNS) is the determination of a [[domain name]] that is associated with a given  [[IP address]] using the [[Domain Name System]] (DNS) of the [[Internet]].\u000a\u000aComputer networks use the Domain Name System to determine the IP address associated with a domain name. This process is also known as ''forward'' DNS resolution.  ''Reverse'' DNS lookup is the inverse process, the resolution of an IP address to its designated domain name.\u000a\u000aThe reverse DNS database of the Internet is rooted in the ''Address and Routing Parameter Area'' (<tt>[[.arpa|arpa]]</tt>) [[top-level domain]] of the Internet. [[IPv4]] uses the <tt>in-addr.arpa</tt> domain and the <tt>ip6.arpa</tt> domain is delegated for [[IPv6]]. The process of reverse resolving an IP address uses the ''pointer'' DNS record type ([[List of DNS record types#Resource records|PTR record]]).\u000a\u000aInformational RFCs (RFC 1033, RFC 1912 Section 2.1) specify that ''"Every Internet-reachable host should have a name"'' and that such names match with a reverse pointer record, but it is not a requirement of standards governing operation of the DNS itself.\u000a\u000a==IPv4 reverse resolution==\u000aReverse DNS lookups for [[IPv4]] addresses use a ''reverse IN-ADDR entry'' in the special domain <tt>in-addr.arpa</tt>. In this domain, an IPv4 address is represented as a concatenated sequence of ''four decimal numbers'', separated by dots, to which is appended the second level domain suffix <tt>.in-addr.arpa</tt>. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four 8-bit portions and converting each 8-bit portion into a decimal number.  These decimal numbers are then concatenated in the order: least significant 8-bit portion first (leftmost), most significant 8-bit portion last (rightmost). It is important to note that ''this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses'' in textual form.\u000aFor example, an address (A) record for <tt>mail.example.com</tt> points to the IP address 192.0.2.5.\u000aIn pointer records of the reverse database, this IP address is stored as the domain name <tt>5.2.0.192.in-addr.arpa</tt> pointing back to its designated host name <tt>mail.example.com</tt>. \u000aThis allows it to pass the [[Forward Confirmed reverse DNS]] process.\u000a\u000a===Classless reverse DNS method===\u000aHistorically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A.  By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using ''canonical name'' ([[CNAME]]) DNS records.\u000a\u000a==IPv6 reverse resolution==\u000aReverse DNS lookups for [[IPv6]] addresses use the special domain <code>ip6.arpa</code>. An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address <code>2001:db8::567:89ab</code> is <code>b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa</code>.\u000a\u000a==Multiple pointer records==\u000aWhile most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended, unless there is a specific need.  For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically.  Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.<ref>[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]</ref> In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause responses to be truncated if they exceed the DNS 512 byte UDP message limit.\u000a\u000a==Records other than PTR records==\u000aRecord types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]] (RFC 4025), [[Secure Shell|SSH]] (RFC 4255) and [[Internet Key Exchange|IKE]] (RFC 4322).\u000a[[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] (RFC 6763) uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.<ref>{{Citation | publisher = IETF | title = RFC\u202f6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}</ref>\u000aLess standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.\u000a\u000a==Uses==\u000aThe most common uses of the reverse DNS include:\u000a* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the "Received:" trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.\u000a* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, dynamically assigned addresses, or other inexpensive Internet services.  Owners of such IP addresses typically assign them generic rDNS names such as "1-2-3-4-dynamic-ip.example.com."  Some corporate anti-spam services take the view that the vast majority, but by no means all, of e-mail that originates from these computers is spam with spam filters refusing e-mail with such rDNS names.<ref>[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus's FAQ]</ref><ref>[http://postmaster.aol.com/info/rdns.html reference page from AOL]</ref> However data has shown that just as much if not more spam has originated from unpatched machines within corporate networks that are more likely to use out of date browsers than cheaper services such as DSL networks not to mention the difficulty of blocking spam from major providers like Yahoo and Hotmail. A recent shift has shown that spamming has switched to mainly coming from hosting companies making using rDNS even less useful.<ref>http://www.mailchannels.com/blog/2013/03/worlds-largest-spam-sources-are-all-hosting-companies/</ref> All of this adds to the argument that the few services that choose to block email servers purely on the basis of rDNS are simply discriminating without merit and often miss out more pro-active and useful indiscriminate anti spam measures.<ref>http://ask.slashdot.org/story/11/10/13/1643202/ask-slashdot-is-reverse-dns-a-worthy-standard-for-fighting-spam</ref>\u000a* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, mainly because [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually can't pass verification for it when they use [[zombie computer]]s to forge domains.\u000a* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address\u000a\u000a==See also==\u000a*[[Forward-confirmed reverse DNS]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a*{{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}\u000a* [http://dns.icann.org ICANN DNS Operations]\u000a* RFC 2317 documents a way to do rDNS delegation for [[Classless Inter-Domain Routing|CIDR]] blocks\u000a* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]\u000a* RDNS policies: [http://postmaster.aol.com/Postmaster.Errors.php#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]\u000a\u000a[[Category:Searching]]\u000a[[Category:Domain name system]]\u000a\u000a[[nl:Domain Name System#Omgekeerde lookups]]'''
p118
sg6
S'Reverse DNS lookup'
p119
ssI44
(dp120
g2
S'http://en.wikipedia.org/wiki/Discounted cumulative gain'
p121
sg4
V'''Discounted cumulative gain''' ('''DCG''') is a measure of ranking quality. In [[information retrieval]], it is often used to measure effectiveness of [[World Wide Web|web]] [[search engine]] [[algorithm]]s or related applications. Using a [[Relevance (information retrieval)|graded relevance]] scale of documents in a search engine result set, DCG measures the usefulness, or ''gain'', of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.<ref>Kalervo Jarvelin, Jaana Kekalainen: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422\u2013446 (2002)</ref>\u000a\u000a== Overview ==\u000a\u000aTwo assumptions are made in using DCG and its related measures.\u000a\u000a# Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)\u000a# Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than irrelevant documents.\u000a\u000aDCG originates from an earlier, more primitive, measure called Cumulative Gain.\u000a\u000a=== Cumulative Gain ===\u000a\u000aCumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position <math>p</math> is defined as:\u000a\u000a:<math> \u005cmathrm{CG_{p}} = \u005csum_{i=1}^{p} rel_{i} </math>\u000a\u000aWhere <math>rel_{i}</math> is the graded relevance of the result at position <math>i</math>.\u000a\u000aThe value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document <math>d_{i}</math> above a higher ranked, less relevant, document <math>d_{j}</math> does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure.\u000a\u000a=== Discounted Cumulative Gain ===\u000a\u000aThe premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position <math>p</math> is defined as:<ref name="stanfordireval">{{cite web|title=Introduction to Information Retrieval - Evaluation|url=http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf|publisher=Stanford University|accessdate=23 March 2014|date=21 April 2013}}</ref>\u000a\u000a:<math> \u005cmathrm{DCG_{p}} = rel_1 + \u005csum_{i=2}^{p} \u005cfrac{rel_{i}}{\u005clog_{2}(i)} </math>\u000a\u000aPreviously there has not been shown any theoretically sound justification for using a [[logarithm]]ic reduction factor<ref>{{cite book | title=Search Engines: Information Retrieval in Practice | author=B. Croft, D. Metzler, and T. Strohman |year=2009 | publisher=''Addison Wesley"}}</ref> other than the fact that it produces a smooth reduction.\u000a\u000aAn alternative formulation of DCG<ref>Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML '05). ACM, New York, NY, USA, 89-96. DOI=10.1145/1102351.1102363 http://doi.acm.org/10.1145/1102351.1102363</ref> places stronger emphasis on retrieving relevant documents:\u000a\u000a:<math> \u005cmathrm{DCG_{p}} = \u005csum_{i=1}^{p} \u005cfrac{ 2^{rel_{i}} - 1 }{ \u005clog_{2}(i+1)} </math>\u000a\u000aThe latter formula is commonly used in industry including major web search companies<ref name="stanfordireval"/> and data science competition platform such as Kaggle.<ref>{{cite web|title=Normalized Discounted Cumulative Gain|url=https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain|accessdate=23 March 2014}}</ref>\u000a\u000aIn Croft, Metzler and Strohman (page 320, 2010), the authors mistakenly claim that these two formulations of DCG are the same when the relevance values of documents are [[binary function|binary]]; <math>rel_{i} \u005cin \u005c{0,1\u005c}</math>.  To see that they are not the same, let there be one relevant document and that relevant document is at rank 2.  The first version of DCG equals 1 / log2(2) = 1.  The second version of DCG equals 1 / log2(2+1) = 0.631.  The way that the two formulations of DCG are the same for binary judgments is in the way gain in the numerator is calculated.  For both formulations of DCG, binary relevance produces gain at rank i of 0 or 1.  No matter the number of relevance grades, the two formulations differ in their discount of gain.\u000a\u000aNote that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2.  When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation.  Clearly, the base of the log affects the value of DCG in both formulations.\u000a\u000aRecently, Wang et al.(2013)<ref>Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, Tie-Yan Liu. 2013. A Theoretical Analysis of NDCG Ranking Measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).</ref> give theoretical guarantee for using the logarithmic reduction factor in NDCG. Specifically, the authors prove for every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets.\u000a\u000a=== Normalized DCG ===\u000a\u000aSearch result lists vary in length depending on the [[Web search query|query]]. Comparing a search engine's performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of <math>p</math> should be normalized across queries. This is done by sorting documents of a result list by relevance, producing the maximum possible DCG till position <math>p</math>, also called Ideal DCG (IDCG) till that position. For a query, the ''normalized discounted cumulative gain'', or nDCG, is computed as:\u000a\u000a:<math> \u005cmathrm{nDCG_{p}} = \u005cfrac{DCG_{p}}{IDCG_{p}} </math>\u000a\u000aThe nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine's ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.\u000a\u000aThe main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial [[relevance feedback]] is available.\u000a\u000a== Example ==\u000a\u000aPresented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning irrelevant, 3 meaning completely relevant, and 1 and 2 meaning "somewhere in between". For the documents ordered by the ranking algorithm as\u000a\u000a:<math> D_{1}, D_{2}, D_{3}, D_{4}, D_{5}, D_{6} </math>\u000a\u000athe user provides the following relevance scores:\u000a\u000a:<math> 3, 2, 3, 0, 1, 2 </math>\u000a\u000aThat is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is:\u000a\u000a:<math> \u005cmathrm{CG_{6}} = \u005csum_{i=1}^{6} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11</math>\u000a\u000aChanging the order of any two documents does not affect the CG measure. If <math>D_3</math> and <math>D_4</math> are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is:\u000a\u000a{| class="wikitable" border="1"\u000a|-\u000a! <math>i</math>\u000a! <math>rel_{i}</math>\u000a! <math>\u005clog_{2}i</math>\u000a! <math> \u005cfrac{rel_{i}}{\u005clog_{2}i} </math>\u000a|-\u000a| 1\u000a| 3\u000a| 0\u000a| N/A\u000a|-\u000a| 2\u000a| 2\u000a| 1\u000a| 2\u000a|-\u000a| 3\u000a| 3\u000a| 1.585\u000a| 1.892\u000a|-\u000a| 4\u000a| 0\u000a| 2.0\u000a| 0\u000a|-\u000a| 5\u000a| 1\u000a| 2.322\u000a| 0.431\u000a|-\u000a| 6\u000a| 2\u000a| 2.584\u000a| 0.774\u000a|}\u000a\u000aSo the <math>DCG_{6}</math> of this ranking is:\u000a\u000a:<math> \u005cmathrm{DCG_{6}} = rel_{1} + \u005csum_{i=2}^{6} \u005cfrac{rel_{i}}{\u005clog_{2}i} = 3 + (2 + 1.892 + 0 + 0.431 + 0.774) = 8.10</math>\u000a\u000aNow a switch of <math>D_3</math> and <math>D_4</math> results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank.\u000a\u000aThe performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized.\u000a\u000aTo normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the [[Monotonic|monotonically decreasing]] sort of the relevance judgments provided by the experiment participant, which is:\u000a\u000a:<math> 3, 3, 2, 2, 1, 0 </math>\u000a\u000aThe DCG of this ideal ordering, or ''IDCG'', is then:\u000a\u000a:<math> \u005cmathrm{IDCG_{6}} = 8.69 </math>\u000a\u000aAnd so the nDCG for this query is given as:\u000a\u000a:<math> \u005cmathrm{nDCG_{6}} = \u005cfrac{DCG_{6}}{IDCG_{6}} = \u005cfrac{8.10}{8.69} = 0.932 </math>\u000a\u000a== Limitations ==\u000a# Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,0 </math> respectively, both would be considered equally good even if later contains a bad result. One way to take into account this limitation is use <math>1 - 2^{rel_{i}}</math> in numerator for scores for which we want to penalize and <math>2^{rel_{i}} - 1</math> for all others. For example, for the ranking judgments <math>Excellent, Fair, Bad</math> one might use numerical scores <math>1,0,-1</math> instead of <math>2,1,0</math>.\u000a# Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,1,1 </math> respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores <math> 1,1,1,0,0 </math> and <math> 1,1,1,1,1 </math> and quote nDCG as nDCG@5.\u000a# Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as "restaurants" nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive.\u000a\u000a== References ==\u000a{{Reflist|1}}\u000a\u000a[[Category:Information retrieval|*]]
p122
sg6
S'Discounted cumulative gain'
p123
ssI173
(dp124
g2
S'http://en.wikipedia.org/wiki/Multimedia search'
p125
sg4
V'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.\u000aMultimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.\u000aWe can distinguish two methodologies in multimedia search:\u000a*'''Metadata search''': the search is made on the layers of [[metadata]].\u000a* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.\u000a\u000a\u000a==Metadata search==\u000a\u000aSearch is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.\u000a\u000aThere are three processes which should be done in this method:\u000a*'''[[Multimedia Information Retrieval#Feature Extraction Methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.\u000a*'''[[ Multimedia Information Retrieval#Feature Extraction Methods |Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])\u000a*'''[[ Multimedia Information Retrieval#Categorization Methods | Categorization of media descriptions ]]''' into classes.\u000a\u000a==[[Query by Example]]==\u000a\u000aIn [[query by example]] the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often it\u2019s used [[Search engine indexing |audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:\u000a*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].\u000a*Compare descriptors of the query and our database\u2019s media.\u000a*List the media sorted by maximum coincidence.\u000a\u000a==Multimedia search engine==\u000aThere are two big search families, in function of the content:\u000a* [[Visual search engine]]\u000a*[[Audio search engine]]\u000a\u000a===[[Visual search engine]]===\u000aInside this family we can distinguish two topics: [[image search]] and [[video search]]\u000a\u000a*'''[[Image search]]''': Although usually it\u2019s used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example [[QR codes]].\u000a*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.\u000a\u000a===[[Audio search engine]]===\u000aThere are different methods of audio searching:\u000a*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].\u000a*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album\u2026) . There are some programs of [[music recognition]]. for example: [[Shazam (service)|Shazam]] or [[SoundHound]].\u000a\u000a==See also==\u000a*[[Search engine indexing]]\u000a*[[Multimedia]]\u000a*[[Multimedia Information Retrieval]]\u000a*[[Streaming media]]\u000a*[[Journal of Multimedia]]\u000a*[[List of search engines#Multimedia|List of search engines]]\u000a*[[Video search engine]]\u000a\u000a==External links==\u000a\u000a[[Category:Searching]]\u000a[[Category:Multimedia]]
p126
sg6
S'Multimedia search'
p127
ssI47
(dp128
g2
S'http://en.wikipedia.org/wiki/Cranfield Experiments'
p129
sg4
S"The '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.<ref>Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.</ref><ref>Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.</ref><ref>Cleverdon, C. W., & Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. \n</ref> \n\nThey represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).\n\n==See also==\n*[[ASLIB]]\n*[[Information history]]\n\n==References==\n{{Reflist}}\n\n[[Category:Experiments]]\n[[Category:Information retrieval]]\n\n\n{{database-stub}}"
p130
sg6
S'Cranfield Experiments'
p131
ssI176
(dp132
g2
S'http://en.wikipedia.org/wiki/Contextual Query Language'
p133
sg4
S'\'\'\'Contextual Query Language\'\'\' (CQL), previously known as \'\'\'Common Query Language\'\'\',<ref>[http://www.loc.gov/standards/sru/cql/spec.html CQL: the Contextual Query Language: Specifications] SRU: Search/Retrieval via URL, Standards, Library of Congress</ref> is a [[formal language]] for representing queries to [[information retrieval]] systems such as [[search engine]]s, [[bibliography|bibliographic catalogs]] and [[museum]] collection information. Based on the [[semantics]] of [[Z39.50]], its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex [[query language]]s. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the [[Library of Congress]].\n\n== Examples of query syntax ==\n\nSimple queries:\n\n<blockquote><tt>dinosaur<br/>\n"complete dinosaur"<br/>\ntitle = "complete dinosaur"<br/>\ntitle exact "the complete dinosaur"</tt></blockquote>\n\nQueries using [[Boolean logic]]:\n\n<blockquote><tt>dinosaur or bird<br/>\nPalomar assignment and "ice age"<br/>\ndinosaur not reptile<br/>\ndinosaur and bird or dinobird<br/>\n(bird or dinosaur) and (feathers or scales)<br/>\n"feathered dinosaur" and (yixian or jehol)</tt></blockquote>\n\nQueries accessing [[index (publishing)|publication indexes]]:\n\n<blockquote><tt>publicationYear < 1980<br/>\nlengthOfFemur > 2.4<br/>\nbioMass >= 100</tt></blockquote>\n\nQueries based on the proximity of words to each other in a document:\n\n<blockquote><tt>ribs prox/distance<=5 chevrons<br/>\nribs prox/unit=sentence chevrons<br/>\nribs prox/distance>0/unit=paragraph chevrons</tt></blockquote>\n\nQueries across multiple [[Dimension (data warehouse)|dimensions]]:\n\n<blockquote><tt>date within "2002 2005"<br/>\ndateRange encloses 2003</tt></blockquote>\n\nQueries based on [[Relevance (information retrieval)|relevance]]:\n\n<blockquote><tt>subject any/relevant "fish frog"<br/>\nsubject any/rel.lr "fish frog"</tt></blockquote>\n\nThe latter example specifies using a specific [[algorithm]] for [[logistic regression]].<ref>[http://srw.cheshire3.org/contextSets/rel/ Relevance Ranking Context Set version 1.1]</ref>\n\n== References ==\n{{Reflist}}\n\n== External links ==\n* [http://www.loc.gov/standards/sru/cql/ CQL home page]\n* [http://www.loc.gov/z3950/agency/ Z39.50 Maintenance Agency]\n* [http://zing.z3950.org/cql/intro.html A Gentle Introduction to CQL]\n\n{{Query languages}}\n\n{{USGovernment|sourceURL=http://www.loc.gov/standards/sru/cql/}}\n{{LOC-stub}}\n\n[[Category:Searching]]\n[[Category:Library science]]\n[[Category:Library of Congress]]\n[[Category:Query languages]]\n[[Category:Knowledge representation languages]]'
p134
sg6
S'Contextual Query Language'
p135
ssI50
(dp136
g2
S'http://en.wikipedia.org/wiki/Relevance feedback'
p137
sg4
V'''Relevance [[feedback]]''' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.\u000a\u000a== Explicit feedback ==\u000a\u000aExplicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.\u000a\u000aUsers may indicate relevance explicitly using a ''binary'' or ''graded'' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.\u000a\u000aThe relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].\u000a\u000aA performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at ''k'' and [[Mean average precision#Mean average precision|mean average precision]].\u000a\u000a== Implicit feedback ==\u000a\u000aImplicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf].\u000a\u000aThe key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:\u000a\u000a# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and\u000a# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback\u000a\u000aAn example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.\u000a\u000a== Blind feedback ==\u000a\u000aPseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:\u000a\u000a# Take the results returned by initial query as relevant results (only top k with k being between 10 to 50 in most experiments).\u000a# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.\u000a# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.\u000a\u000aSome experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.\u000a\u000aThis automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.<ref>Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202''Query expansion using local and global document analysis''], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.</ref> Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.<ref>Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546''Positional relevance model for pseudo-relevance feedback''], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.</ref> \u000aSpecifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.\u000a\u000aBlind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.\u000a\u000a== Using relevance information ==\u000a\u000aRelevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].\u000a\u000a==Further reading==\u000a*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin's lecture notes, adapted from Doug Oard's\u000a*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from ''Modern Information Retrieval''\u000a*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\u000a\u000a== References ==\u000a{{reflist|2}}\u000a\u000a[[Category:Information retrieval]]\u000a\u000a[[zh:\u76f8\u5173\u53cd\u9988]]
p138
sg6
S'Relevance feedback'
p139
ssI179
(dp140
g2
S'http://en.wikipedia.org/wiki/IBM Omnifind'
p141
sg4
S"'''IBM OmniFind''' was an [[enterprise search]] platform from [[IBM]].\nIt did come in several packages adapted to different business needs, including OmniFind Enterprise Edition, OmniFind Enterprise Starter Edition, and OmniFind Discovery Edition.<ref>[http://www-01.ibm.com/software/ecm/omnifind/library.html IBM - OmniFind - Library]</ref> IBM OmniFind as a standalone product was withdrawn in April 2011<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS911-075 IBM US Announcement Letter]</ref> and is now part of [[IBM Watson Content Analytics with Enterprise Search]].<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?infotype=AN&subtype=CA&htmlfid=897/ENUS211-133 IBM US Announcement Letter]</ref>\n\n'''IBM OmniFind Yahoo! Edition''' was a free-of-charge version that could handle up to 500,000 documents in its index and was intended for small businesses. IBM OmniFind Yahoo! Edition was simple to install, provided a user friendly front end for administration, and incorporated technology from the open source [[Lucene]] project. IBM withdrew this product from marketing effective September 22, 2010 and withdrew support effective June 30, 2011.<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS910-115 IBM US Announcement Letter]</ref>\n\n'''IBM OmniFind Personal E-mail Search''' was a research product launched in 2007 for doing [[semantic search]] over personal emails by extracting and organizing concepts and relationships (such as phone numbers and addresses). The project appears to have been silently abounded sometimes around 2010.\n\n== See also ==\n* [[Languageware]]\n* [[UIMA]]\n* [[Comparison of enterprise search software]]\n* [[List of enterprise search vendors]]\n\n==External links==\n* [http://www.ibm.com/software/data/enterprise-search/ IBM OmniFind]\n* [http://omnifind.ibm.yahoo.com/ IBM OmniFind Yahoo! Edition] {{Dead link|date=May 2012}}\n* [http://www.alphaworks.ibm.com/tech/emailsearch IBM OmniFind Personal E-mail Search] {{Dead link|date=January 2012}}\n* [http://www.opentestsearch.com/search-engines/ibm-omnifind-yahoo-edition-review/ Online demo and review of IBM OmniFind Yahoo! Edition]\n\n==Notes==\n{{reflist}}\n\n[[Category:IBM software|OmniFind]]\n[[Category:Searching]]"
p142
sg6
S'IBM Omnifind'
p143
ssI53
(dp144
g2
S'http://en.wikipedia.org/wiki/Negative search'
p145
sg4
S'{{Multiple issues|\n{{unreferenced|date=March 2009}}\n{{orphan|date=February 2009}}\n{{confusing|date=March 2009}}\n}}\n\n\'\'\'Negative Search\'\'\' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.\n\nNegative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.\n\nNegative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.\n\nNegative Search can also apply to searches where the user has a clear understanding of \'\'\'Negative Intent\'\'\' (what they don\'t want) rather than what they do.\n\nExamples of Negative Intent are:\n\n- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don\'t want.\n\n- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don\'t want.\n\n- An investigator is looking for a car but has no other information on that car on which to base a search.\n\n==Negative Search Classifiers==\n\nIf there are two forms of search (positive and negative) it follows that there are two forms of classifier models: \'\'\'Inclusive Classifiers\'\'\' and \'\'\'Exclusive Classifiers\'\'\'.\n\n[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.\n\n==Irrelevancy as a Desirable Construct==\n\nPositive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.\n\nIt follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.\n\n==Degrees of Passivity==\n\nPositive Search involves an active search by a user with no degree of passivity (or openness). For example: "I am only interested in the Hilton Hotel in Vientiane on [[New Year\\\'s Eve|New Years Eve]]."\n\nDiscovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: "I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there\'s a better hotel, let me know"\n\nNegative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: "I need a holiday and really don\'t care where as long as its good."\n\nSearchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don\'t know they\'re looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.\n\n==References==\n{{Reflist}}\n\n[[Category:Information retrieval]]'
p146
sg6
S'Negative search'
p147
ssI182
(dp148
g2
S'http://en.wikipedia.org/wiki/Desktop search'
p149
sg4
V{{multiple issues|\u000a{{Cleanup|date=October 2010}}\u000a{{technical|date=October 2014}}\u000a}}\u000a\u000a[[File:Puggle-search.png|thumb|Puggle Desktop Search]]\u000a[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]\u000a'''Desktop search''' tools search within a user's own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.\u000a\u000aOne of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.\u000a\u000aA variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.\u000a\u000aDesktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users needs to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside [[unstructured data]] \u2014 the information stored on an end user's PC, the directories (folders) and files they've created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.<ref>{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.</ref>  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don't have ready access.\u000a\u000aCompanies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index \u2014 but not preview \u2014 items they should not even know exist.{{Citation needed|date = November 2009}}\u000a\u000aHistorically, full desktop search comes from the work of [[Apple inc.|Apple Computer's]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.\u000a\u000a== Technologies ==\u000aMost desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine,<ref>{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}</ref> which performs searches over only filenames &mdash; not the files' contents &mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,<ref>{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}</ref> which performs searches over filenames and files' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products, such as Lookeen,<ref>{{cite web|title=Real-Time Indexing and Lookeen 8|url=http://www.lookeen.net/2884/News/real-time-ndexing-and-lookeen-8/|publisher=Lookeen|accessdate=26 October 2014}}</ref> have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.\u000a\u000aDesktop search tools typically collect three types of information about files:\u000a* file and folder names\u000a* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]\u000a* file content (for supported types of documents only)\u000a\u000aTo search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a ''Microsoft Office Filter'' might be used to search inside [[Microsoft Office]] documents.\u000a\u000aLong-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.<ref>[http://www.niallkennedy.com/blog/archives/2006/10/video-search.html "The current state of video search", by Niall Kennedy]</ref><ref>[http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html "The current state of audio search", by Niall Kennedy]</ref>\u000a\u000aThe sector attracted considerable attention from the struggle between Microsoft and Google.<ref>[http://news.bbc.co.uk/1/hi/technology/3952285.stm "Search wars hit desktop computers". (Oct 2004) BBC News]</ref> According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]'s complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.<ref>[http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/ "Microsoft agrees to change Vista Desktop Search Tool" (Jun 2007)]</ref>\u000a\u000aAs of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more. <ref>[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ "Google Desktop Update" (Sept 2011)]</ref>  \u000a\u000aX1 makes one of the leading desktop search products on the market. X1 Search 8 is a software alternative to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more. <ref>[http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/ "X1 rises again with Desktop Search 8, Virtual Edition" (May 2013)]</ref>   \u000a\u000a==Platforms & their histories==\u000aThere are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS & [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.\u000a\u000a'''Windows'''\u000a\u000aToday's Windows Search replaced WDS (Windows Desktop Search). WDS, in turn, replaced Indexing Service. A "a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching"<ref>https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx</ref> Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.<ref>https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx</ref> This made indexing large amounts of files require extremely powerful hardware and very long wait times.\u000a\u000aIn 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought "Instant searching" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.<ref>http://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx</ref> Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.\u000a\u000aWith the release of Windows Vista came Windows Search 3.1. Unlike it's predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the RAM and CPU requirements were greatly reduced. Cutting back indexing times immensely. This brings us to the Windows Search 4.0 which is currently running on all PCs with Windows 7 and up.\u000a\u000a'''Mac OS'''\u000a\u000aMac OS was the first to fully implement Desktop Search, it allowed users to fully search all documents with in their Macintosh computer. This means file format types, meta-data on those file formats and the content within the files. Released in 1994 two years before Windows Search was released, AppleSearch already had content searching. The biggest issue that AppleSearch had large resource requirements "AppleSearch requires at least a 68040 processor and 5MB of RAM."<ref>http://infomotions.com/musings/tricks/manuscript/1600-0001.html</ref> A Macintosh computer that had these specs cost around $1400 in today's dollars that's around $2050.<ref>http://stats.areppim.com/calc/calc_usdlrxdeflator.php</ref> On top of that, the software it self cost around $1400 for a single licenses.\u000a\u000aIn 1997, Sherlock was released alongside Mac OS 8.5. Sherlock, named after the famous fictional detective Sherlock Holmes, was integrated into Mac OS's file browser: Finder. Sherlock extended the desktop search to the world wide web. Allowing users to now search locally and externally. Adding the web to Sherlock was relatively easy as the plugins only needed to be written in a plain text file. Sherlock was included in every single Mac OS 8, 9 and 10 until 10.5.\u000a\u000aSpotlight was released in 2005, on Mac OSX 10.4, is a Selection-based search which means the user invokes a query using only the mouse. It allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage. Spotlight also uses a built-in Oxford American Dictionary and calculator to offer quick access to definitions and small calculations.<ref>http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html</ref> While Spotlight had a initially long start-up time (for first time set up). The entire hard disk was indexed, and as files are added to the hard disk, the index is constantly being updated in the background. This is done using minimal CPU & RAM resources, making searching relatively easy and quick.\u000a\u000a'''Linux'''\u000a\u000aFor Linux, we will primarily cover the Ubuntu distribution as it was and currently is still the most popular version of Linux. Strangely enough, Ubuntu didn't have desktop search until Feisty Fawn 7.04. Using Tracker<ref>http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/</ref> desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. Considering the fact that both are UNIX based systems. Tracker, was released in late 2007 was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting, and meta-data matching, but support for searching through emails and messages (instant messages) was added. Years later, in 2014 Recoll<ref>http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING</ref> was added to Linux distributions, it works with other search programs such as Tracker and Beagle to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. The wonderful thing about Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example you could search for just file types or by content.<ref>http://archive09.linux.com/feature/114283</ref>\u000a\u000a==See also==\u000a*[[List of search engines#Desktop search engines|List of desktop search engines]]\u000a\u000a== References ==\u000a<!--* [http://ims.dei.unipd.it/members/agosti/teaching/2006-07/ir/ Maristella Agosti's website]-->\u000a{{reflist|2}}\u000a\u000a== External links ==\u000a* ''[http://www.slate.com/id/2111643/ Keeper Finders]'', by Paul Boutin, ''[[Slate (magazine)|Slate]]'', December 31, 2004 &mdash; A comparison of Google, Ask Jeeves, HotBot, MSN and Copernic desktop search tools.\u000a* [http://www.goebelgroup.com/desktopmatrix.htm GoebelGroup.com's desktop search tools comparison chart] - Date of last update: 15 January 2007.\u000a* [http://labnol.blogspot.com/2004/10/detailed-comparison-of-desktop-search.html A detailed comparison of desktop search tools] - dated 2004.\u000a* [http://www.wikinfo.org/index.php/Comparison_of_desktop_search_software Comparison of desktop search software] - Date of last update: March 2008\u000a* [http://tbox.codeplex.com/ TBox] - DevTool, with ability to do fast search by text files\u000a\u000a{{Navigationbox Desktopsearch}}\u000a\u000a{{DEFAULTSORT:Desktop Search}}\u000a[[Category:Desktop search engines| ]]\u000a[[Category:Searching]]
p150
sg6
S'Desktop search'
p151
ssI56
(dp152
g2
S'http://en.wikipedia.org/wiki/Enterprise search'
p153
sg4
S'\'\'\'Definition:\'\'\' \'\'\'Enterprise search\'\'\' is the organized retrieval of \'\'\'structured\'\'\' and \'\'\'unstructured\'\'\' data within an organization. \n\n\n\'\'\'Enterprise search\'\'\' is the practice of making content from multiple enterprise-type sources, such as [[database]]s and [[intranet]]s, searchable to a defined audience.\n\n==Enterprise search summary==\n"Enterprise Search" is used to describe the software of search information within an enterprise (though the search function and its results may still be public).<ref>[http://www.aiim.org/What-is-Enterprise-Search What is Enterprise Search?]</ref> Enterprise search can be contrasted with [[web search]], which applies search technology to documents on the open web, and [[desktop search]], which applies search technology to the content on a single computer.\n\nEnterprise search systems index data and documents from a variety of sources such as: [[file systems]], [[intranets]], [[document management system]]s, [[e-mail]], and [[databases]]. Many enterprise search systems integrate structured and unstructured data in their collections.<ref>[http://www.arma.org/bookstore/files/Delgado.pdf The New Face of Enterprise Search: Bridging Structured and Unstructured Information]</ref> Enterprise search systems also use access controls to enforce a security policy on their users.<ref>[http://www.ideaeng.com/tabId/98/itemId/118/Mapping-Security-Requirements-to-Enterprise-Search.aspx Mapping Security Requirements to Enterprise Search - Part 1: Defining Specific Security Requirements]</ref>\n\nEnterprise search can be seen as a type of [[vertical search]] of an enterprise.\n\n==Components of an enterprise search system==\nIn an enterprise search system, content goes through various phases from source repository to search results:\n\n=== Content awareness ===\nContent awareness (or "content collection") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its [[API]]s. This model is used when realtime indexing is important. In the pull model, the software gathers content from sources using a connector such as a [[web crawler]] or a [[database]] connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content.<ref>[http://www.information-management.com/issues/20_7/content_management_data_integration_indexing_metadata-10019105-1.html Understanding Content Collection and Indexing]</ref>\n\n=== Content processing and analysis ===\nContent from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve [[Recall (information retrieval)|recall]] or [[Precision (information retrieval)|precision]]. These may include [[stemming]], [[lemmatization]], [[synonym]] expansion, [[entity extraction]], [[part of speech]] tagging.\n\nAs part of processing and analysis, [[tokenization (lexical analysis)|tokenization]] is applied to split the content into [[Lexical analysis#Token|tokens]] which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall.<ref>[http://packages.python.org/Whoosh/stemming.html Stemming, Variations, and Accent Folding]</ref>\n\n=== Indexing ===\nThe resulting text is stored in an [[Index (search engine)|index]], which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and [[term frequency]].\n\n=== Query Processing ===\nUsing a web page, the user issues a [[Web search query|query]] to the system. The query consists of any terms the user enters as well as navigational actions such as [[faceted search|faceting]] and paging information.\n\n=== Matching ===\nThe processed query is then compared to the stored index, and the search system returns results (or "hits") referencing source documents that match. Some systems are able to present the document as it was indexed.\n\n==Differences from web search==\nBeyond the difference in the kinds of materials being indexed, enterprise search systems also typically include functionality that is not associated with the mainstream [[web search engine]]s. These include:\n*Adapters to index content from a variety of repositories, such as [[databases]] and [[content management systems]].\n*[[Federated search]], which consists of\n# transforming a query and broadcasting it to a group of disparate databases or external content sources with the appropriate syntax,\n# merging the results collected from the databases,\n# presenting them in a succinct and unified format with minimal duplication, and\n# providing a means, performed either automatically or by the portal user, to sort the merged result set.\n*[[Enterprise bookmarking]], collaborative [[tag (metadata)|tagging]] systems for capturing knowledge about structured and semi-structured enterprise data.\n*[[Entity extraction]] that seeks to locate and classify elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n*[[Faceted search]], a technique for accessing a collection of information represented using a [[faceted classification]], allowing users to explore by filtering available information.\n*Access control, usually in the form of an [[Access control list]] (ACL), is often required to restrict access to documents based on individual user identities. There are many types of access control mechanisms for different content sources making this a complex task to address comprehensively in an enterprise search environment (see below).\n*[[Text clustering]], which groups the top several hundred search results into topics that are computed on the fly from the search-results descriptions, typically titles, excerpts (snippets), and meta-data.  This technique lets users navigate the content by topic rather than by the meta-data that is used in faceting. Clustering compensates for the problem of incompatible meta-data across multiple enterprise repositories, which hinders the usefulness of faceting.\n*[[User interfaces]], which in web search are deliberately kept simple in order not to distract the user from clicking on ads, which generates the revenue.  Although the business model for enterprise search could include showing ads, in practice this is not done.  To enhance end user productivity, enterprise vendors continually experiment with rich UI functionality which occupies significant screen space, which would be problematic for web search.\n\n==Relevance factors for enterprise search==\nThe factors that determine the relevance of search results within the context of an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich [[hyperlink|link structure]] as is found on the web\'s [[hypertext]] content, however, a new breed of Enterprise search engines based on a bottom-up [[Web 2.0]] technology are providing both a contributory approach and [[hyperlink]]ing within the enterprise. Algorithms like [[PageRank]] exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent relevance factor. In contrast, enterprises typically have to use other query-independent factors, such as a document\'s recency or popularity, along with query-dependent factors traditionally associated with [[information retrieval]] algorithms.  Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user\'s attention.\n\n==Access Control - early binding vs late binding==\nSecurity and restricted access to documents is an important matter in Enteprise Search. There are two main approaches to apply restricted access: early binding vs late binding.<ref>[http://enterprisesearch.co/enterprise-search-document-access-control/ Enterprise Search: document access control]</ref>\n\n===Late binding===\nPermissions are analyzed and assigned to documents at query stage. Query engine generates a document set and before returning it to a user this set is filtered based on user access rights. It is costly process but accurate (based on user permissions at the moment of query).\n\n===Early binding===\nPermissions are analyzed and assigned to documents at indexing stage. It is much more effective than late binding, but could be inaccurate (user might be granted or revoked permissions between in the period between indexing and querying).\n\n==Search Relevance Testing options==\nSearch application relevance can be determined by following relevance testing options like<ref>[http://searchhub.org/2009/09/02/debugging-search-application-relevance-issues/  Debugging Search Application Relevance Issues]</ref>\n*Focus groups\n*Reference evaluation protocol (based on relevance judgements of results from agreed-upon queries performed against common document corpuses)\n*Empirical testing\n*[[A/B testing]]\n*Log analysis on a Beta production site\n*Online ratings\n\n==See also==\n*[[Comparison of enterprise search software]]\n*[[List of enterprise search vendors]]\n*[[List of Search Engines]]\n*[[Collaborative search engine]]\n*[[Data Defined Storage]] \n*[[Enterprise bookmarking]]\n*[[Enterprise information access]]\n*[[Knowledge management]]\n*[[Text mining]]\n*[[Faceted search]]\n*[[Information Extraction]]\n*[[Vertical search|Vertical Search]]\n\n==References==\n{{Reflist}}\n\n{{DEFAULTSORT:Enterprise Search}}\n[[Category:Information retrieval]]\n[[Category:Searching]]'
p154
sg6
S'Enterprise search'
p155
ssI185
(dp156
g2
S'http://en.wikipedia.org/wiki/Concordance (publishing)'
p157
sg4
V{{sister\u000a|project=wiktionary\u000a|text=See the [[Wiktionary:Wiktionary:Concordances|list of concordances]] in [[Wiktionary]], the free dictionary\u000a}}\u000a\u000aA '''concordance''' is an alphabetical list of the principal words used in a book or body of work, with their immediate [[context (language use)#Verbal context|context]]s.  Because of the time, difficulty, and expense involved in creating a concordance in the pre-[[computer]] era, only works of special importance, such as the [[Vedas]],<ref>{{cite book|first = Maurice | last = Bloomfield| authorlink = Maurice Bloomfield | title = A Vedic Concordance| year  = 1990| publisher = Motilal Banarsidass Publ| isbn = 81-208-0654-9}}</ref> [[Bible]], [[Qur'an]] or the works of [[William Shakespeare|Shakespeare]] or classical Latin and Greek authors,<ref>{{cite journal | first = Roy | last = Wisby | authorlink = Roy Wisby | title = Concordance Making by Electronic Computer: Some Experiences with the Wiener Genesis| journal = The Modern Language Review | publisher = Modern Humanities Research Association | volume = 57 | issue = 2 | pages = 161\u2013172 | date = April 1962 | doi=10.2307/3720960}}</ref> had concordances prepared for them. \u000a[[File:Mordechai nathan hebrew latin concordance.jpg|right|thumb|225px|Mordecai Nathan's Hebrew-Latin Concordance of the Bible]]\u000aA concordance is more than an index; additional material, such as commentary, definitions, and topical cross-indexing make producing them a labor-intensive process, even when assisted by computers.\u000a\u000aAlthough an automatically generated [[subject indexing|index]] lacks the richness of a published concordance, the ability to combine the result of queries concerning multiple terms (such as searching for words near other words) has reduced interest in concordance publishing.  In addition, mathematical techniques such as [[Latent Semantic Indexing]] have been proposed as a means of automatically identifying linguistic information based on word context.\u000a\u000aA '''bilingual concordance''' is a concordance based on [[aligned parallel text]].\u000a\u000aA '''topical concordance''' is a list of subjects that a book (usually The Bible) covers, with the immediate context of the coverage of those subjects. Unlike a traditional concordance, the indexed word does not have to appear in the verse. The most well known topical concordance is [[Nave's Topical Bible]].\u000a\u000aThe first concordance, to the [[Vulgate]] Bible, was compiled by [[Hugh of St Cher]] (d.1262), who employed 500 monks to assist him. In 1448 Rabbi Mordecai Nathan completed a concordance to the Hebrew Bible. It took him ten years. 1599 saw a concordance to the Greek New Testament published by Henry Stephens and the Septuagint was done a couple of years later by Conrad Kircher in 1602. The first concordance to the English bible was published in 1550 by Mr Marbeck. According to Cruden it did not employ the verse numbers devised by Robert Stephens in 1545 but "the pretty large concordance" of Mr Cotton did. Then followed [[Cruden's Concordance]] and [[Strong's Concordance]].\u000a\u000a==Use in linguistics==\u000aConcordances are frequently used in [[linguistics]], when studying a text. For example:    \u000a* comparing different usages of the same word\u000a* analysing keywords\u000a* analysing [[word frequencies]]\u000a* finding and analysing phrases and [[idioms]]\u000a* finding [[translation]]s of subsentential elements, e.g. [[terminology]], in [[Bitext#Bitexts and translation memories|bitexts and translation memories]]\u000a* creating indexes and word lists (also useful for publishing)\u000a\u000aConcordancing techniques are widely used in national corpora such as [[American National Corpus]], [[British National Corpus]], and [[Corpus of Contemporary American English]] available on-line.  Stand-alone applications that employ concordancing techniques are known as concordancers.<ref>[http://www.lexically.net/wordsmith/introduction.htm?gclid=COjFnvGKhakCFVJX4Qod-RqjjQ Introduction to WordSmith]</ref> Some of them have integrated part-of-speech taggers and enable the user to create his/her own pos-annotated corpora to conduct various type of searches adopted in corpus linguistics.<ref>[http://yatsko.zohosites.com/linguistic-toobox-a-concordancer.html Linguistic Toolbox]</ref>\u000a\u000a==Inversion==\u000a\u000aThe reconstruction of the text of some of the [[Dead Sea Scrolls]] involved a concordance.\u000a\u000aAccess to some of the scrolls was governed by a "secrecy rule" that allowed only the original International Team or their designates to view the original materials. After the death of [[Roland de Vaux]] in 1971, his successors repeatedly refused to even allow the publication of photographs to other scholars. This restriction was circumvented by [[Martin Abegg]] in 1991, who used a computer to "invert" a concordance of the missing documents made in the 1950s which had come into the hands of scholars outside of the International Team, to obtain an approximate reconstruction of the original text of 17 of the documents.<ref>{{cite web |last= Hawrysch |first= George |title= Dr. George Hawrysch's speech on concordance book launch |work= The Ukrainian Weekly, No. 31, Vol. LXX |publisher= Ukrainian National Association |date= 2002-08-04 |url= http://www.ukrweekly.com/old/archive/2002/310217.shtml |accessdate= 2008-06-19}}</ref><ref>{{cite web |last= Jillette |first= Penn |title= You May Already be a "Computer Expert" |url= http://pennandteller.com/sincity/penn-n-teller/pcc/deadsea.html |accessdate= 2008-06-14}}</ref> This was soon followed by the release of the original text of the scrolls.\u000a\u000a== See also ==\u000a* [[Back-of-the-book index]]\u000a* [[A Vedic Word Concordance]]\u000a* [[Bible concordance]]\u000a* [[Bitext]]\u000a* [[Concordancer]]\u000a* [[Cross-reference]]\u000a* [[Index (publishing)|Index]]\u000a* [[Key Word in Context|KWIC]]\u000a* [[Text mining]]\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.opensourceshakespeare.org/concordance/ Shakespeare concordance] - A concordance of Shakespeare's complete works (from Open Source Shakespeare)\u000a* [http://www.arts.ualberta.ca/~ukr/skovoroda/NEW/ Online Concordance to the Complete Works of Hryhorii Skovoroda] - A concordance to Hryhorii Skovoroda's complete works (University of Alberta, Edmonton, Canada)\u000a* [http://infomotions.com/alex/ Alex Catalogue of Electronic Texts] - The Alex Catalogue is a collection of public domain electronic texts from American and English literature as well as Western philosophy. Each of the 14,000 items in the Catalogue are available as full-text but they are also complete with a concordance. Consequently, you are able to count the number of times a particular word is used in a text or list the most common (10, 25, 50, etc.) words.\u000a* [http://victorian.lang.nagoya-u.ac.jp/concordance/ Hyper-Concordance] - The Hyper-Concordance is written in C++, a program that scans and displays lines based on a command entered by the user. The main advantage of the C++ program is that it not only identifies the concordance lines but the words occurring to the left and the right of the word or phrase searched. It also reports the total number of text lines, the total word count and the number of occurrences of the word or phrase searched. The full text of the book is displayed in a box at the bottom of the screen. Each line of the text is numbered, and the line number and the term(s) searched provide a link to the full text.\u000a* [http://cherry.conncoll.edu/cohar/Programs.htm Concord] - Page includes link to Concord, an on-the-fly KWIC concordance generator.  Works with at least some non-Latin scripts (modern Greek, for instance).  Multiple choices for sorting results; multi-platform; Open Source.\u000a* [http://buschmeier.org/bh/study/ccd/ ConcorDance] - A concordance interface to the WorldWideWeb, it uses Google's or Yahoo's search engine to find concordances and can be used directly from the browser.\u000a* [http://ctext.org/tools/concordance Chinese Text Project Concordance Tool] - Concordance lookup and discussion of the continued importance of printed concordances in [[Sinology]] - [[Chinese Text Project]]\u000a* [http://khc.sourceforge.net/en/ KH Coder] - A free software for KWIC concordance and collocation stats generation. Various statistical analysis functions are also available such as co-occurrence network, multidimensional scaling, hierarchical cluster analysis, and correspondence analysis of words.\u000a\u000a{{DEFAULTSORT:Concordance (Publishing)}}\u000a[[Category:Concordances (publishing)| ]]\u000a[[Category:Indexing]]\u000a[[Category:Searching]]\u000a[[Category:Library science]]\u000a[[Category:Information science]]\u000a[[Category:Reference works]]
p158
sg6
S'Concordance (publishing)'
p159
ssI59
(dp160
g2
S'http://en.wikipedia.org/wiki/Search-based application'
p161
sg4
V'''Search-based applications''' ('''SBA''') are [[software applications]] in which a [[Search engine|search engine platform]] is used as the core infrastructure for information access and reporting. SBAs use [[Semantic technology|semantic technologies]] to aggregate, normalize and classify [[Unstructured data|unstructured]], [[Semi-structured data|semi-structured]] and/or [[Structured data|structured content]] across multiple repositories, and employ [[Natural language processing|natural language technologies]] for accessing the aggregated information.\u000a\u000a== Pre-Conditions ==\u000a\u000aSearch based applications are fully packaged applications that:<ref>Worldwide Search and Discovery 2009 Vendor Shares: An Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.</ref>\u000a* Are built on a search backbone to enable sub-second access to information in multiple formats and from multiple sources\u000a* Are delivered as a unified work environment to support a specific task or workflow, for example: eDiscovery, financial services regulatory compliance, fraud detection, voice of the customer, sales prospecting, pharmaceutical research, anti-terrorism intelligence, or customer support.\u000a* Integrate all the tools that are commonly needed for that specific task or workflow, including:\u000a** Multi-source information access\u000a** Authoring\u000a** Collaboration\u000a** Business process\u000a** Reporting and analysis\u000a** Alerting\u000a** Visualization\u000a* Provide pre-configured data integration with multiple repositories of information in multiple formats as appropriate for the application domain.\u000a* Integrate domain knowledge to support the particular task, including industry taxonomies and vocabularies, internal processes, workflow for the task, connectors to specialized collections of information, and decision heuristics typical of the field.\u000a* Provide a compelling user interface and interaction design that eliminates the need for users to \u201cpogo stick\u201d or continually jump from one application to another. This buffers the user from the complexity of operating separate applications and enables them to focus on getting work done.\u000a* Are quick to deploy, easy to customize or extend, and economical to administer\u000a\u000a== Practical Uses ==\u000a\u000aSBAs are used for a variety of purposes, including:\u000a\u000a* ''' Enterprise Business Applications:''' For example, [[Customer Relationship Management]] (CRM), [[Enterprise Resource Planning]] (ERP), [[Supply Chain Management]] (SCM), Compliance & Discovery, and [[Business Intelligence]] (BI)\u000a\u000a* ''' Web Applications:''' Typically, B2B, B2C and C2B applications that [[Mashup (digital)|mash-up]] data and functionality from diverse sources (databases, Web content, user-generated content, mapping data and functions, etc.)\u000a\u000aThe use of a search platform as the core infrastructure for software applications has been enabled largely by two search engine features:  1) Scalability 2) Ad hoc access to multiple heterogeneous sources from a single point of access.\u000a\u000aSearch based applications have proven popular and effective because they provide a dynamic, scalable access infrastructure that can be integrated with other features that information workers need:  task-specific, and easy to use work environments that integrate features that are usually designed to be used as separate applications, collaborative features, domain knowledge, and security.\u000a\u000aSearch engines are not a replacement for database systems; they are a complement. They have been optimally engineered to facilitate access to information, not to record and store transactions. In addition, the mathematical and statistical processors integrated to date into search engines remain relatively simple. At present, therefore, databases still provide a more effective structure for complex analytical functions.Search applications also focus on providing quality results considering search relevancy.\u000a\u000a==References==\u000a{{Reflist}}\u000a\u000a==Further Reading==\u000a* Worldwide Search and Discovery 2009 Vendor Shares: An Update on Market Trends, IDC #223926, July, 2010 by Susan Feldman and Hadley Reynolds.\u000a* Butler Group [http://www.butlergroup.com/webinarIntroduction.asp?mcr=EXA190509&scr=EXA190509 Webinar on Search Based Applications] explaining SBA and how they work\u000a* Presentation on [http://www.informationbuilders.com/support/developers/presentations/?109 Search Based Applications] by   [[Information Builders]]\u000a* IDC Executive Brief [http://www.exalead.com/software/forms/download.php?resourceid=69 "The Information Advantage: Information Access in Tomorrow's Enterprise,"] October 2009, downloadable from the [[Exalead|Exalead.com]] website. Adapted from [http://www.idc.com/getdoc.jsp?containerId=217936 Hidden Costs of Information Work: A Progress Report] and [http://www.idc.com/getdoc.jsp?containerId=219883 Worldwide Search and Discovery Software 2009\u20132013 Forecast Update and 2008 Vendor Shares] by Susan Feldman, IDC.\u000a* IDC [http://www.kmworld.com/downloads/66062/Search_Market_Map_Chart.pdf Search and Discovery Software: 2009 Market Map]\u000a* KMWorld article [http://www.kmworld.com/Articles/Editorial/Feature/Search-based-applications-support-critical-decision-making-66062.aspx Search-based applications support critical decision making]\u000a* Kellblog post [http://www.kellblog.com/2010/02/11/idcs-definiton-of-search-based-applications/ IDC's Definition of Search-Based Applications]\u000a* Steve-Kearns' [http://www.basistech.com/knowledge-center/search/2010-05-building-multilingual-search-based-applications.pdf Building Multilingual Search Based Applications] presentation at Apache Lucene EuroCon 2010 conference\u000a* Information Today article [http://newsbreaks.infotoday.com/NewsBreaks/Attivio-Upgrades-Its-Active-Intelligence-Engine-67608.asp Attivio Upgrades Its Active Intelligence Engine]\u000a* [http://lucidworks.com/blog/debugging-search-application-relevance-issues/ Debugging Search Application Relevance Issues] by Grant Ingersoll. Accessed October 22, 2014.\u000a* [http://www.mind7.fr/en/information_intelligence.html Explanatory video on SBA's and Content Analysis]\u000a\u000a== See also ==\u000a{{col-begin}}\u000a{{col-2}}\u000a* [[Agile application]]\u000a* [[Agile development]]\u000a* [[Business Intelligence 2.0]] (BI 2.0)\u000a* [[Enterprise Search]]\u000a* [[Search oriented architecture]]\u000a* [[Software as a service]]\u000a* [[Lookeen]]\u000a* [[Lucene]]\u000a* [[Exalead]]\u000a{{col-end}}\u000a\u000a<!-- Categories -->\u000a[[Category:Enterprise application integration]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines| ]]\u000a[[Category:Internet terminology]]
p162
sg6
S'Search-based application'
p163
ssI188
(dp164
g2
S'http://en.wikipedia.org/wiki/Search-based software engineering'
p165
sg4
V{{copy edit|date=October 2013}}\u000a\u000a{{Use dmy dates|date=November 2011}}\u000a\u000a'''Search-based software engineering''' ('''SBSE''') is an approach to apply [[metaheuristic]] search techniques like [[genetic algorithms]], [[simulated annealing]] and [[tabu search]] to [[software engineering]] problems. It is inspired by the observation that many activities in [[software engineering]] can be formulated as [[Optimization (mathematics)|optimization]] problems. Due to the [[computational complexity]] of these problems, exact [[Optimization (mathematics)|optimization]] techniques of [[operations research]] like [[linear programming]] or [[dynamic programming]] are mostly impractical for large scale [[software engineering]] problems. Because of this, researchers and practitioners have used [[metaheuristic]] search techniques to find near optimal or good-enough solutions.\u000a\u000aBroadly speaking SBSE problems can be divided into two types. The first are black-box optimization problems, for example, assigning people to tasks (a typical [[combinatorial optimization]] problem). \u000aWith this sort of problem domain, the underlying problem could have come from the software industry, but equally it could have originated from any domain where people are assigned tasks. \u000aThe second type are white-box problems where operations on source code need to be considered.<ref>\u000a{{Cite conference\u000a| doi = 10.1109/SCAM.2010.28\u000a| conference = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)\u000a| pages = 7\u201319\u000a| last = Harman\u000a| first = Mark\u000a| title = Why Source Code Analysis and Manipulation Will Always be Important\u000a| booktitle = 10th IEEE Working Conference on Source Code Analysis and Manipulation (SCAM 2010)\u000a| year = 2010\u000a}}</ref>\u000a\u000a__TOC__\u000a\u000a==Definition==\u000a\u000aThe basic idea of SBSE is to take a software engineering problem and convert it into a computational search problem which can be tackled with a [[metaheuristic]]. \u000aThis essentially involves a number of stages. Firstly defining a search space (the set of possible solutions to the problem). \u000aThis space is typically too large to be explored exhaustively and therefore a  [[metaheuristic]] is employed to sample this space. \u000aSecondly, a metric <ref>\u000a{{Cite conference\u000a| doi = 10.1109/METRIC.2004.1357891\u000a| conference = 10th International Symposium on Software Metrics, 2004\u000a| pages = 58\u201369\u000a| last = Harman\u000a| first = Mark\u000a|author2=John A. Clark\u000a | title = Metrics are fitness functions too\u000a| booktitle = Proceedings of the 10th International Symposium on Software Metrics, 2004 \u000a| year = 2004\u000a}}</ref> (also called a fitness function, cost function, objective function or quality measure) is used to measure the quality of a potential solution. Many software engineering problems can be reformulated as a computational search problem.<ref>{{Cite journal\u000a| doi = 10.1049/ip-sen:20030559\u000a| issn = 1462-5970\u000a| volume = 150\u000a| issue = 3\u000a| pages = 161\u2013175\u000a| last = Clark\u000a| first = John A.\u000a| coauthors = Dolado, José Javier; Harman, Mark; Hierons, Robert M.; Jones, Bryan F.; Lumkin, M.; Mitchell, Brian S.; Mancoridis, Spiros; Rees, K.; Roper, Marc; Shepperd, Martin J.\u000a| title = Reformulating software engineering as a search problem\u000a| journal = IEE Proceedings - Software \u000a| year = 2003\u000a}}</ref>\u000a\u000aThe term "[[search-based application]]", in contrast, refers to using [[search engine technology]], rather than search techniques, in another industrial application.\u000a\u000a==Brief history==\u000a\u000aOne of the earliest attempts in applying [[Optimization (mathematics)|optimization]] to a [[software engineering]] problem was reported by [[Webb Miller]] and David Spooner in 1976 in the area of software testing.<ref>\u000a{{Cite journal\u000a| doi = 10.1109/TSE.1976.233818\u000a| issn = 0098-5589\u000a| volume = SE-2\u000a| issue = 3\u000a| pages = 223\u2013226\u000a| last = Miller\u000a| first = Webb\u000a| last2 = Spooner\u000a| first2 = David L. \u000a| title = Automatic Generation of Floating-Point Test Data\u000a| journal = IEEE Transactions on Software Engineering\u000a| year = 1976\u000a}}</ref> \u000aIn 1992, Xanthakis and his colleagues applied a search technique to a [[software engineering]] problem for the first time.<ref>S. Xanthakis, C. Ellis, C. Skourlas, A. Le Gall, S. Katsikas and K. Karapoulios, "Application of genetic algorithms to software testing," in ''Proceedings of the 5th International Conference on Software Engineering and its Applications'', Toulouse, France, 1992, pp.&nbsp;625\u2013636</ref> \u000aThe term SBSE was first used in 2001 by [[Mark Harman (computer scientist)|Harman]] and Jones.<ref>\u000a{{Cite journal\u000a| doi = 10.1016/S0950-5849(01)00189-6\u000a| issn = 0950-5849\u000a| volume = 43\u000a| issue = 14\u000a| pages = 833\u2013839\u000a| last = Harman\u000a| first = Mark\u000a| last2 = Jones\u000a| first2 = Bryan F.\u000a| title = Search-based software engineering\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-10-31\u000a| date = 2001-12-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S0950584901001896\u000a}}</ref> Since then, the research community has grown to include more than 800 authors in 2013, from approximately 270 institutions in 40 countries.{{Citation needed|date=October 2013}}\u000a\u000a==Application areas==\u000a\u000aSearch-based software engineering is applicable to almost all phases of the [[software life cycle|software development process]]. [[Software testing]] has been one of the major applications of search techniques in [[software engineering]].<ref>\u000a{{Cite journal\u000a| doi = 10.1002/stvr.294\u000a| issn = 1099-1689\u000a| volume = 14\u000a| issue = 2\u000a| pages = 105\u2013156\u000a| last = McMinn\u000a| first = Phil\u000a| title = Search-based software test data generation: a survey\u000a| journal = Software Testing, Verification and Reliability\u000a| accessdate = 2013-10-31\u000a| year = 2004\u000a| url = http://onlinelibrary.wiley.com/doi/10.1002/stvr.294/abstract\u000a}}</ref> Search techniques have also been applied to other [[software engineering]] activities, for instance, [[requirements analysis]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/j.infsof.2003.07.002\u000a| issn = 0950-5849\u000a| volume = 46\u000a| issue = 4\u000a| pages = 243\u2013253\u000a| last = Greer\u000a| first = Des\u000a| last2 = Ruhe\u000a| first2 = Guenther\u000a| title = Software release planning: an evolutionary and iterative approach\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-09-06\u000a| date = 2004-03-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S095058490300140X\u000a}}</ref>\u000a<ref>{{Cite conference\u000a| doi = 10.1109/SBES.2009.23\u000a| conference = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09\u000a| pages = 207\u2013215\u000a| last = Colares\u000a| first = Felipe\u000a| last2 = Souza\u000a| first2 = Jerffeson\u000a| last3 = Carmo\u000a| first3 = Raphael\u000a| last4 = Pádua\u000a| first4 = Clarindo\u000a| last5 = Mateus\u000a| first5 = Geraldo R.\u000a| title = A New Approach to the Software Release Planning\u000a| booktitle = XXIII Brazilian Symposium on Software Engineering, 2009. SBES '09\u000a| year = 2009\u000a}}</ref> [[software design]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/S0950-5849(01)00195-1\u000a| issn = 0950-5849\u000a| volume = 43\u000a| issue = 14\u000a| pages = 891\u2013904\u000a| last = Clark\u000a| first = John A.\u000a| last2 = Jacob\u000a| first2 = Jeremy L. \u000a| title = Protocols are programs too: the meta-heuristic search for security protocols\u000a| journal = Information and Software Technology\u000a| accessdate = 2013-10-31\u000a| date = 2001-12-15\u000a| url = http://www.sciencedirect.com/science/article/pii/S0950584901001951\u000a}}</ref> [[software development]],<ref>\u000a{{Cite journal\u000a| doi = 10.1016/j.ins.2006.12.020\u000a| issn = 0020-0255\u000a| volume = 177\u000a| issue = 11\u000a| pages = 2380\u20132401\u000a| last = Alba\u000a| first = Enrique\u000a| last2 = Chicano\u000a| first2 = J. Francisco \u000a| title = Software project management with GAs\u000a| journal = Information Sciences\u000a| accessdate = 2013-10-31\u000a| date = 2007-06-01\u000a| url = http://www.sciencedirect.com/science/article/pii/S0020025507000175\u000a}}</ref> and [[software maintenance]].<ref>\u000a{{Cite conference\u000a| doi = 10.1109/ICSM.2005.79\u000a| conference = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05\u000a| pages = 240\u2013249\u000a| last = Antoniol\u000a| first = Giuliano\u000a| last2 = Di Penta\u000a| first2 = Massimiliano \u000a| last3 = Harman\u000a| first3 = Mark\u000a| title = Search-based techniques applied to optimization of project planning for a massive maintenance project\u000a| booktitle = Proceedings of the 21st IEEE International Conference on Software Maintenance, 2005. ICSM'05\u000a| year = 2005\u000a}}</ref>\u000a\u000a===Requirements engineering===\u000a\u000a[[Requirements engineering]] is the process by which the needs of a software's users and environment are determined and managed. Search-based methods have been used for requirements selection and optimisation with the goal of finding the best possible subset of requirements that matches users' requests and different constraints such as limited resources and interdependencies between requirements. This problem is often tackled as a [[MCDM|multiple-criteria decision-making]] problem and, generally speaking, involves presenting the decision maker with a range of good compromises between cost and user satisfaction.<ref>\u000a{{Cite thesis\u000a| type = Ph.D.\u000a| publisher = University of London\u000a| last = Zhang\u000a| first = Yuanyuan\u000a| title = Multi-Objective Search-based Requirements Selection and Optimisation\u000a| location = Strand, London, UK\u000a| date = February 2010\u000a| url = http://eprints.ucl.ac.uk/170695/\u000a}}</ref>\u000a<ref>\u000aY.&nbsp;Zhang and M.&nbsp;Harman and S.&nbsp;L.&nbsp;Lim, "[http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/images/Research_Student_Information/RN_11_12.pdf Search Based Optimization of Requirements Interaction Management]," Department of Computer Science, University College London, Research Note RN/11/12, 2011.\u000a</ref>\u000a\u000a===Debugging and maintenance===\u000a\u000aIdentifying a [[software bug]] (or a [[code smell]]) and then [[debugging]] (or [[refactoring]]) the software is largely a manual and labor-intensive endeavor, though the process is supported by a number of tools. One objective of SBSE is to automatically identify bugs (for example via [[mutation testing]]), then automatically fix them.\u000a\u000a[[Genetic programming]], a biologically-inspired technique which involves evolving programs through the use of crossover and mutation, has been used to search for repairs to programs by altering a few lines of source code. The [http://dijkstra.cs.virginia.edu/genprog/ GenProg Evolutionary Program Repair] software was shown to be able to repair 55 out of 105 bugs for approximately $8 each.<ref>{{Cite conference\u000a| doi = 10.1109/ICSE.2012.6227211\u000a| conference = 2012 34th International Conference on Software Engineering (ICSE)\u000a| pages = 3\u201313\u000a| last = Le Goues\u000a| first = Claire\u000a| last2 = Dewey-Vogt\u000a| first2 = Michael\u000a| last3 = Forrest\u000a| first3 = Stephanie\u000a| last4 = Weimer\u000a| first4 = Westley \u000a| title = A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each\u000a| booktitle = 2012 34th International Conference on Software Engineering (ICSE)\u000a| year = 2012\u000a}}</ref>\u000a\u000a[[Coevolution]] has also been used as an approach. It follows a predator and prey metaphor where a population of programs and a population of [[Unit Testing|unit tests]] evolve together and influence each other.<ref>{{Cite conference\u000a| doi = 10.1109/CEC.2008.4630793\u000a| conference = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)\u000a| pages = 162\u2013168\u000a| last = Arcuri\u000a| first = Andrea\u000a| last2 = Yao\u000a| first2 = Xin \u000a| title = A novel co-evolutionary approach to automatic software bug fixing\u000a| booktitle = IEEE Congress on Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence)\u000a| year = 2008\u000a}}</ref>\u000a\u000a===Testing===\u000a\u000aSearch-based software engineering has been applied to software testing, including automatic generation of test cases (test data), test case minimization and test case prioritization. [[Regression testing]] has also received some attention.\u000a\u000a===Optimizing software===\u000aThe use of SBSE in [[program optimization]], or modifying a piece of software to make it more efficient in terms of speed and resource use, has been the object of developing research interest and success. Genetic programming has been used to improve programs. In one instance, a 50,000 line program was genetically improved, resulting in a program 70 times faster on average.<ref>\u000a{{Cite journal\u000a| last = Langdon\u000a| first = William B.\u000a| last2 = Harman\u000a| first2 = Mark \u000a| title = Optimising Existing Software with Genetic Programming\u000a| journal = IEEE Transactions on Evolutionary Computation\u000a| url = http://www0.cs.ucl.ac.uk/staff/w.langdon/ftp/papers/Langdon_2013_ieeeTEC.pdf\u000a}}</ref>\u000a\u000a===Project management===\u000aA number of decisions which are normally made by a project manager can be done automatically, for example, project scheduling.<ref>\u000a{{Cite conference\u000a| publisher = ACM\u000a| doi = 10.1145/2330163.2330332\u000a| isbn = 978-1-4503-1177-9\u000a| pages = 1221\u20131228\u000a| last = Minku\u000a| first = Leandro L.\u000a| last2 = Sudholt\u000a| first2 = Dirk\u000a| last3 = Yao\u000a| first3 = Xin \u000a| title = Evolutionary algorithms for the project scheduling problem: runtime analysis and improved design\u000a| booktitle = Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference\u000a| location = New York, NY, USA\u000a| series = GECCO '12\u000a| accessdate = 2013-10-31\u000a| year = 2012\u000a| url = http://doi.acm.org/10.1145/2330163.2330332\u000a}}</ref>\u000a\u000a==Tools==\u000a\u000aThere are a number of tools available for SBSE approaches. These include tools like [[OpenPAT]].<ref> \u000a{{cite conference\u000a|ref        = harv\u000a|last       = Mayo\u000a|first      = M.\u000a|coauthors  = Spacey, S.\u000a|title      = Predicting Regression Test Failures Using Genetic Algorithm-Selected Dynamic Performance Analysis Metrics\u000a|url        = http://rd.springer.com/chapter/10.1007/978-3-642-39742-4_13\u000a|format     = PDF\u000a|journal    = Proceedings of the 5th International Symposium on Search-Based Software Engineering (SSBSE)\u000a|volume     = 8084\u000a|pages      = 158\u2013171\u000a|year       = 2013\u000a}}</ref>\u000aand Evosuite <ref>(http://www.evosuite.org/)</ref>\u000aand a code coverage measurement for Python\u000a<ref>\u000ahttps://pypi.python.org/pypi/coverage\u000a</ref>\u000a\u000a==Methods and techniques==\u000a\u000aThere are a number of methods and techniques available. \u000aA non-exhaustive list of these tools includes:\u000a\u000a\u2022[[profiling (computer programming)|Profiling]]\u000a<ref>http://java-source.net/open-source/profilers</ref> via [[instrumentation]] in order to monitor certain parts of a program as it is executed.\u000a\u000a\u2022Obtaining an [[abstract syntax tree]] associated with the program, which can be automatically examined to gain insights into the structure of a program.\u000a\u000a\u2022Applications of [[program slicing]] relevant to SBSE include [[software maintenance]], [[Optimization (computer science)|optimization]], [[Program analysis (computer science)|program analysis]].\u000a\u000a\u2022[[Code coverage]] allows measuring how much of the code is executed with a given \u000aset of input data.\u000a\u000a\u2022[[Static program analysis]]\u000a\u000a==Industry acceptance==\u000a\u000aAs a relatively new area of research, SBSE does not yet benefit from broad industry acceptance. One issue is that software engineers are reluctant to adopt tools over which they have little control or that generate solutions that are quite different from the ones humans would produce.<ref>\u000a{{cite web\u000a |url        = http://shape-of-code.coding-guidelines.com/2013/10/18/programming-using-genetic-algorithms-isnt-that-what-humans-already-do/\u000a |title      = Programming using genetic algorithms: isn\u2019t that what humans already do ;-)\u000a |last       = Jones\u000a |first      = Derek\u000a |date       = 18 October 2013\u000a |website    = The Shape of Code\u000a |accessdate = 31 October 2013\u000a}}\u000a</ref>\u000aIn the context of SBSE use in fixing or improving programs, developers need to be confident that any automatically produced modification does not generate unexpected behavior outside the scope of a system's requirements and testing environment. Considering that fully automated programming has yet to be achieved, a desirable property of such modifications would be that they need to be easily understood by humans to favor program maintainability.<ref>\u000a{{Cite journal\u000a| doi = 10.1007/s11219-013-9208-0\u000a| issn = 1573-1367\u000a| volume = 21\u000a| issue = 3\u000a| pages = 421\u2013443\u000a| last = Le Goues\u000a| first = Claire\u000a| last2 = Forrest \u000a| first2 = Stephanie \u000a| last3 = Weimer\u000a| first3 = Westley\u000a| title = Current challenges in automatic software repair\u000a| journal = Software Quality Journal\u000a| accessdate = 2013-10-31\u000a| date = 2013-09-01\u000a| url = http://link.springer.com/article/10.1007/s11219-013-9208-0\u000a}}\u000a</ref>\u000a\u000aAnother concern is that SBSE might make the software engineer redundant. Researchers have argued that, on the contrary, the motivation for SBSE is to enhance the relationship between the engineer and the program.<ref>\u000a{{Cite conference\u000a| publisher = IEEE Press\u000a| conference = First International Workshop on Combining Modelling with Search-Based Software Engineering,First International Workshop on Combining Modelling with Search-Based Software Engineering\u000a| pages = 49\u201350\u000a| last = Simons\u000a| first = Christopher L.\u000a| title = Whither (away) software engineers in SBSE?\u000a| location = San Francisco, USA\u000a| accessdate = 2013-10-31\u000a| date = May 2013\u000a| url = http://eprints.uwe.ac.uk/19938/\u000a}}</ref>\u000a\u000a==See also==\u000a{{Portal|Software Testing}}\u000a*[[Program analysis (computer science)]]\u000a*[[Dynamic program analysis]]\u000a\u000a==References==\u000a{{reflist|colwidth=30em}}\u000a\u000a==External links==\u000a*[http://crestweb.cs.ucl.ac.uk/resources/sbse_repository/ Repository of publications on SBSE]\u000a*[http://neo.lcc.uma.es/mase/ Metaheuristics and Software Engineering]\u000a*[http://sir.unl.edu/portal/index.php  Software-artifact Infrastructure Repository]\u000a*[http://2013.icse-conferences.org/ International Conference on Software Engineering]\u000a*[http://www.sigevo.org/wiki/tiki-index.php Genetic and Evolutionary Computation (GECCO)]\u000a*[http://scholar.google.co.uk/citations?view_op=search_authors&hl=en&mauthors=label:sbse Google Scholar page on Search-based software engineering]\u000a\u000a[[Category:2001 introductions]]\u000a[[Category:Software engineering]]\u000a[[Category:Software testing]]\u000a[[Category:Searching]]\u000a[[Category:Search algorithms]]\u000a[[Category:Optimization algorithms and methods]]\u000a[[Category:Genetic algorithms]]
p166
sg6
S'Search-based software engineering'
p167
ssI62
(dp168
g2
S'http://en.wikipedia.org/wiki/Isearch'
p169
sg4
V{{for|the adware|Isearch (malware)}}\u000a\u000a'''Isearch''' is [[open-source software|open-source]] [[text retrieval]] software first developed in 1994 by Nassib Nassar as part of the Isite [[Z39.50]] information framework. The project started at the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) of the North Carolina supercomputing center MCNC and funded by the [[National Science Foundation]] to follow in the track of [[Wide Area Information Server|WAIS]] and develop prototype systems for distributed information networks encompassing Internet applications, library catalogs and other information resources.\u000a\u000aThe main features of Isearch include full text and field searching, relevance ranking, Boolean queries, and support for many document types such as HTML, mail folders, list digests, MEDLINE, BibTeX, SGML/XML, FGDC Metadata, NASA DIF, ANZLIC metadata, ISO 19115 metadata and many other resource types and document formats.\u000a\u000aIt was the first search engine to be designed from the ground up to support [[SGML]] and ISO [[Z39.50]] search and retrieval. It included many innovations including the "document type" model\u2014which is simply a (object oriented) method of associating each document with a class of functions providing a standard interface for accessing the document. It was one of the first engines (if not the first) to ever support XML.\u000a\u000aThe Isearch search/indexing text algorithms were based on [[Gaston Gonnet]]'s seminal work into PAT arrays and trees for text retrieval--- ideas that were developed for the New Oxford English Dictionary Project at the Univ. of Waterloo, and provided the seeds for [[Tim Bray]]'s PAT SGML engine that formed the basis of [[Open Text]]. One of the limiting factors, however, of the  Isearch design was that it was not well suited to handle the extremely large data sets that became popular in the mid to late 1990s. In many cases Isearch was adapted or modified to use different algorithms but usually retained the document type model and the architectural relationship with Isite.\u000a\u000aIsearch was widely adopted and used in hundreds of public search sites, including  many high profile projects such as the [http://patft1.uspto.gov/ U.S. Patent and Trademark Office (USPTO) patent search],[http://clearinghouse3.fgdc.gov/  the Federal Geographic Data Clearinghouse (FGDC)], the NASA Global Change Master Directory, the NASA EOS Guide System, the NASA Catalog Interoperability Project, the Astronomical pre-print service based at the Space Telescope Science Institute, The PCT Electronic Gazette at the World Intellectual Property Organization (WIPO), Linsearch (a search engine for Open Source Software designed by Miles Efron), the SAGE Project of the Special Collections Department at Emory University, Eco Companion Australasia (an environmental geospatial resources catalog), Australian National Genomic Information Service (ANGIS), the [[Open Directory Project]] and numerous governmental portals in the context of the Government Information Locator Service (GILS) [[United States Government Printing Office|GPO]] mandate (ended in 2005?).\u000a\u000aFrom 1994 to 1998 most of the development was centered around the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) in North Carolina (Engine core) and BSn in Germany (Doctypes). By 1998 much of the open-source Isearch core developers re-focused development into several spin-offs. In 1998 it became part of the Advanced Search Facility reference software platform funded by the U.S. Department of Commerce.\u000a\u000aA/WWW Enterprises now maintains the open source version for public usage, supported by paying government clients, such as the U.S. Patent and Trademark Office, NASA, and the FGDC who have provided support to enhance the functionality and reliability of the software. The software suite is considered a reference implementation of catalog service software.\u000a\u000aAs of 2010, the open source version of Isearch is still used on 250+ nodes of FGDC, and by ANZLIC in Australia and selected Geospatial OneStop contributors to facilitate harvesting by GOS, including NOAA, Census Bureau and the Tenn. Field Office of the US Fish and Wildlife Service, among others.\u000a\u000a==References==\u000a*[http://www.springerlink.com/content/g5e2wfd0lekygvut/ Application of Metadata Concepts to Discovery of Internet Resources]\u000a*[http://www.springerlink.com/content/b5chmkgx8akg4m2h/ An Operational Metadata Framework for Searching, Indexing, and Retrieving Distributed Geographic Information Services on the Internet]\u000a* The UNIX Web Server Book, Second Edition, by R. Douglas Matthews et al. (Ventana Press, 1997).\u000a* [http://www.webtechniques.com/archives/1997/05/nassar/  "Searching With Isearch". May 1997, Web Techniques]\u000a* [http://www.itl.nist.gov/fipspubs/fip192.htm FIPS-192: APPLICATION PROFILE FOR THE GOVERNMENT INFORMATION LOCATOR SERVICE (GILS)]\u000a* [http://www.uneca.org/awich/AWICH%20Workshop/YaoundeWorkshop/Clearinghouse%20Yaounde.pdf Clearinghouse and Metadata Concepts, Danel Behanu, U.N. Economic Commission for Africa,  2004]\u000a* [http://web.archive.org/web/19991006225226/http://www.whitehouse.gov/OMB/memoranda/m9805.html M-98-05 Guidance on the Government Information Locator Service] published by the [[Office of Management and Budget|OMB]]\u000a* [http://www.hpcwire.com/archives/3149.html 01/1995 Press Release: Patent Office Launch Internet AIDS Patent Library]\u000a\u000a==External links==\u000a*[http://www.fgdc.gov/dataandservices/isite U.S. Federal Geographic Data Committee Isite]\u000a*[http://isite.awcubed.com/ Isite/Isearch2 Documentation Site]\u000a*[ftp://ftp.awcubed.com/pub/Software Current Isearch download site]\u000a*[http://www.etymon.com/tr.html Etymon: Isearch]\u000a*[http://www.ibu.de/node/52 BSn/NONMONOTONIC Lab: IB Search Engine], embeddable search engine. A commercial spin-off from the Isearch project.\u000a\u000a===Comparisons===\u000a* [http://www.ukoln.ac.uk/metadata/roads/product-comparison/  Product Comparison: Information Gateway Software]\u000a* [http://wrg.upf.edu/WRG/dctos/Middleton-Baeza.pdf  A Comparison of Open Source Search Engines, Christian Middleton, Ricardo Baeza-Yates]\u000a* [http://www.infomotions.com/musings/opensource-indexers/ Comparing Open Source Indexers]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Free search engine software]]
p170
sg6
S'Isearch'
p171
ssI191
(dp172
g2
S'http://en.wikipedia.org/wiki/Variable neighborhood search'
p173
sg4
V'''Variable neighborhood search''' (VNS),<ref>{{cite journal |pages=367\u2013407 |last1 = Hansen  |first1 = P.|last2 = Mladenovic|first2 = N.|last3 = Perez|first3 = J.A.M.|title=Variable neighbourhood search: methods and applications\u000a|volume=175 |journal= Annals of Operations Research |year=2010 |doi=10.1007/s10479-009-0657-6}}</ref> proposed by [[Mladenovi\u0107, Hansen]], 1997,<ref name=".....">{{cite journal\u000a | author = Nenad Mladenovi´c, Pierre Hansen\u000a | year = 1997\u000a | title = Variable neighborhood search\u000a | journal = Computers and Operations Research\u000a | volume = 24\u000a | issue= 11\u000a | pages = 1097\u20131100\u000a | doi=10.1016/s0305-0548(97)00031-2\u000a }}\u000a</ref> is a [[metaheuristic]] method for solving a set of [[combinatorial optimization (mathematics)|combinatorial optimization]] and global optimization problems.\u000aIt explores distant neighborhoods of the current incumbent solution, and moves from there to a new one if and only if an improvement was made. The local search method is applied repeatedly to get from solutions in the neighborhood to local optima.\u000aVNS was designed for approximating solutions of discrete and continuous optimization problems and according to these, it is aimed for solving [[linear programming|linear program]] problems, [[linear programming|integer program]] problems, mixed integer program problems, [[nonlinear programming|nonlinear program]] problems, etc.\u000a\u000a== Introduction ==\u000aVNS systematically changes the neighborhood in two phases: firstly, descent to find a [[local optimum]] and finally, a perturbation phase to get out of the corresponding valley.\u000a\u000aApplications are rapidly increasing in number and pertain to many fields: [[location theory]], [[cluster analysis]], [[scheduling]], [[Vehicle routing problem|vehicle routing]], [[Network planning and design|network design]], lot-sizing, [[artificial intelligence]], engineering, pooling problems, biology, [[Phylogenetics|phylogeny]], [[wikt:reliability|reliability]], geometry, telecommunication design, etc.\u000a\u000aThere are several books important for understanding VNS, such as: ''Handbook of Metaheuristics'', 2010,<ref>{{cite journal |last1=Gendreau|  first1=M.|last2= Potvin|first2=J-Y.|title=Handbook of Metaheuristics|publisher =Springer|year=2010 }}</ref> Handbook of Metaheuristics, 2003<ref>{{cite journal|last1=Glover|  first1=F.|last2= Kochenberger|first2=G.A.|title=Handbook of Metaheuristics|publisher = Kluwer Academic Publishers |year=2003}}</ref> and Search methodologies, 2005.<ref>{{cite journal |last1=Burke|first1=EK.|last2= Kendall | first2=G.| title=Search methodologies. Introductory tutorials in optimization and decision support techniques |journal = Springer|year=2005}}</ref>\u000aEarlier work that motivated this approach can be found in\u000a# Davidson, W.C.,<ref>{{cite journal |last1=Davidson  |first1=W.C.|title=Variable metric algorithm for minimization  |journal= Argonne National Laboratory Report ANL-5990 |year=1959 }}</ref>\u000a# Fletcher, R., Powell, M.J.D.,<ref>{{cite journal |pages=163\u2013168 |last1=Fletcher |first1=R. |last2=Powell |first2=M.J.D. |title=Rapidly convergent descent method for minimization|volume=6 |journal=Comput.J. |year=1963 |doi=10.1093/comjnl/6.2.163}}</ref>\u000a# Mladenovi´c, N.<ref>{{cite journal |pages= 112 |last1=Mladenovi´c |first1=N. |title=A variable neighborhood algorithm\u2014a new metaheuristic for combinatorial optimization | journal=Abstracts of papers presented at Optimization Days, Montr´eal |year=1995 }}\u000a</ref> and 4. Brimberg, J., Mladenovi´c, N.<ref>{{cite journal |pages=1\u201312 |last1=Brimberg |first1=J. |last2 = Mladenovi´c |first2=N. |title=A variable neighborhood algorithm for solving the continuous location-allocation problem |volume=10 |journal=Stud. Locat. Anal. |year=1996}}</ref> Recent surveys on VNS  methodology as well as numerous applications can be found in 4OR, 2008.<ref>{{cite journal |pages=319\u2013360 |last1=Hansen |first1=P. |last2 = Mladenovi´c |first2=N. |last3= Perez| first3=J.A.M|title=Variable neighbourhood search: methods and applications|volume=6 |journal=4OR |year=2008 |doi=10.1007/s10288-008-0089-1}}</ref> and Annals of OR, 2010.\u000a\u000a== Basic description ==\u000aDefine one deterministic [[optimization problem]] with\u000a\u000a<math> \u005cmin {\u005c{f (x)|x \u005cin X, X \u005csubseteq S\u005c}} </math>, (1)\u000a\u000awhere ''S'', ''X'', ''x'', and ''f''  are the solution space, the feasible set, a feasible solution, and a real-valued [[mathematical optimization|objective function]], respectively. If ''S'' is a finite but large set, a combinatorial optimization problem is defined. If <math>{S = R^{n}}</math>, there is continuous optimization model.\u000a\u000aA solution <math>{x^* \u005cin X}</math> is optimal if\u000a\u000a<math> {f (x^{*}) \u005cleq f (x), \u005cqquad \u005cforall{x}\u005c, \u005cin X} </math>.\u000a\u000aExact algorithm for problem (1) is to be found an optimal solution ''x*'', with the validation of its optimal structure, or if it is unrealizable, in procedure have to be shown that there is no  achievable solution, i.e., <math>X =\u005cvarnothing</math>, or the solution is unbounded. CPU time has to be finite and short. For continuous optimization, it is reasonable to allow for some degree of tolerance, i.e., to stop when a feasible solution <math>x^{*}</math> has been found such that\u000a\u000a<math> {f (x^{*}) \u005cleq f (x) + \u005cepsilon, \u005cqquad \u005cforall{x}\u005c, \u005cin X} </math> or\u000a<math> {(f (x^{*})- f (x))/ f (x^{*})  <  \u005cepsilon  , \u005cqquad \u005cforall{x}\u005c, \u005cin X} </math>\u000a\u000aSome heuristics speedily accept an approximate solution, or optimal solution but one with no validation of its optimality.\u000aSome of them have an incorrect certificate, i.e., the solution <math>x_h</math> obtained satisfies\u000a\u000a<math> {(f (x_{h})- f (x))/ f (x_{h})  \u005cleq  \u005cepsilon  , \u005cqquad \u005cforall{x}\u005c, \u005cin X} </math>\u000afor some <math>\u005cepsilon</math>, though this is rarely small.\u000a\u000aHeuristics are faced with the problem of local optima as a result of avoiding boundless computing time.\u000aA local optimum <math>x_L</math> of problem is such that\u000a\u000a<math> {f (x_{L}) \u005cleq f (x), \u005cqquad \u005cforall{x}\u005c, \u005cin N(x_{L}) \u005ccap X} </math>\u000a\u000awhere <math> N(x_{L})</math>  denotes a neighborhood of <math> x_{L} </math>\u000a\u000a== Description ==\u000aAccording to (Mladenovic, 1995), VNS is a metaheuristic which systematically performs the procedure of neighborhood change, both in descent to local minima and in escape from the valleys which contain them.\u000a\u000aVNS is built upon the following perceptions:\u000a\u000a# A local minimum with respect to one neighbourhood structure is not necessarily a local minimum for another neighbourhood structure.\u000a# A global minimum is a local minimum with respect to all possible neighborhood structures.\u000a# For many problems, local minima with respect to one or several neighborhoods are relatively close to each other.\u000a\u000aUnlike many other metaheuristics, the basic schemes of VNS and its extensions are simple and require few, and sometimes no parameters. Therefore, in addition to providing very good solutions, often in simpler ways than other methods, VNS gives insight into the reasons for such a performance, which, in turn, can lead to more efficient and sophisticated implementations.\u000a\u000aThere are several papers where it could be studied among recently mentioned, such as (Hansen and Mladenovi´c 1999, 2001a, 2003, 2005; Moreno-Pérez et al.;<ref>{{cite journal||last1=Moreno-Pérez|first1=JA.|last2=Hansen|first2=P. |last3=Mladenovic|first3=N.| title = Parallel variable neighborhood search|journal=Alba E (ed) Parallel metaheuristics: a new class of algorithms|year=2005}}</ref>)\u000a\u000a==[[Local search (optimization)|Local search]]==\u000a\u000aA local search heuristic is performed through choosing an initial solution x, discovering a direction of descent from x, within a neighbourhood N(x), and proceeding to the minimum of f(x) within N(x) in the same direction. If there is no direction of descent, the heuristic stops; otherwise, it is iterated. Usually the highest direction of descent, also related to as best improvement, is used. This set of rules is summarized in Algorithm 1, where we assume that an initial solution x is given. The output consists of a local minimum, also denoted by x, and its value. Observe that a neighbourhood structure N(x) is defined for all x \u2208 X. At each step, the neighbourhood N(x) of x is explored completely. As this may be timeconsuming, an alternative is to use the first descent heuristic. Vectors <math>x^i \u005cin N(x)</math> are then enumerated systematically and a move is made as soon as a direction for the descent is found. This is summarized in Algorithm 2.\u000a\u000aAlgorithm 1 Best improvement (highest descent) heuristic\u000a\u000aFunction BestImprovement(x)\u000a\u000a  1: repeat\u000a  2:     x' \u2190 x\u000a  3:     x\u2190argmin_{f (y)}, y\u2208N(x)\u000a  4: until ( f (x) \u2265 f (x'))\u000a  5: return x\u000a\u000aAlgorithm 2 First improvement (first descent) heuristic\u000a\u000aFunction FirstImprovement(x)\u000a\u000a  1: repeat\u000a  2:    x' \u2190 x; i\u21900\u000a  3:    repeat\u000a  4:       i\u2190i+1\u000a  5:       x\u2190argmin{ f (x), f (x^i)}, x^i  \u2208 N(x)\u000a  6:    until ( f (x) < f (x^i) or i = |N(x)|)\u000a  7: until ( f (x) \u2265 f (x'))\u000a  8: return x\u000a\u000aLet one denote <math> \u005cmathcal{ N}_k(k=1, . . . ,k_{max}) </math>, a finite set of pre-selected neighborhood structures, and with <math>\u005cmathcal{N}_k(x)</math> the set of solutions in the ''kth'' neighborhood of ''x''.\u000a\u000aOne will also use the notation <math>\u005cmathcal{N'}_k(x), k = 1, . . . , k'_{max} </math> when describing local descent. Neighborhoods <math>\u005cmathcal{N}_k(x)</math> or <math>\u005cmathcal{N'}_k(x)</math> may be induced from one or more [[metric (mathematics)|metric]] (or quasi-metric) functions introduced into a solution space ''S''.\u000aAn optimal solution <math>x_{opt}</math> (or [[maxima and minima|global minimum]]) is a feasible solution where a minimum of problem ( is reached. We call ''x' \u2208 X'' a local minimum of problem with respect to <math>\u005cmathcal{N}_k(x) </math>, if there is no solution <math> x \u005cin \u005cmathcal{N'}_k(x) \u005csubseteq X </math> such that <math>f (x) < f (x')</math>.\u000a\u000aIn order to solve problem by using several neighbourhoods, facts 1\u20133 can be used in three different ways: (i) deterministic; (ii) [[stochastic]]; (iii) both deterministic and stochastic. We first give in Algorithm 3 the steps of the neighbourhood change function which will be used later. Function NeighbourhoodChange() compares the new value f(x') with the incumbent value f(x) obtained in the neighbourhood k (line 1). If an improvement is obtained, k is returned to its initial value and the new incumbent updated (line 2). Otherwise, the next neighbourhood is considered (line 3).\u000a\u000aAlgorithm 3&nbsp;\u2013 Neighborhood change\u000a\u000aFunction NeighborhoodChange (x, x', k)\u000a\u000a<code>\u000a 1: if f (x') < f(x) then\u000a 2:    x \u2190 x' // Make a move\u000a 3:    k \u2190 1 // Initial neighborhood\u000a 4: else\u000a 5:    k \u2190 k+1 // Next neighborhood\u000a\u000a</code>\u000a\u000aWhen VNS does not render good solution, there are several steps which could be helped in process, such as comparing first and best improvement strategies in local search, reducing neighborhood, intensifying shaking, adopting VND, adopting FSS, and experimenting with parameter settings.\u000a\u000aThe Basic VNS (BVNS) method (Mladenovic and Hansen 1997) combines deterministic and stochastic changes of neighbourhood. Its steps are given in Algorithm 4. Often successive neighbourhoods <math> \u005cmathcal{N}_k</math> will be nested. Observe that point x' is generated at random in Step 4 in order to avoid cycling, which might occur if a deterministic rule were applied. In Step 5, the first improvement local search (Algorithm 2) is usually\u000aadopted. However, it can be replaced with best improvement (Algorithm 1).\u000a\u000aAlgorithm 4: Basic VNS\u000a\u000aFunction VNS (x, kmax, tmax );\u000a\u000a<code>\u000a\u000a 1: repeat\u000a 2:    k \u2190 1;\u000a 3:    repeat\u000a 4:       x' \u2190Shake(x, k) /* Shaking */;\u000a 5:       x'' \u2190 FirstImprovement(x' ) /* Local search */;\u000a 6:       NeighbourhoodChange(x, x', k) /* Change neighbourhood */;\u000a 7:    until k = k_max ;\u000a 8:    t \u2190CpuTime()\u000a 9: until t > t_max ;\u000a\u000a</code>\u000a\u000aThe basic VNS is a first improvement [[method of steepest descent|descent method]] with randomization. Without much additional effort, it can be transformed into a descent-ascent method: in NeighbourhoodChange() function, replace also x by x" with some probability, even if the solution is worse than the incumbent. It can also be changed into a best improvement method: make a move to the best neighbourhood k* among all k_max of them.\u000aAnother variant of the basic VNS can be to find a solution x' in the \u201cShaking\u201d step as the best among b (a parameter) randomly generated solutions from the ''k''th neighbourhood. There are two possible variants of this extension: (1) to perform only one local search from the best among b points; (2) to perform all b local searches and then choose the best. In paper (Fleszar and Hindi<ref>{{cite journal|last1=Fleszar|first1=K|last2=Hindi|first2=KS|title=Solving the resource-constrained project scheduling problem by a variable neighborhood search|journal=Eur J Oper Res|year=2004|volume=155|issue=2|pages=402\u2013413|doi=10.1016/s0377-2217(02)00884-6}}</ref>) could be found algorithm.\u000a\u000a== Extensions ==\u000a* VND<ref>{{cite journal|last1=Brimberg|first1=J.|last2=Hansen|first2=P.|last3=Mladenovic|first3=N.|last4=Taillard |first4=E. |title=Improvements and comparison of heuristics for solving the multisource Weber problem|journal=Oper. Res.|year=2000|volume=48 |pages=444\u2013460 |doi=10.1287/opre.48.3.444.12431}}</ref>\u000a:The variable neighborhood descent (VND) method is obtained if a change of neighborhoods is performed in a deterministic way. In the descriptions :of its algorithms, we assume that an initial solution x is given. Most local search heuristics in their descent phase use very few :neighbourhoods. The final solution should be a local minimum with respect to all <math>k_{max}</math> neighbourhoods; hence the chances to reach :a global one are larger when using VND than with a single neighbourhood structure.\u000a* RVNS<ref>{{cite journal|last1=Mladenovic|first1=N.|last2=Petrovic|first2=J.|last3=Kovacevic-Vujcic|first3=V.|last4=Cangalovic |first4=M. |title=Solving spread spectrum radar polyphase code design problem by tabu search and variable neighborhood search|journal=Eur. J. Oper. Res.|year=2003b|volume=151 |pages=389\u2013399 |doi=10.1016/s0377-2217(02)00833-0}}</ref>\u000a\u000a:The reduced VNS (RVNS) method is obtained if random points are selected from <math>\u005cmathcal{N}_k(x)</math> and no descent is made. Rather, the :values of these new points are compared with that of the incumbent and an update takes place in case of improvement. It is assumed that a :stopping condition has been chosen like the maximum [[CPU time]] allowed <math>t_{max}</math> or the maximum number of iterations :between two improvements.\u000a:To simplify the description of the algorithms it is used <math>t_{max}</math> below. Therefore, RVNS uses two parameters: <math>t_{max}</math> :and <math>k_{max}</math>. RVNS is useful in very large instances, for which local search is costly. It has been observed that the best value for :the parameter k_max is often 2. In addition, the maximum number of iterations between two improvements is usually used as a stopping condition. :RVNS is akin to a [[Monte-Carlo method]], but is more systematic.\u000a* Skewed VNS\u000a:The skewed VNS (SVNS) method (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P.|last2=Jaumard|first2=B|last3=Mladenovi´c|first3=N|last4=Parreira |first4=A |title=Variable neighborhood search :for weighted maximum satisfiability problem|journal=Les Cahiers du GERAD G\u20132000\u201362, HEC Montréal, Canada|year=2000}}</ref> addresses the :problem of exploring valleys far from the incumbent solution. Indeed, once the best solution in a large region has been found, it is necessary to :go some way to obtain an improved one. Solutions drawn at random in distant neighbourhoods may differ substantially from the incumbent and VNS :can then degenerate, to some extent, into the Multistart heuristic (in which descents are made iteratively from solutions generated at random, a :heuristic which is known not to be very efficient). Consequently, some compensation for distance from the incumbent must be made.\u000a* Variable Neighbourhood Decomposition Search\u000a:The variable neighbourhood decomposition search (VNDS) method (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P|last2=Mladenovi´c|first2=N|last3=Pérez-Brito|first3=D |title=Variable neighborhood decomposition :search|journal=J Heuristics|year=2001|volume=7|issue=4|pages=335\u2013350}}</ref> extends the basic VNS into a two-level VNS scheme based upon :decomposition of the problem. For ease of presentation, but without loss of generality, it is assumed that the solution x represents the set of :some elements.\u000a* Parallel VNS\u000a:Several ways of parallelizing VNS have recently been proposed for solving the p-Median problem. In García-López et al.:<ref>{{cite journal|last1=García-López|first1=F|last2=Melián-Batista|first2=B|last3= Moreno-Pérez|first3= JA|last4= |first4=JM :|title=The parallel :variable neighborhood search for the p-median problem|journal=J Heuristics|year=2002|volume=8|issue=3|pages=375\u2013388}}</ref>&nbsp; three of them :are tested: (i) parallelize local search; (ii) augment the number of solutions drawn from the current neighbourhood and make a :local search in :parallel from each of them and (iii) do the same as (ii) but update the information about the best solution found. Three Parallel :VNS strategies :are also suggested for solving the [[Travelling purchaser problem]] in Ochi et al.<ref>{{cite journal|last1=Ochi|first1=LS|last2=Silva|first2=MB|last3= Drummond|first3= L|title=Metaheuristics based on GRASP and VNS for solving traveling purchaser :problem|journal=MIC\u20192001, Porto|year=2001|pages=489\u2013494}}</ref>\u000a* Primal-dual VNS\u000a:For most modern heuristics, the difference in value between the optimal solution and the obtained one is completely unknown. Guaranteed :performance of the primal heuristic may be determined if a [[upper and lower bounds|lower bound]] on the objective function value is known. To :this end, the standard approach is to relax the integrality condition on the primal variables, based on a mathematical programming formulation of :the problem.\u000a:However, when the dimension of the problem is large, even the relaxed problem may be impossible to solve exactly by standard :commercial solvers. :Therefore, it seems a good idea to solve dual relaxed problems heuristically as well. It was obtained guaranteed bounds on :the primal heuristics :performance.  In Primal-dual VNS (PD-VNS) (Hansen et al.)<ref>{{cite journal|last1=Hansen|first1=P|last2=Brimberg|first2=J|last3=Uro\u0161evi´c|first3=D|last4=Mladenovi´c|first4=N|title=Primal-dual variable neighborhood search for the simple plant location problem|journal=INFORMS J Comput|year=2007a|volume=19|issue=4|pages=552\u2013564|doi=10.1287/ijoc.1060.0196}}</ref> one :possible general way to attain both the guaranteed bounds and the exact solution is proposed.\u000a* Variable Neighborhood Branching.)<ref>{{cite journal|last1=Hansen|first1=P.|last2=Mladenovic|first2=N.|last3=Urosevic|first3=D.|title=Variable neighborhood search and local branching|journal=Computers and Operations Research|year=2006|volume=33|pages=3034\u20133045|doi=10.1016/j.cor.2005.02.033}}</ref>\u000a:The mixed integer linear programming (MILP) problem consists of maximizing or minimizing a linear function, subject to equality or inequality :constraints, and integrality restrictions on some of the variables.\u000a* Variable Neighborhood Formulation Space Search .)<ref>{{cite journal|last1=Mladenovic|first1=N.|last2=Plastria|first2=F.|author2-link=Frank Plastria|last3=Urosevic|first3=D.|title=Reformulation descent applied to circle packing problems|journal=Computers and Operations Research|year=2006|volume=32|pages=2419\u20132434|doi=10.1016/j.cor.2004.03.010}}</ref>\u000a:FSS is method which is very useful because, one problem could be defined in addition formulations and moving through formulations is legitimate. :It is proved that local search works within formulations, implying a final solution when started from some initial solution in first formulation. :Local search systematically alternates between different formulations which was investigated for [[Circle packing in a circle|circle packing]] :problem (CPP) where [[stationary point]] for a [[nonlinear programming]] formulation of CPP in [[Cartesian coordinate system|Cartesian coordinates]] is not strictly a stationary point in [[Polar coordinate system|polar coordinates]].\u000a\u000a== Development ==\u000aIn order to make a simple version of VNS, here is the list of steps which should be made. Most of it is very similar with steps in other metaheuristics.\u000a# It is necessary to be involved in problem, give some examples and try to solve them\u000a# Study books, surveys and scientific papers\u000a# Try to test some benchmarks\u000a# Choose appropriate data structure for representing in memory\u000a# Find initial solution\u000a# Calculate objective function\u000a# Design a procedure for Shaking\u000a# Choose an local search heuristic with some moves as drop, add, swap, interchange, etc.\u000a# Compare VNS with other methods from the literature\u000a\u000a== Applications ==\u000aApplications of VNS, or of varieties of VNS are very abundant and numerous. Some fields where it could be found collections of scientific papers:\u000a* Industrial applications\u000a* Design problems in communication\u000a* Location problems\u000a* [[Data mining]]\u000a* [[Graph theory|Graph problems]]\u000a* [[Knapsack problem|Knapsack]] and packing problems\u000a* Mixed integer problems\u000a* Time tabling\u000a* [[Scheduling]]\u000a* [[Vehicle routing problem]]s\u000a* [[Arc routing]] and waste collection\u000a* Fleet sheet problems\u000a* Extended vehicle routing problems\u000a* Problems in biosciences and chemistry\u000a* Continuous optimization\u000a* Other optimization problems\u000a* Discovery science\u000a\u000a== Conclusion ==\u000aVNS implies several features which are presented in Hansen and Mladenovic<ref>{{cite journal|last1=Hansen|first1=P|last2=Mladenovi´c|first2=N|title=Variable neighborhood search|journal=Glover F, Kochenberger G (eds) Handbook\u000aof Metaheuristics|year=2003|issue=Kluwer, Dordrecht|pages=145\u2013184}}</ref> and some are presented here:\u000a\u000a(i) Simplicity: VNS is simple a simple and clear which is universally applicable;\u000a\u000a(ii) Precision: VNS is formulated in precise mathematical definitions;\u000a\u000a(iii) Coherence: all actions of the heuristics for solving problems follow from the VNS principles;\u000a\u000a(iv) Effectiveness: VNS supplies optimal or near-optimal solutions for all or at least most realistic instances;\u000a\u000a(v) Efficiency: VNS takes a moderate computing time to generate optimal or near-optimal solutions;\u000a\u000a(vi) Robustness: the functioning of the VNS is coherent over a variety of instances;\u000a\u000a(vii) User friendliness: VNS has no parameters, so it is easy for understanding, expressing and using;\u000a\u000a(viii) Innovation: VNS is generating new types of application.\u000a\u000a(ix) Generality: VNS is inducing to good results for a wide variety of\u000aproblems;\u000a\u000a(x) Interactivity: VNS allows the user to incorporate his knowledge to improve the resolution process;\u000a\u000a(xi) Multiplicity: VNS is able to produce a certain near-optimal solutions from which the user can choose;\u000a\u000aInterest in VNS is growing quickly, evidenced by the increasing number of papers published each year on this topic (10 years ago, only a few; 5 years ago, about a dozen; and about 50 in 2007).\u000aMoreover, the 18th EURO mini-conference held in Tenerife in November 2005 was entirely devoted to VNS. It led to special issues of [[Institute of Mathematics and its Applications|IMA Journal of Management Mathematics]] in 2007, European Journal of Operational Research (http://www.journals.elsevier.com/european-journal-of-operational-research/), and Journal of Heuristics (http://www.springer.com/mathematics/applications/journal/10732/) in 2008.\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://toledo.mi.sanu.ac.rs/~grujicic/vnsconference EURO Mini Conference XXVIII on Variable Neighbourhood Search]\u000a\u000a[[Category:Searching]]
p174
sg6
S'Variable neighborhood search'
p175
ssI65
(dp176
g2
S'http://en.wikipedia.org/wiki/TeLQAS'
p177
sg4
S"'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.<ref>Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.</ref>\n\n==Architecture==\nTeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.<ref>Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).</ref> The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.\n\n==References==\n<references/>\n\n[[Category:Computational linguistics]]\n[[Category:Information retrieval]]\n[[Category:Natural language processing software]]"
p178
sg6
S'TeLQAS'
p179
ssI194
(dp180
g2
S'http://en.wikipedia.org/wiki/Locate (Unix)'
p181
sg4
S"{{lowercase}}\n'''<code>locate</code>''', a [[Unix]] utility first created in 1983,<ref>\nRef: [[Usenix]] ''';login:''', Vol 8, No 1, February/March, 1983, p. 8.\n</ref>\nserves to find [[computer file|file]]s on [[filesystem]]s. It searches through a prebuilt [[database]] of files generated by '''<code>updatedb</code>''' or by a [[Daemon (computing)|daemon]] and compressed using [[incremental encoding]]. It operates significantly faster than <code>[[find]]</code>, but requires regular updating of the database. This sacrifices overall efficiency (because of the regular interrogation of filesystems even when no user needs information) and absolute accuracy (since the database does not update in [[Real-time computing|real time]]) for significant speed improvements (particularly on very large filesystems).\n\nThe GNU version forms a part of [[GNU Findutils]].\n\nSome versions can also index network filesystems.\n\n==mlocate==\nmlocate is a locate/updatedb implementation.\n\n[https://fedorahosted.org/mlocate/ mlocate site]\n\n==References==\n<references/>\n\n==External links==\n* {{man|1|locate|FreeBSD}}\n* [https://www.gnu.org/software/findutils/findutils.html GNU Findutils]\n\nVariants:\n* {{wayback|url=http://slocate.trakker.ca/|title=slocate (Secure Locate)|date=20090204031919}}\n** {{man|1|slocate|die.net}}\n* [http://carolina.mff.cuni.cz/~trmac/blog/mlocate/ <code>mlocate</code>] - faster updates\n** {{man|1|locate|die.net|mlocate}}\n* [http://rlocate.sourceforge.net/ rlocate] - always up-to-date\n* [http://www.kde-apps.org/content/show.php/KwickFind+(Locate+GUI+Frontend)?content=54817 KwickFind] - KDE GUI frontend for locate\n* [http://www.locate32.net/ Locate32 for Windows] Windows analog of GNU locate with GUI, released under GNU license\n\n{{unix commands}}\n\n[[Category:GNU Project software]]\n[[Category:Unix file system-related software]]\n[[Category:Searching]]\n\n{{Unix-stub}}"
p182
sg6
S'Locate (Unix)'
p183
ssI68
(dp184
g2
S'http://en.wikipedia.org/wiki/Faceted search'
p185
sg4
V{{mergeto|Faceted classification|date=January 2015}}\u000a'''Faceted search''', also called '''faceted navigation''' or '''faceted browsing''', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.<ref name="Faceted Search">[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan & Claypool, 2009</ref>\u000a\u000aFacets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.\u000a\u000aWithin the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].{{fact|date=May 2014}}\u000a\u000a==Development==\u000a\u000aThe [[Association for Computing Machinery]]'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:\u000a<blockquote>\u000aThe web search world, since its very beginning, has offered two paradigms:\u000a*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.\u000a*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. \u000aOver the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].<ref name="sigir06">[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]</ref>\u000a</blockquote>\u000a\u000a==Technology==\u000a\u000aVarious search engine software supports faceted classification.\u000a\u000a* [[Apache Lucene]] and derived software:\u000a**  [[Apache Solr]]\u000a** [[Swiftype]]\u000a** [[Elasticsearch]]\u000a* A number of major vendors listed at [[Comparison of enterprise search software#Faceted_Navigation]]\u000a* [[Dieselpoint]]\u000a* [[Endeca]]\u000a* iSeek, search engine for general web and education<ref>[http://www.iseek.com iSeek]</ref>\u000a* [[SpeedTrack]]<ref>http://www.speedtrack.com/technology</ref>\u000a* XSEARCH<ref>[http://www.weitkamper.com]</ref>\u000a\u000a==Mass market use==\u000a\u000aFaceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] such as [[Swiftype]] provide software for implementing faceted search applications.\u000a\u000aOnline retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. <ref name="Smashing Magazine: The Current State of E-Commerce Search (2014)">[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.</ref> Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.\u000a\u000a==Libraries and information science==\u000a\u000a\u000a\u000a\u000aIn 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.<ref name="Major classification systems : the Dewey Centennial">[http://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]</ref>\u000a\u000aModern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system.\u000a\u000aInfoHarness<ref>{{cite journal|last1=Shklar|first1=Leon|last2=Thatte|first2=Satish|last3=Marcus|first3=Howard|last4=Sheth|first4=Amit|title=The "InfoHarness" Information Integration Platform|journal=Proceedings of the Second International Conference on the World Wide Web|date=1994|url=http://citeseer.uark.edu:8080/citeseerx/viewdoc/summary?doi=10.1.1.43.4042|accessdate=5 January 2015}}</ref> <ref>{{cite journal|last1=Shklar|first1=Leon|last2=Sheth|first2=Amit|last3=Kashyap|first3=Vipul|last4=Shah|first4=Kshitij|title=InfoHarness: Use of automatically generated metadata for search and retrieval of heterogeneous information|journal=Advanced Information Systems Engineering|date=20 July 2005|doi=10.1007/3-540-59498-1_248|url=http://link.springer.com/chapter/10.1007/3-540-59498-1_248|accessdate=5 January 2015|ref=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.7442}}</ref> is one of the first Web System (developed in 1994) that provided faceted search over heterogeneous information artifacts such as Web pages, images, videos and documents. The [[CiteSeerX]] project<ref>[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.</ref> at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.\u000a\u000a==See also==\u000a* [[Enterprise Search]]\u000a* [[Exploratory search]]\u000a* [[Faceted classification]]\u000a* [[Human\u2013computer information retrieval]]\u000a* [[Information Extraction]]\u000a* [[NoSQL]]\u000a* [[Trove (website)]]\u000a\u000a==References==\u000a<References/>\u000a\u000a{{DEFAULTSORT:Faceted Search}}\u000a[[Category:Information retrieval]]
p186
sg6
S'Faceted search'
p187
ssI197
(dp188
g2
S'http://en.wikipedia.org/wiki/Daffodil (software)'
p189
sg4
V{{Orphan|date=February 2009}}\u000aThe '''Daffodil''' system is a virtual [[digital library]] system for strategic support of users during the information search process. It implements mainly high-level search functions, so-called stratagems, which provide functionality beyond today's digital libraries.  The Daffodil system was developed as a research project starting as a collaboration between the University of Dortmund (Germany) and the IZ Bonn (Germany), funded by the [[Deutsche Forschungsgemeinschaft]] (DFG) (2000\u20132004). \u000a\u000aCurrently the Daffodil framework is extended to become an experimental evaluation platform for digital library evaluation at the [[University of Duisburg-Essen]].\u000a\u000a== External links ==\u000a* [http://www.dlib.org/dlib/june04/kriewel/06kriewel.html A description of functions and services]\u000a* [http://www.is.informatik.uni-duisburg.de/projects/daffodil/index.html Project description]\u000a\u000a[[Category:Library science]]\u000a[[Category:Searching]]\u000a\u000a\u000a{{Compu-library-stub}}
p190
sg6
S'Daffodil (software)'
p191
ssI71
(dp192
g2
S'http://en.wikipedia.org/wiki/30 Digits'
p193
sg4
V{{Use dmy dates|date=July 2013}}\u000a{{multiple issues|\u000a{{notability|Companies|date=August 2012}}\u000a{{refimprove|date=August 2012}}\u000a{{peacock|date=August 2012}}\u000a{{advert|date=August 2012}}\u000a}}\u000a{{Infobox company\u000a| logo = [[Image:30 Digits Logo.jpg|center]]\u000a| name = 30 Digits GmbH\u000a| type = [[Private company|Private]]\u000a| foundation = 2008\u000a| location = [[Munich]], Germany\u000a| area_served = [[Europe]] <br/> [[North America]] <br/> [[South America]] <br/> [[Asia]]\u000a| key_people = Justin Gilbreath (Managing Director) <br/> Mathis Koblin (Director of R&D)\u000a| industry = [[Information access]] <br/> [[Information retrieval]] <br/> [[Web mining]] <br/> [[Open Source software]]\u000a| products = [[Search Engines]] <br/> Information Discovery Suite <br/> [[Apache Lucene]] <br/> [[Apache Solr]] <br/> Web Extractor\u000a| company_slogan = Linking People to Content\u000a| homepage = {{url|http://www.30digits.com}}\u000a}}\u000a\u000a'''30 Digits''' is a privately held information access and retrieval company with headquarters in [[Munich, Germany]]<ref>{{cite web |url=http://www.digitalpublic.de/web-20-suchmaschinen-holen-auf |title=Web 2.0 \u2013 Suchmaschinen holen auf}}</ref> located in the "Münchner Technologie Zentrum".<ref>{{cite web |url=http://www.mtz.de/index.php?id=12 |title=List of companies located in the MTZ (Münchner Technologie Zentrum)}}</ref> The company was founded in 2008 and offers software that is a mix of privately developed code and leading [[Open Source]] technology primarily from the [[Apache Software Foundation]].\u000a\u000aThe company focuses on [[enterprise information access]] solutions from areas ranging from call-center applications to [[enterprise search]] to database offloading. The company also focuses on solutions created out of unstructured content on the web being structured for analysis,<ref>{{cite web |url=http://www.crmmanager.de/magazin/artikel_2165_enterprise_20_wahlkampf.html |title=Enterprise 2.0: Was ein Unternehmen von Obamas Wahlkampf lernen kann}}</ref> often referred to as [[web harvesting]].  This can be for monitoring security threats or observing customer reactions to products.  It can even be used to gather complex address and other details about entities like properties.<ref>{{cite web |url=http://www.prweb.com/releases/2011/03/prweb5186764.htm |title=viewr Selects 30 Digits as Primary Property Data Provider }}</ref>  Sometimes the focuses blend together in areas like Market or Business Intelligence where both internal and external information needs extraction, analysis, and retrieval capabilities.\u000a\u000aIn addition to the software solutions, 30 Digits Professional Services offers services to assist customer in designing and deploying the correct solutions for the challenge at hand. {{citation needed|date=August 2012}} Trainings, support, and consulting are available both on 30 Digits software and the Open Source software they work with like [[Lucene]] and [[Solr]].<ref>{{cite web |url=http://www.aktiv-verzeichnis.de/details/30-digits-gmbh.html |title=Company description from the Aktiv Verzeichnis}}</ref>\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a==External links==\u000a* [http://www.30digits.com  Company website]\u000a* [http://www.imittelstand.de/mittelstandsliste/webcode/ww1213 Article (in German) placing 30 Digits Web Extractor in Top20 Business Intelligence tools for the "Initiative Mittelstand" 2009]\u000a* [http://lucene.apache.org/  Lucene website]\u000a* [http://lucene.apache.org/solr/ Solr website]\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Search engine software]]\u000a[[Category:Software industry]]\u000a[[Category:Software companies of Germany]]\u000a[[Category:Business software]]\u000a[[Category:Companies based in Munich]]\u000a[[Category:Companies of Germany]]\u000a[[Category:Companies of Europe]]
p194
sg6
S'30 Digits'
p195
ssI200
(dp196
g2
S'http://en.wikipedia.org/wiki/Inversion (discrete mathematics)'
p197
sg4
V{{contradict-other-multiple|Permutation|Lehmer code|Factorial number system|date=March 2013}}\u000a[[File:Inversion set and vector of a permutation.svg|thumb|right|380px|The permutation (4,1,5,2,6,3) has the inversion vector (0,1,0,2,0,3) and the inversion set {(1,2),(1,4),(3,4),(1,6),(3,6),(5,6)}. The inversion vector [[w:Factorial number system|converted]] to decimal is 373.]]\u000a[[File:Inversion set 16; wp(13,11, 7,15).svg|thumb|250px|Inversion set of the permutation<br>(0,15, 14,1, 13,2, 3,12,<br>11,4, 5,10, 6,9, 8,7)<br>showing the pattern of the<br>[[Thue\u2013Morse sequence]]]]\u000aIn [[computer science]] and [[discrete mathematics]], an '''inversion''' is a pair of places of a sequence where the elements on these places are out of their natural [[total order|order]].\u000a\u000a== Definitions ==\u000a\u000aFormally, let <math>(A(1), \u005cldots, A(n))</math> be a sequence of ''n'' distinct numbers.  If <math>i < j</math> and <math>A(i) > A(j)</math>, then the pair <math>(i, j)</math> is called an inversion of <math>A</math>.{{sfn|Cormen|Leiserson|Rivest|Stein|2001|pp=39}}{{sfn|Vitter|Flajolet|1990|pp=459}}\u000a\u000aThe '''inversion number''' of a sequence is one common measure of its sortedness.{{sfn|Barth|Mutzel|2004|pp=183}}{{sfn|Vitter|Flajolet|1990|pp=459}}  Formally, the inversion number is defined to be the number of inversions, that is, \u000a:<math>\u005ctext{inv}(A) = \u005c# \u005c{(A(i),A(j)) \u005cmid i < j \u005ctext{ and } A(i) > A(j)\u005c}</math>.{{sfn|Barth|Mutzel|2004|pp=183}}  \u000aOther measures of (pre-)sortedness include the minimum number of elements that can be deleted from the sequence to yield a fully sorted sequence, the number and lengths of sorted "runs" within the sequence, and the smallest number of exchanges needed to sort the sequence.{{sfn|Mahmoud|2000|pp=284}} Standard [[comparison sort]]ing algorithms can be adapted to compute the inversion number in time {{math|O(''n'' log ''n'')}}.\u000a\u000aThe '''inversion vector''' ''V(i)'' of the sequence is defined for ''i'' = 2, ..., ''n'' as <math>V[i] = \u005cleft\u005cvert\u005c{k \u005cmid k < i \u005ctext{ and } A(k) > A(i)\u005c}\u005cright\u005cvert</math>.  In other words each element is the number of elements preceding the element in the original sequence that are greater than it.  Note that the inversion vector of a sequence has one less element than the sequence, because of course the number of preceding elements that are greater than the first is always zero.  Each permutation of a sequence has a unique inversion vector and it is possible to construct any given permutation of a (fully sorted) sequence from that sequence and the permutation's inversion vector.{{sfn|Pemmaraju|Skiena|2003|pp=69}}\u000a\u000a==Weak order of permutations==\u000aThe set of permutations on ''n'' items can be given the structure of a [[partial order]], called the '''weak order of permutations''', which forms a [[lattice (order)|lattice]].\u000a\u000aTo define this order, consider the items being permuted to be the integers from 1 to ''n'', and let Inv(''u'') denote the set of inversions of a permutation ''u'' for the natural ordering on these items. That is, Inv(''u'') is the set of ordered pairs (''i'', ''j'') such that 1 \u2264 ''i'' < ''j'' \u2264 ''n'' and ''u''(''i'') > ''u''(''j''). Then, in the weak order, we define ''u'' \u2264 ''v'' whenever Inv(''u'') \u2286 Inv(''v'').\u000a\u000aThe edges of the [[Hasse diagram]] of the weak order are given by permutations ''u'' and ''v'' such that ''u < v'' and such that ''v'' is obtained from ''u'' by interchanging two consecutive values of ''u''. These edges form a [[Cayley graph]] for the [[symmetric group|group of permutations]] that is isomorphic to the [[skeleton (topology)|skeleton]] of a [[permutohedron]].\u000a\u000aThe identity permutation is the minimum element of the weak order, and the permutation formed by reversing the identity is the maximum element.\u000a\u000a== See also ==\u000a{{wikiversity|Inversion (discrete mathematics)}}\u000a{{commons|Category:Inversion (discrete mathematics)|Inversion (discrete mathematics)}}\u000a* [[Factorial number system]] (a factorial number is a reflected inversion vector)\u000a* [[Permutation group#Transpositions, simple transpositions, inversions and sorting|Transpositions, simple transpositions, inversions and sorting]]\u000a* [[Damerau\u2013Levenshtein distance]]\u000a* [[Parity of a permutation]]\u000a\u000a'''Sequences in the [[On-Line Encyclopedia of Integer Sequences|OEIS]]:'''\u000a* [https://oeis.org/wiki/Index_to_OEIS:_Section_Fa#factorial Index entries for sequences related to factorial numbers]\u000a* Reflected inversion vectors: {{OEIS link|A007623}} and {{OEIS link|A108731}}\u000a* Sum of inversion vectors, cardinality of inversion sets: {{OEIS link|A034968}}\u000a* Inversion sets of finite permutations interpreted as binary numbers: {{OEIS link|A211362}} &nbsp; (related permutation: {{OEIS link|A211363}})\u000a* Finite permutations that have only 0s and 1s in their inversion vectors: {{OEIS link|A059590}} &nbsp; (their inversion sets: {{OEIS link|A211364}})\u000a* Numbers of permutations of n elements with k inversions; Mahonian numbers: {{OEIS link|A008302}} &nbsp; (their row maxima; Kendall-Mann numbers: {{OEIS link|A000140}})\u000a* Number of connected labeled graphs with n edges and n nodes: {{OEIS link|A057500}}\u000a* Arrays of permutations with similar inversion sets and inversion vectors: {{OEIS link|A211365}}, {{OEIS link|A211366}}, {{OEIS link|A211367}}, {{OEIS link|A211368}}, {{OEIS link|A211369}}, {{OEIS link|A100630}}, {{OEIS link|A211370}}, {{OEIS link|A051683}}\u000a\u000a== References ==\u000a{{reflist|4|refs=}}\u000a\u000a=== Source bibliography ===\u000a{{refbegin|1}}\u000a* {{cite journal|ref=harv|first1=Wilhelm|last1=Barth|first2=Petra|last2=Mutzel|author2-link=Petra Mutzel|title=Simple and Efficient Bilayer Cross Counting|journal=[[Journal of Graph Algorithms and Applications]]|volume=8|issue=2|pages=179&ndash;194|year=2004|doi=10.7155/jgaa.00088}}\u000a* {{cite book|ref=harv\u000a | first1=Thomas H.|last1=Cormen|authorlink1=Thomas H. Cormen\u000a | last2=Leiserson|first2=Charles E.|authorlink2=Charles E. Leiserson\u000a | last3=Rivest|first3=Ronald L.|authorlink3=Ron Rivest\u000a | last4=Stein|first4=Clifford|authorlink4=Clifford Stein\u000a | title = [[Introduction to Algorithms]]\u000a | publisher = MIT Press and McGraw-Hill\u000a | year = 2001\u000a | isbn = 0-262-53196-8\u000a | edition = 2nd\u000a }}\u000a* {{cite book|ref=harv|title=Sorting: a distribution theory|chapter=Sorting Nonrandom Data|volume=54|series=Wiley-Interscience series in discrete mathematics and optimization|first=Hosam Mahmoud|last=Mahmoud|publisher=Wiley-IEEE|year=2000|isbn=978-0-471-32710-3}}\u000a* {{cite book|ref=harv|title=Computational discrete mathematics: combinatorics and graph theory with Mathematica|chapter=Permutations and combinations|first1=Sriram V.|last1=Pemmaraju|first2=Steven S.|last2=Skiena|publisher=Cambridge University Press|year=2003|isbn=978-0-521-80686-2}}\u000a* {{cite book|ref=harv|title=Algorithms and Complexity|volume=1|editor1-first=Jan|editor1-last=van Leeuwen|editor1-link=Jan van Leeuwen|edition=2nd|publisher=Elsevier|year=1990|isbn=978-0-444-88071-0|chapter=Average-Case Analysis of Algorithms and Data Structures|first1=J.S.|last1=Vitter|first2=Ph.|last2=Flajolet}}\u000a{{refend}}\u000a\u000a=== Further reading ===\u000a* {{cite journal|ref=harv|journal=Journal of Integer Sequences|volume=4|year=2001|title=Permutations with Inversions|first=Barbara H.|last=Margolius}}\u000a\u000a=== Presortedness measures ===\u000a* {{cite journal|ref=harv|journal=Lecture Notes in Computer Science|year=1984|volume=172|pages=324&ndash;336|doi=10.1007/3-540-13345-3_29|title=Measures of presortedness and optimal sorting algorithms|first=Heikki|last=Mannila|authorlink=Heikki Mannila}}\u000a* {{cite journal|ref=harv|first1=Vladimir|last1=Estivill-Castro|first2=Derick|last2=Wood|title=A new measure of presortedness|journal=Information and Computation|volume=83|issue=1|pages=111&ndash;119|year=1989|doi=10.1016/0890-5401(89)90050-3}}\u000a* {{cite journal|ref=harv|first=Steven S.|last=Skiena|year=1988|title=Encroaching lists as a measure of presortedness|journal=BIT|volume=28|issue=4|pages=755&ndash;784|doi=10.1007/bf01954897}}\u000a\u000a[[Category:Permutations]]\u000a[[Category:Order theory]]\u000a[[Category:String similarity measures]]\u000a[[Category:Sorting algorithms]]\u000a[[Category:Combinatorics]]\u000a[[Category:Discrete mathematics]]
p198
sg6
S'Inversion (discrete mathematics)'
p199
ssI74
(dp200
g2
S'http://en.wikipedia.org/wiki/MAREC'
p201
sg4
V{{other uses}}\u000aThe '''MA'''trixware '''RE'''search '''C'''ollection ('''MAREC''') is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions.<ref>Merz C., (2003) A Corpus Query Tool For Syntactically Annotated Corpora Licentiate Thesis, The University of Zurich, Department of Computation linguistic, Switzerland</ref><ref>Biber D., Conrad S., and Reppen R. (2000) Corpus Linguistics: Investigating Language Structure and Use. Cambridge University Press, 2nd edition</ref> It consists of 19 million patent documents in different languages, normalised to a highly specific [[XML]] schema.\u000a\u000aMAREC is intended as raw material for research in areas such as [[information retrieval]], [[natural language processing]] or [[machine translation]], which require large amounts of complex documents.<ref>Manning, C. D. and Schütze, H. (2002) Foundations of statistical natural language processing Cambridge, MA, Massachusetts Institute of Technology (MIT)  ISBN 0-262-13360-1.</ref> The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.\u000a\u000aIn MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as [[International Patent Classification|IPC]] codes.<ref>European Patent Office (2009) [http://documents.epo.org/projects/babylon/eponet.nsf/0/1AFC30805E91D074C125758A0051718A/$File/guidelines_2009_complete_en.pdf Guidelines for examination in the European Patent Office], Published by European Patent Office, Germany (April 2009)</ref>\u000a\u000aMAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics \u2013 news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language.<ref>Järvelin A. , Talvensaari T. , Järvelin Anni, (2008) Data driven methods for improving mono- and cross-lingual IR performance in noisy environments, Proceedings of the second workshop on Analytics for noisy unstructured text data, (Singapore)</ref> Since the patent document refers to the same \u201cinvention\u201d or \u201cconcept of idea\u201d the text is a translation of the invention, but it does not have to be a direct translation of the text itself \u2013 text parts could have been removed or added for clarification reasons.\u000a\u000aThe 19,386,697 XML files measure a total of 621 GB and are hosted by the [[Information Retrieval Facility]]. Access and support are free of charge for research purposes.\u000a\u000a== Use Cases ==\u000a* MAREC is used in the [[Patent Language Translations Online (PLuTO)]] project.\u000a\u000a== References ==\u000a{{Reflist}}\u000a\u000a== External links ==\u000a* [http://www.ir-facility.org/prototypes/marec User guide and statistics]\u000a* [http://ir-facility.org Information Retrieval Facility]\u000a\u000a[[Category:Corpora]]\u000a[[Category:Information retrieval]]\u000a[[Category:Machine translation]]\u000a[[Category:Natural language processing]]\u000a[[Category:XML]]
p202
sg6
S'MAREC'
p203
ssI203
(dp204
g2
S'http://en.wikipedia.org/wiki/String metric'
p205
sg4
V{{redirect|String distance|the distance between strings and the fingerboard in musical instruments|Action (music)}}\u000a\u000aIn [[mathematics]] and [[computer science]], a '''string metric''' (also known as a '''string similarity metric''' or '''string distance function''') is a [[metric (mathematics)|metric]] that measures [[distance]] ("inverse similarity") between two [[string (computer science)|text strings]] for [[approximate string  matching]] or comparison and in [[approximate string  matching|fuzzy string searching]]. Necessary requirement for a string ''metric'' (e.g. in contrast to [[string matching]]) is fulfillment of the [[triangle inequality]]. For example the strings "Sam" and "Samuel" can be considered to be close. A string metric provides a number indicating an algorithm-specific indication of distance.\u000a\u000aThe most widely known string metric is a rudimentary one called the [[Levenshtein distance|Levenshtein Distance]] (also known as Edit Distance).  It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. Simplistic string metrics such as [[Levenshtein distance]] have expanded to include phonetic, [[token (parser)|token]], grammatical and character-based methods of statistical comparisons.\u000a\u000aA widespread example of a string metric is [[DNA]] [[sequence analysis]] and RNA analysis, which are performed by optimized string metrics to identify matching sequences.\u000a\u000aString metrics are used heavily in [[information integration]] and are currently used in areas including [[Data analysis techniques for fraud detection|fraud detection]], [[fingerprint analysis]], [[plagiarism detection]], [[ontology merging]], [[DNA analysis]], RNA analysis, [[image analysis]], evidence-based machine learning, [[database]] [[data deduplication]], [[data mining]], Web interfaces, e.g. [[Ajax (programming)|Ajax]]-style suggestions as you type, [[data integration]], and semantic [[knowledge integration]].\u000a\u000a==List of string metrics==\u000a\u000a<!-- This can be a separate article, someday. -->\u000a* [[Sørensen\u2013Dice coefficient]]\u000a* [[Hamming distance]]\u000a* [[Levenshtein distance]] and [[Damerau\u2013Levenshtein distance]]\u000a* [[Block distance]] or [[L1 distance]] or [[City block distance]]\u000a* [[Simple matching coefficient]] (SMC)\u000a* [[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]\u000a* [[Most frequent k characters]]\u000a* [[Tversky index]]\u000a* [[Overlap coefficient]]\u000a* [[Variational distance]]\u000a* [[Hellinger distance]] or [[Bhattacharyya distance]]\u000a* [[Information radius]] ([[Jensen\u2013Shannon divergence]])\u000a* [[Skew divergence]]\u000a* [[Confusion probability]]\u000a* [[Kendall_tau_distance|Tau metric]], an approximation of the [[Kullback\u2013Leibler divergence]]\u000a* [[Fellegi and Sunters metric]] (SFS)\u000a* [[Maximal matches]]\u000a* [[Lee distance]]\u000a\u000a==Selected string measures examples==\u000a\u000a{| class="wikitable"\u000a|-\u000a! Name\u000a! Example\u000a|-\u000a|[[Hamming distance]]\u000a| "'''</span>ka<span style="color:#0082ff">rol</span>in</span>'''" and "'''</span>ka<span style="color:red;">thr</span>in</span>'''" is 3.\u000a|-\u000a|[[Levenshtein distance]] and [[Damerau\u2013Levenshtein distance]]\u000a| \u000a# '''k'''itten \u2192 '''s'''itten (substitution of "s" for "k")\u000a# sitt'''e'''n \u2192 sitt'''i'''n (substitution of "i" for "e")\u000a# sittin \u2192 sittin'''g''' (insertion of "g" at the end).\u000a<!--|-\u000a|[[Simple matching coefficient]] (SMC)\u000a|-->\u000a<!--|-\u000a|-\u000a|[[Jaccard similarity]] or [[Jaccard coefficient]] or [[Tanimoto coefficient]]\u000a|-->\u000a|-\u000a|[[Most frequent k characters]]\u000a|MostFreqKeySimilarity('<span style="color:red;">r</span><span style="color:#0082ff">e</span>s<span style="color:#0082ff">e</span>a<span style="color:red;">r</span>ch', 's<span style="color:#0082ff">ee</span>king', 2) = 2\u000a<!--|-\u000a|[[Tversky index]]\u000a|-->\u000a<!--|-\u000a|[[Overlap coefficient]]\u000a|-->\u000a<!--|-\u000a|[[Variational distance]]\u000a|-->\u000a<!--|-\u000a|[[Hellinger distance]] or [[Bhattacharyya distance]]\u000a|-->\u000a<!--|-\u000a|[[Information radius]] ([[Jensen\u2013Shannon divergence]])\u000a|-->\u000a<!--|-\u000a|[[Skew divergence]]\u000a|-->\u000a<!--|-\u000a|[[Confusion probability]]\u000a|-->\u000a<!--|-\u000a|[[Tau metric]], an approximation of the [[Kullback\u2013Leibler divergence]]\u000a|-->\u000a<!--|-\u000a|[[Fellegi and Sunters metric]] (SFS)\u000a|-->\u000a<!--|-\u000a|[[Maximal matches]]\u000a|-->\u000a|}\u000a\u000a==See also==\u000a* [[approximate string  matching]]\u000a* [[String matching]]\u000a* [http://www.speech.cs.cmu.edu/ Carnegie Mellon University open source library]\u000a* [http://rockymadden.com/stringmetric/ StringMetric project] a [[Scala programming language|Scala]] library of string metrics and phonetic algorithms\u000a* [https://github.com/NaturalNode/natural Natural project] a [[JavaScript]] natural language processing library which includes implementations of popular string metrics\u000a\u000a==External links==\u000a*http://www.dcs.shef.ac.uk/~sam/stringmetrics.html {{Dead link|date=July 2011}} A fairly complete overview {{wayback|url=http://www.dcs.shef.ac.uk/~sam/stringmetrics.html#ukkonen}}\u000a\u000a{{DEFAULTSORT:String Metric}}\u000a[[Category:String similarity measures| ]]\u000a[[Category:Metrics]]\u000a\u000a[[de:Ähnlichkeitsanalyse]]
p206
sg6
S'String metric'
p207
ssI77
(dp208
g2
S'http://en.wikipedia.org/wiki/Taganode Local Search Engine'
p209
sg4
V'''The Open Local Search Engine from Taganode''' is a [[search engine]] specifically targeting [[mobile phone]]s. It is based on local [[search algorithm]]s to find new places of interest within a specified distance.\u000a\u000aThe Taganode search engine offers an Open Developer [[Application programming interface|API]] that any one can use freely when writing new applications for [[iPhone]]s, [[Android (operating system)|Android phones]] and other platforms.\u000a\u000aThe search engine is optimized for mobile phones by low [[Bandwidth (computing)|bandwidth]] usage and only makes the simplest service calls to try to be compatible with as many mobile phones as possible. The Taganode search service is at this moment present in [[London]], [[Rome]], [[Venice]], [[Amsterdam]], [[Berlin]], [[Sweden]] and in [[Denmark]].\u000a\u000a== References ==\u000a<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->\u000a*{{cite news|url=http://www.webfinanser.com/nyheter/164362/taganode-gratis-reseguide-finns-nu-aven-i-venedig/|title=Taganode \u2013 Gratis reseguide finns nu även i Venedig |date=2009-10-12|work=Webfinanser|language=Swedish|accessdate=18 December 2009}}\u000a*{{cite news|url=http://www.webfinanser.com/nyheter/159761/en-ny-och-innovativ-soktjanst-for-resenarer-i-europa/|title=En ny och innovativ söktjänst för resenärer i Europa|date=September 19, 2009|work=Webfinanser |language=Swedish|accessdate=18 December 2009}}\u000a\u000a== External links ==\u000a* [http://www.taganode.com Official site]\u000a* [http://www.mynewsdesk.com/se/view/pressrelease/taganode-free-guide-now-available-in-rome-325701/  Taganode Service in Venice] (press release)\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]\u000a[[Category:Mobile phones]]
p210
sg6
S'Taganode Local Search Engine'
p211
ssI206
(dp212
g2
S'http://en.wikipedia.org/wiki/Edit distance'
p213
sg4
VIn [[computer science]], '''edit distance''' is a way of quantifying how dissimilar two [[String (computing)|strings]] (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in [[natural language processing]], where automatic [[Spell checker|spelling correction]] can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In [[bioinformatics]], it can be used to quantify the similarity of [[macromolecule]]s such as [[DNA]], which can be viewed as strings of the letters A, C, G and T.\u000a\u000aSeveral definitions of edit distance exist, using different sets of string operations. One of the most common variants is called [[Levenshtein distance]], named after the Soviet Russian computer scientist [[Vladimir Levenshtein]]. In this version, the allowed operations are the removal or insertion of a single character, or the substitution of one character for another. Levenshtein distance may also simply be called "edit distance", although several variants exist.<ref name="navarro">{{Cite doi/10.1145.2F375360.375365}}</ref>{{rp|32}}\u000a\u000a==Formal definition and properties==\u000aGiven two strings {{mvar|a}} and {{mvar|b}} on an alphabet {{mvar|\u03a3}} (e.g. the set of [[ASCII]] characters, the set of [[byte]]s [0..255], etc.), the edit distance d({{mvar|a}}, {{mvar|b}}) is the minimum-weight series of edit operations that transforms {{mvar|a}} into {{mvar|b}}. One of the simplest sets of edit operations is that defined by Levenshtein in 1966:<ref name="slp"/>\u000a\u000a:'''Insertion''' of a single symbol. If {{mvar|a}} = {{mvar|u}}{{mvar|v}}, then inserting the symbol {{mvar|x}} produces {{mvar|u}}{{mvar|x}}{{mvar|v}}. This can also be denoted \u03b5\u2192{{mvar|x}}, using \u03b5 to denote the empty string.\u000a:'''Deletion''' of a single symbol changes {{mvar|u}}{{mvar|x}}{{mvar|v}} to {{mvar|u}}{{mvar|v}} ({{mvar|x}}\u2192\u03b5).\u000a:'''Substitution''' of a single symbol {{mvar|x}} for a symbol {{mvar|y}} \u2260 {{mvar|x}} changes {{mvar|u}}{{mvar|x}}{{mvar|v}} to {{mvar|u}}{{mvar|y}}{{mvar|v}} ({{mvar|x}}\u2192{{mvar|y}}).\u000a\u000aIn Levenshtein's original definition, each of these operations has unit cost (except that substitution of a character by itself has zero cost), so the Levenshtein distance is equal to the minimum ''number'' of operations required to transform {{mvar|a}} to {{mvar|b}}. A more general definition associates non-negative weight functions {{mvar|w}}<sub>ins</sub>({{mvar|x}}), {{mvar|w}}<sub>del</sub>({{mvar|x}}) and {{mvar|w}}<sub>sub</sub>({{mvar|x}}&nbsp;{{mvar|y}}) with the operations.<ref name="slp">{{cite book |author1=Daniel Jurafsky |author2=James H. Martin |title=Speech and Language Processing |publisher=Pearson Education International |pages=107\u2013111}}</ref>\u000a\u000aAdditional primitive operations have been suggested. A common mistake when typing text is '''transposition''' of two adjacent characters commonly occur, formally characterized by an operation that changes {{mvar|u}}{{mvar|x}}{{mvar|y}}{{mvar|v}} into {{mvar|u}}{{mvar|y}}{{mvar|x}}{{mvar|v}} where {{mvar|x}}, {{mvar|y}} \u2208 {{mvar|\u03a3}}.<ref name="ukkonen83">{{cite conference |author=Esko Ukkonen |title=On approximate string matching |conference=Foundations of Computation Theory |year=1983 |pages=487\u2013495 |publisher=Springer}}</ref><ref name="ssm"/>\u000aFor the task of correcting [[Optical character recognition|OCR]] output, '''merge''' and '''split''' operations have been used which replace a single character into a pair of them or vice-versa.<ref name="ssm">{{cite journal |first1=Klaus U. |last1=Schulz |first2=Stoyan |last2=Mihov |year=2002 |id={{citeseerx|10.1.1.16.652}} |title=Fast string correction with Levenshtein automata |journal=International Journal of Document Analysis and Recognition |volume=5 |issue=1 |pages=67\u201385 |doi=10.1007/s10032-002-0082-8}}</ref>\u000a\u000aOther variants of edit distance are obtained by restricting the set of operations. [[Longest common subsequence]] (LCS) distance is edit distance with insertion and deletion as the only two edit operations, both at unit cost.<ref name="navarro"/>{{rp|37}} Similarly, by only allowing substitutions (again at unit cost), [[Hamming distance]] is obtained; this must be restricted to equal-length strings.<ref name="navarro"/>\u000a[[Jaro\u2013Winkler distance]] can be obtained from an edit distance where only transpositions are allowed.\u000a\u000a===Example===\u000aThe [[Levenshtein distance]] between "kitten" and "sitting" is 3. The minimal edit script that transforms the former into the latter is:\u000a\u000a# '''k'''itten \u2192 '''s'''itten (substitution of "s" for "k")\u000a# sitt'''e'''n \u2192 sitt'''i'''n (substitution of "i" for "e")\u000a# sittin \u2192 sittin'''g''' (insertion of "g" at the end).\u000a\u000aLCS distance (insertions and deletions only) gives a different distance and minimal edit script:\u000a\u000a# delete '''k''' at 0\u000a# insert '''s''' at 0\u000a# delete '''e''' at 4\u000a# insert '''i''' at 4\u000a# insert '''g''' at 6\u000a\u000afor a total cost/distance of 5 operations.\u000a\u000a===Properties===\u000aEdit distance with non-negative cost satisfies the axioms of a [[Metric (mathematics)|metric]], giving rise to a [[metric space]] of strings, when the following conditions are met:<ref name="navarro"/>{{rp|37}}\u000a\u000a* Every edit operation has positive cost;\u000a* for every operation, there is an inverse operation with equal cost.\u000a\u000aWith these properties, the metric axioms are satisfied as follows:\u000a\u000a:{{mvar|d}}({{mvar|a}}, {{mvar|a}}) = 0, since each string can be trivially transformed to itself using exactly zero operations.\u000a:{{mvar|d}}({{mvar|a}}, {{mvar|b}}) > 0 when {{mvar|a}} \u2260 {{mvar|b}}, since this would require at least one operation at non-zero cost.\u000a:{{mvar|d}}({{mvar|a}}, {{mvar|b}}) = {{mvar|d}}({{mvar|b}}, {{mvar|a}}) by equality of the cost of each operation and its inverse.\u000a:Triangle inequality: {{mvar|d}}({{mvar|a}}, {{mvar|c}}) \u2264 {{mvar|d}}({{mvar|a}}, {{mvar|b}}) + {{mvar|d}}({{mvar|b}}, {{mvar|c}}).<ref>{{cite conference |author1=Lei Chen |author2=Raymond Ng |title=On the marriage of L\u209a-norms and edit distance |conference=Proc. 30th Int'l Conf. on Very Large Databases (VLDB) |volume=30 |year=2004}}</ref>\u000a\u000aLevenshtein distance and LCS distance with unit cost satisfy the above conditions, and therefore the metric axioms. Variants of edit distance that are not proper metrics have also been considered in the literature.<ref name="navarro"/>\u000a\u000aOther useful properties of unit-cost edit distances include:\u000a\u000a* LCS distance is bounded above by the sum of lengths of a pair of strings.<ref name="navarro"/>{{rp|37}}\u000a* LCS distance is an upper bound on Levenshtein distance.\u000a* For strings of the same length, Hamming distance is an upper bound on Levenshtein distance.<ref name="navarro"/>\u000a\u000aRegardless of cost/weights, the following property holds of all edit distances:\u000a\u000a* When {{mvar|a}} and {{mvar|b}} share a common prefix, this prefix has no effect on the distance. Formally, when {{mvar|a}} = {{mvar|uv}} and {{mvar|b}} = {{mvar|uw}}, then {{mvar|d}}({{mvar|a}}, {{mvar|b}}) = {{mvar|d}}({{mvar|v}}, {{mvar|w}}).<ref name="ssm"/> This allows speeding up many computations involving edit distance and edit scripts, since common prefixes and suffixes can be skipped in linear time.\u000a\u000a==Computation==\u000a===Basic algorithm===\u000a{{main|Wagner\u2013Fischer algorithm}}\u000aUsing Levenshtein's original operations, the edit distance between <math>a = a_1\u005cldots a_n</math> and <math>b = b_1\u005cldots b_m</math> is given by <math>d_{mn}</math>, defined by the recurrence<ref name="slp"/>\u000a\u000a:<math>d_{i0} = \u005csum_{k=1}^{i} w_\u005cmathrm{del}(b_{k}), \u005cquad  for\u005c; 1 \u005cleq i \u005cleq m</math>\u000a:<math>d_{0j} = \u005csum_{k=1}^{j} w_\u005cmathrm{ins}(a_{k}), \u005cquad  for\u005c; 1 \u005cleq j \u005cleq n</math>\u000a:<math>d_{ij} = \u005cbegin{cases} d_{i-1, j-1} & \u005cquad a_{j} = b_{i}\u005c\u005c \u005cmin \u005cbegin{cases} d_{i-1, j} + w_\u005cmathrm{del}(b_{i})\u005c\u005c d_{i,j-1} + w_\u005cmathrm{ins}(a_{j}) \u005c\u005c d_{i-1,j-1} + w_\u005cmathrm{sub}(a_{j}, b_{i}) \u005cend{cases} & \u005cquad a_{j} \u005cneq b_{i}\u005cend{cases} , \u005cquad  for\u005c; 1 \u005cleq i \u005cleq m, 1 \u005cleq j \u005cleq n.</math>\u000a\u000aThis algorithm can be generalized to handle transpositions by adding another term in the recursive clause's minimization.<ref name="ukkonen83"/>\u000a\u000aThe straightforward, [[Recursion (computer science)|recursive]] way of evaluating this recurrence takes [[exponential time]]. Therefore, it is usually computed using a [[dynamic programming]] algorithm that is commonly credited to [[Wagner\u2013Fischer algorithm|Wagner and Fischer]],<ref>{{cite journal |author1=R. Wagner |author2=M. Fischer |title=The string-to-string correction problem |journal=J. ACM |volume=21 |year=1974 |pages=168\u2013178 |doi=10.1145/321796.321811}}</ref> although it has a history of multiple invention.<ref name="slp"/><ref name="ukkonen83"/>\u000aAfter completion of the Wagner\u2013Fischer algorithm, a minimal sequence of edit operations can be read off as a backtrace of the operations used during the dynamic programming algorithm starting at <math>d_{mn}</math>.\u000a\u000aThis algorithm has a [[time complexity]] of \u0398({{mvar|m}}{{mvar|n}}). When the full dynamic programming table is constructed, its [[space complexity]] is also \u0398({{mvar|m}}{{mvar|n}}); this can be improved to \u0398(min({{mvar|m}},{{mvar|n}})) by observing that at any instant, the algorithm only requires two rows (or two columns) in memory. However, this optimization makes it impossible to read off the minimal series of edit operations.<ref name="ukkonen83"/> A linear-space solution to this problem is offered by [[Hirschberg's algorithm]].<ref>{{cite book |last=Skiena |first=Steven |authorlink=Steven Skiena |title = The Algorithm Design Manual |publisher=[[Springer Science+Business Media]] |edition=2nd |year = 2010 |isbn=1-849-96720-2}}</ref>{{rp|634}}\u000a\u000a===Improved algorithms===\u000aImproving on the Wagner\u2013Fisher algorithm described above, [[Esko Ukkonen|Ukkonen]] describes several variants,<ref>{{cite journal |title=Algorithms for approximate string matching |journal=Information and Control |volume=64 |issue=1\u20133 |year=1985 |url=http://www.cs.helsinki.fi/u/ukkonen/InfCont85.PDF}}</ref> one of which takes two strings and a maximum edit distance {{mvar|s}}, and returns min({{mvar|s}}, {{mvar|d}}). It achieves this by only computing and storing a part of the dynamic programming table around its diagonal. This algorithm takes time O({{mvar|s}}×min({{mvar|m}},{{mvar|n}})), where {{mvar|m}} and {{mvar|n}} are the lengths of the strings. Space complexity is O({{mvar|s}}²) or O({{mvar|s}}), depending on whether the edit sequence needs to be read off.<ref name="ukkonen83"/>\u000a\u000a==Applications==\u000aEdit distance finds applications in [[computational biology]] and natural language processing, e.g. the correction of spelling mistakes or OCR errors, and [[approximate string matching]], where the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected.\u000a\u000aVarious algorithms exist that solve problems beside the computation of distance between a pair of strings, to solve related types of problems.\u000a\u000a* [[Hirschberg's algorithm]] computes the optimal [[Sequence alignment|alignment]] of two strings, where optimality is defined as minimizing edit distance.\u000a* [[Approximate string matching]] can be formulated in terms of edit distance. Ukkonen's 1985 algorithm takes a string {{mvar|p}}, called the pattern, and a constant {{mvar|k}}; it then builds a [[deterministic finite state automaton]] that finds, in an arbitrary string {{mvar|s}}, a substring whose edit distance to {{mvar|p}} is at most {{mvar|k}}<ref>{{cite journal |author=Esko Ukkonen |title=Finding approximate patterns in strings |journal=J. Algorithms |volume=6 |pages=132\u2013137 |year=1985 |doi=10.1016/0196-6774(85)90023-9}}</ref> (cf. the [[Aho\u2013Corasick string matching algorithm|Aho\u2013Corasick algorithm]], which similarly constructs an automaton to search for any of a number of patterns, but without allowing edit operations). A similar algorithm for approximate string matching is the [[bitap algorithm]], also defined in terms of edit distance.\u000a* [[Levenshtein automaton|Levenshtein automata]] are finite-state machines that recognize a set of strings within bounded edit distance of a fixed reference string.<ref name="ssm"/>\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a[[Category:String similarity measures]]
p214
sg6
S'Edit distance'
p215
ssI80
(dp216
g2
S'http://en.wikipedia.org/wiki/Binary Independence Model'
p217
sg4
V{{context|date=June 2012}}\u000aThe '''Binary Independence Model''' (BIM)<ref name="cyu76" /><ref name="jones77"/> is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.\u000a\u000a==Definitions==\u000aThe Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.\u000aThe representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector ''d = (x<sub>1</sub>, ..., x<sub>m</sub>)'' where ''x<sub>t</sub>=1'' if term ''t'' is present in the document ''d'' and ''x<sub>t</sub>=0'' if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.\u000a"Independence" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the "naive" assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.\u000a\u000aThe probability ''P(R|d,q)'' that a document is relevant derives from the probability of relevance of the terms vector of that document ''P(R|x,q)''. By using the [[Bayes rule]] we get:\u000a\u000a<math>P(R|x,q) = \u005cfrac{P(x|R,q)*P(R|q)}{P(x|q)}</math>\u000a\u000awhere ''P(x|R=1,q)'' and ''P(x|R=0,q)'' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is ''x''.\u000aThe exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.\u000a\u000a''P(R=1|q)'' and ''P(R=0|q)'' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query ''q''. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.\u000aSince a document is either relevant or nonrelevant to a query we have that:\u000a\u000a<math>P(R=1|x,q) + P(R=0|x,q) = 1</math>\u000a\u000a=== Query Terms Weighting ===\u000aGiven a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the\u000aterms in the query such that the retrieval effectiveness will be high. Let <math>p_i</math> and <math>q_i</math> be the probability that a relevant document and an irrelevant document has the <math>i^{th}</math> term respectively. Yu and [[Gerard Salton|Salton]],<ref name="cyu76" /> who first introduce BIM, propose that the weight of the <math>i^{th}</math> term is an increasing function of <math>Y_i =  \u005cfrac{p_i *(1-q_i)}{(1-p_i)*q_i}</math>. Thus, if <math>Y_i</math> is higher than <math>Y_j</math>, the weight\u000aof term <math>i</math> will be higher than that of term <math>j</math>. Yu and Salton<ref name="cyu76" /> showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Spärck Jones|Spärck Jones]]<ref name="jones77"/> later showed that if the <math>i^{th}</math> term is assigned the weight of <math>log Y_i</math>, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.\u000a\u000aThe Binary Independence Model was introduced by Yu and Salton.<ref name="cyu76" /> The name Binary Independence Model was coined by Robertson and Spärck Jones.<ref name="jones77"/>\u000a\u000a== See also ==\u000a\u000a* [[Bag of words model]]\u000a\u000a==Further reading==\u000a* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | coauthors=Prabhakar Raghavan & Hinrich Schütze | publisher=Cambridge University Press | year=2008}}\u000a* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&uuml;ttcher | coauthors=Charles L. A. Clarke & Gordon V. Cormack | publisher=MIT Press | year=2010}}\u000a\u000a==References==\u000a{{Reflist|refs=\u000a<ref name="cyu76">{{cite doi | 10.1145/321921.321930 }}</ref>\u000a<ref name="jones77">{{cite doi | 10.1002/asi.4630270302 }}</ref> \u000a}}\u000a\u000a[[Category:Information retrieval]]\u000a[[Category:Probabilistic models]]
p218
sg6
S'Binary Independence Model'
p219
ssI209
(dp220
g2
S'http://en.wikipedia.org/wiki/String kernel'
p221
sg4
VIn [[machine learning]] and [[data mining]], a '''string kernel''' is a [[Positive-definite kernel|kernel function]] that operates on [[String (computer science)|strings]], i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings ''a'' and ''b'' are, the higher the value of a string kernel ''K''(''a'', ''b'') will be.\u000a\u000aUsing string kernels with [[Kernel trick|kernelized]] learning algorithms such as [[support vector machine]]s allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued [[feature vector]]s.<ref name="Lodhi"/> String kernels are used in domains where sequence data are to be [[Cluster analysis|clustered]] or [[statistical classification|classified]], e.g. in [[text mining]] and [[bioinformatics|gene analysis]].<ref>\u000a{{Citation\u000a  | title = The spectrum kernel: A string kernel for SVM protein classification\u000a  | last = Leslie\u000a  | first = C.\u000a  | last2 = Eskin\u000a  | first2 = E.\u000a  | last3 = Noble\u000a  | first3 = W.S.\u000a  | booktitle = Proceedings of the Pacific Symposium on Biocomputing\u000a  | volume = 7\u000a  | pages = 566\u2013575\u000a  | year = 2002\u000a}}</ref>\u000a\u000a==Informal introduction==\u000a\u000aSuppose one wants to compare some text passages automatically and indicate their relative similarity.\u000aFor many applications, it might be sufficient to find some keywords which match exactly.\u000aOne example where exact matching is not always enough is found in [[Spam (electronic)|spam]] detection.<ref>\u000a{{Citation\u000a  | title = Improved Online Support Vector Machines Spam Filtering Using String Kernels\u000a  | last = Amayri\u000a  | first = O.\u000a}}</ref>\u000aAnother would be in computational gene analysis, where [[Homology (biology)|homologous]] [[genes]] have [[mutated]], resulting in common subsequences along with deleted, inserted or replaced symbols.\u000a<!--- TODO insert a picture here --->\u000a\u000a==Motivation==\u000a\u000aSince several well-proven data clustering, classification and information retrieval\u000a<!--- and other ... see manifold learning --->\u000amethods (for example support vector machines) are designed to work on vectors\u000a(i.e. data are elements of a vector space), using a string kernel allows the extension of these methods to handle sequence data.\u000a\u000aThe string kernel method is to be contrasted with earlier approaches for text classification where feature vectors only indicated\u000athe presence or absence of a word.\u000aNot only does it improve on these approaches, but it is an example for a whole class of kernels adapted to data structures, which\u000abegan to appear at the turn of the 21st century. A survey of such methods has been compiled by Gärtner.<ref>\u000a{{Citation\u000a  | last = Gärtner\u000a  | first = T.\u000a  | title = A survey of kernels for structured data\u000a  | journal = CM SIGKDD Explorations Newsletter\u000a  | publisher = [[Association for Computing Machinery|ACM]]\u000a  | year = 2003\u000a  | volume = 5\u000a  | number = 1\u000a  | page = 58}}\u000a</ref>\u000a\u000a==Definition==\u000a\u000aA [[Kernel trick|kernel]] on a domain <math>D</math> is a function <math>K: D \u005ctimes D \u005crightarrow \u005cmathbb{R}</math>\u000asatisfying some conditions (being [[symmetric]] in the arguments, [[continuous function|continuous]] and [[Positive-semidefinite function|positive semidefinite]] in a certain sense).\u000a\u000a[[Mercer's theorem]] asserts that <math>K</math> can then be expressed as <math>K(x,y)=\u005cvarphi(x)\u005ccdot \u005cvarphi(y)</math> with <math>\u005cvarphi</math> mapping the arguments into an [[inner product space]].\u000a\u000aWe can now reproduce the definition of a '''string subsequence kernel'''<ref name="Lodhi">{{Cite journal\u000a  | last = Lodhi\u000a  | first = Huma\u000a  | last2 = Saunders\u000a  | first2 = Craig\u000a  | last3 = Shawe-Taylor\u000a  | first3 = John\u000a  | last4 = Cristianini\u000a  | first4 = Nello\u000a  | last5 = Watkins\u000a  | first5 = Chris\u000a  | title = Text classification using string kernels\u000a  | journal = [[Journal of Machine Learning Research]]\u000a  | year = 2002\u000a  | pages = 419\u2013444}}</ref>\u000aon strings over an [[Alphabet (computer science)|alphabet]] <math>\u005cSigma</math>. Coordinate-wise, the mapping is defined as follows:\u000a\u000a:<math>\u005cvarphi_u :\u000a\u005cleft\u005c{\u000a\u005cbegin{array}{l}\u000a\u005cSigma^n \u005crightarrow \u005cmathbb{R}^{\u005cSigma^n} \u005c\u005c\u000a s \u005cmapsto \u005csum_{\u005cmathbf{i} : u=s_{\u005cmathbf{i}}} \u005clambda^{l(\u005cmathbf{i})}\u000a\u005cend{array}\u000a\u005cright.\u000a</math>\u000a\u000aThe <math>\u005cmathbf{i}</math> are [[multiindices]] and <math>u</math> is a string of length <math>n</math>:\u000asubsequences can occur in a non-contiguous manner, but gaps are penalized.\u000aThe parameter <math>\u005clambda</math> may be set to any value between <math>0</math> (gaps are not allowed) and <math>1</math>\u000a(even widely-spread "occurrences" are weighted the same as appearances as a contiguous substring).\u000a\u000a<!--- TODO put an example here !!! --->\u000a\u000aFor several relevant algorithms, data enters into the algorithm only in expressions involving an inner product of feature vectors,\u000ahence the name [[kernel methods]]. A desirable consequence of this is that one does not need to explicitly calculate the transformation <math>\u005cphi(x)</math>, only the inner product via the kernel, which may be a lot quicker, especially when [[approximation|approximated]].<ref name=Lodhi/>\u000a<!--- ==Efficitent Computation== --->\u000a<!--- == See also == --->\u000a\u000a==References==\u000a<!--- cite "alignment kernels", precursor --->\u000a{{Reflist}}\u000a\u000a[[Category:Algorithms on strings]]\u000a[[Category:Kernel methods for machine learning]]\u000a[[Category:Natural language processing]]\u000a[[Category:String similarity measures]]
p222
sg6
S'String kernel'
p223
ssI83
(dp224
g2
S'http://en.wikipedia.org/wiki/Champion list'
p225
sg4
S"{{orphan|date=January 2011}}\n\nA '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].\n\n[[Category:Information retrieval]]\n\n\n{{computing-stub}}"
p226
sg6
S'Champion list'
p227
ssI212
(dp228
g2
Vhttp://en.wikipedia.org/wiki/Wagner\u2013Fischer algorithm
p229
sg4
VIn [[computer science]], the '''Wagner\u2013Fischer algorithm''' is a [[dynamic programming]] algorithm that computes the [[edit distance]] between two strings of characters.\u000a\u000a==History==\u000aThe Wagner\u2013Fischer algorithm has a history of [[multiple invention]]. Navarro lists the following inventors of it, with date of publication, and acknowledges that the list is incomplete:<ref name="navarro"/>{{rp|43}}\u000a* Vintsyuk, 1968\u000a* [[Needleman\u2013Wunsch algorithm|Needleman and Wunsch]], 1970\u000a* Sankoff, 1972\u000a* Sellers, 1974\u000a* Wagner and Fischer, 1974\u000a* Lowrance and Wagner, 1975\u000a\u000a==Calculating distance==\u000aThe Wagner\u2013Fischer algorithm computes edit distance based on the observation that if we reserve a [[Matrix (mathematics)|matrix]] to hold the edit distances between all [[prefix (computer science)|prefix]]es of the first string and all prefixes of the second, then we can compute the values in the matrix by [[flood fill]]ing the matrix, and thus find the distance between the two full strings as the last value computed.\u000a\u000aA straightforward implementation, as [[pseudocode]] for a function ''EditDistance'' that takes two strings, ''s'' of length ''m'', and ''t'' of length ''n'', and returns the Levenshtein distance between them, looks as follows. Note that the inputs strings are one-indexed, while the matrix ''d'' is zero-indexed, and <code>[i..k]</code> is a closed range.\u000a\u000a  '''int''' EditDistance('''char''' s[1..m], '''char''' t[1..n])\u000a    ''// For all i and j, d[i,j] will hold the Levenshtein distance between''\u000a    ''// the first i characters of s and the first j characters of t.''\u000a    ''// Note that d has (m+1)  x(n+1) values.\u000a    '''let''' d be a 2-d array of '''int''' with dimensions [0..m, 0..n]\u000a   \u000a    '''for''' i '''in''' [0..m]\u000a      d[i, 0] \u2190 i ''// the distance of any first string to an empty second string''\u000a    '''for''' j '''in''' [0..n]\u000a      d[0, j] \u2190 j ''// the distance of any second string to an empty first string''\u000a   \u000a    '''for''' j '''in''' [1..n]\u000a      '''for''' i '''in''' [1..m]\u000a        '''if''' s[i] = t[j] '''then'''  <!-- not: s[i-1] = t[j-1] -->\u000a          d[i, j] \u2190 d[i-1, j-1]       ''// no operation required''\u000a        '''else'''\u000a          d[i, j] \u2190 minimum of\u000a                     (\u000a                       d[i-1, j] + 1,  ''// a deletion''\u000a                       d[i, j-1] + 1,  ''// an insertion''\u000a                       d[i-1, j-1] + 1 ''// a substitution''\u000a                     )\u000a   \u000a    '''return''' d[m,n]\u000a\u000aTwo examples of the resulting matrix (hovering over an underlined number reveals the operation performed to get that number):\u000a<center>\u000a{|\u000a|\u000a{|class="wikitable"\u000a|-\u000a| \u000a| \u000a!k \u000a!i \u000a!t \u000a!t \u000a!e \u000a!n\u000a|-\u000a| ||0 ||1 ||2 ||3 ||4 ||5 ||6\u000a|-\u000a!s\u000a|1 ||{{H:title|substitution of 'k' for 's'|1}} ||2 ||3 ||4 ||5 ||6\u000a|-\u000a!i\u000a|2 ||2 ||{{H:title|'i' equals 'i'|1}} ||2 ||3 ||4 ||5\u000a|-\u000a!t\u000a|3 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 ||4\u000a|-\u000a!t\u000a|4 ||4 ||3 ||2 ||{{H:title|'t' equals 't'|1}} ||2 ||3 \u000a|-\u000a!i\u000a|5 ||5 ||4 ||3 ||2 ||{{H:title|substitution of 'e' for 'i'|2}} ||3\u000a|-\u000a!n\u000a|6 ||6 ||5 ||4 ||3 ||3 ||{{H:title|'n' equals 'n'|2}}\u000a|-\u000a!g\u000a|7 ||7 ||6 ||5 ||4 ||4 ||{{H:title|insert 'g'|3}}\u000a|}\u000a|\u000a{|class="wikitable"\u000a|\u000a|\u000a!S\u000a!a\u000a!t\u000a!u\u000a!r\u000a!d\u000a!a\u000a!y\u000a|-\u000a| \u000a|0 ||1 ||2 ||3 ||4 ||5 ||6 ||7 ||8\u000a|-\u000a!S\u000a|1 ||{{H:title|'S' equals 'S'|0}} ||{{H:title|insert 'a'|1}} ||{{H:title|insert 't'|2}} ||3 ||4 ||5 ||6 ||7\u000a|-\u000a!u\u000a|2 ||1 ||1 ||2 ||{{H:title|'u' equals 'u'|2}} ||3 ||4 ||5 ||6\u000a|-\u000a!n\u000a|3 ||2 ||2 ||2 ||3 ||{{H:title|substitution of 'r' for 'n'|3}} ||4 ||5 ||6\u000a|-\u000a!d\u000a|4 ||3 ||3 ||3 ||3 ||4 ||{{H:title|'d' equals 'd'|3}} ||4 ||5 \u000a|-\u000a!a\u000a|5 ||4 ||3 ||4 ||4 ||4 ||4 ||{{H:title|'a' equals 'a'|3}} ||4\u000a|-\u000a!y\u000a|6 ||5 ||4 ||4 ||5 ||5 ||5 ||4 ||{{H:title|'y' equals 'y'|3}}\u000a|}\u000a|}\u000a</center>\u000a\u000aThe [[invariant (mathematics)|invariant]] maintained throughout the algorithm is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. At the end, the bottom-right element of the array contains the answer.\u000a\u000a===Proof of correctness===\u000aAs mentioned earlier, the [[invariant (mathematics)|invariant]] is that we can transform the initial segment <code>s[1..i]</code> into <code>t[1..j]</code> using a minimum of <code>d[i,j]</code> operations. This invariant holds since:\u000a* It is initially true on row and column 0 because <code>s[1..i]</code> can be transformed into the empty string <code>t[1..0]</code> by simply dropping all <code>i</code> characters. Similarly, we can transform <code>s[1..0]</code> to <code>t[1..j]</code> by simply adding all <code>j</code> characters.\u000a* If <code>s[i] = t[j]</code>, and we can transform <code>s[1..i-1]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can do the same to <code>s[1..i]</code> and just leave the last character alone, giving <code>k</code> operations.\u000a* Otherwise, the distance is the minimum of the three possible ways to do the transformation:\u000a** If we can transform <code>s[1..i]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can simply add <code>t[j]</code> afterwards to get <code>t[1..j]</code> in <code>k+1</code> operations (insertion).\u000a** If we can transform <code>s[1..i-1]</code> to <code>t[1..j]</code> in <code>k</code> operations, then we can remove <code>s[i]</code> and then do the same transformation, for a total of <code>k+1</code> operations (deletion).\u000a** If we can transform <code>s[1..i-1]</code> to <code>t[1..j-1]</code> in <code>k</code> operations, then we can do the same to <code>s[1..i]</code>, and exchange the original <code>s[i]</code> for <code>t[j]</code> afterwards, for a total of <code>k+1</code> operations (substitution).\u000a* The operations required to transform <code>s[1..n]</code> into <code>t[1..m]</code> is of course the number required to transform all of <code>s</code> into all of <code>t</code>, and so <code>d[n,m]</code> holds our result.\u000a\u000aThis proof fails to validate that the number placed in <code>d[i,j]</code> is in fact minimal; this is more difficult to show, and involves an [[Reductio ad absurdum|argument by contradiction]] in which we assume <code>d[i,j]</code> is smaller than the minimum of the three, and use this to show one of the three is not minimal.\u000a\u000a===Possible improvements===\u000aPossible improvements to this algorithm include:\u000a* We can adapt the algorithm to use less space, [[Big O notation|''O'']](''m'') instead of ''O''(''mn''), since it only requires that the previous row and current row be stored at any one time.\u000a* We can store the number of insertions, deletions, and substitutions separately, or even the positions at which they occur, which is always <code>j</code>.\u000a* We can normalize the distance to the interval <code>[0,1]</code>.\u000a* If we are only interested in the distance if it is smaller than a threshold ''k'', then it suffices to compute a diagonal stripe of width ''2k+1'' in the matrix. In this way, the algorithm can be run in [[Big O notation|''O'']](''kl'') time, where ''l'' is the length of the shortest string.<ref>{{cite book |author=Gusfield, Dan |title=Algorithms on strings, trees, and sequences: computer science and computational biology |publisher=Cambridge University Press |location=Cambridge, UK |year=1997 |isbn=0-521-58519-8 }}</ref>\u000a* We can give different penalty costs to insertion, deletion and substitution. We can also give penalty costs that depend on which characters are inserted, deleted or substituted.\u000a* This algorithm [[parallel computing|parallelizes]] poorly, due to a large number of [[data dependency|data dependencies]]. However, all the <code>cost</code> values can be computed in parallel, and the algorithm can be adapted to perform the <code>minimum</code> function in phases to eliminate dependencies.\u000a* By examining diagonals instead of rows, and by using [[lazy evaluation]], we can find the Levenshtein distance in ''O''(''m'' (1 + ''d'')) time (where ''d'' is the Levenshtein distance), which is much faster than the regular dynamic programming algorithm if the distance is small.<ref>{{cite journal |author=Allison L |title=Lazy Dynamic-Programming can be Eager |journal=Inf. Proc. Letters |volume=43 |issue=4 |pages=207\u201312 |date=September 1992 |url=http://www.csse.monash.edu.au/~lloyd/tildeStrings/Alignment/92.IPL.html |doi=10.1016/0020-0190(92)90202-7}}</ref>\u000a\u000a==Seller's variant for string search==\u000aBy initializing the first row of the matrix with zeros, we obtain a variant of the Wagner\u2013Fischer algorithm that can be used for [[fuzzy string searching|fuzzy string search]] of a string in a text.<ref name="navarro">{{cite doi|10.1145/375360.375365}}</ref> This modification gives the end-position of matching substrings of the text. To determine the start-position of the matching substrings, the number of insertions and deletions can be stored separately and used to compute the start-position from the end-position.<ref>Bruno Woltzenlogel Paleo. [http://www.logic.at/people/bruno/Papers/2007-GATE-ESSLLI.pdf An approximate gazetteer for GATE based on levenshtein distance]. Student Section of the European Summer School in Logic, Language and Information ([[European Summer School in Logic, Language and Information|ESSLLI]]), 2007.</ref>\u000a\u000aThe resulting algorithm is by no means efficient, but was at the time of its publication (1980) one of the first algorithms that performed approximate search.<ref name="navarro"/>\u000a\u000a== References ==\u000a{{Reflist|30em}}\u000a\u000a{{DEFAULTSORT:Wagner-Fischer algorithm}}\u000a[[Category:Algorithms on strings]]\u000a[[Category:String similarity measures]]
p230
sg6
VWagner\u2013Fischer algorithm
p231
ssI86
(dp232
g2
S'http://en.wikipedia.org/wiki/Greenpilot'
p233
sg4
V{{COI|date=April 2010}}\u000aThe online portal '''Greenpilot''' is a service provided by the German National Library of Medicine, ZB MED.\u000a\u000aThe project is funded by the German Research Foundation ([[Deutsche Forschungsgemeinschaft]]) and gets its technical support from  [[Averbis]] Ltd. The portal first went online May 29, 2009 and currently runs in the updated beta version. In the context of the 'Germany - Land of Ideas' (Deutschland - Land der Ideen) initiative under the patronage of the [[President of Germany]] [[Horst Köhler]] the ZB MED was awarded the distinction 'Selected Landmark 2009' (Ausgewählter Ort 2009).<ref>[http://idw-online.de/pages/de/news315583 Pressemitteilung im Informationsdienst Wissenschaft vom 15. Mai 2009 ]</ref>\u000a\u000a==Objective==\u000aThe Greenpilot portal is a [[digital library]] specialised in the fields of Nutritional, Agricultural and Environmental Sciences. It aims to provide researchers in the three fields with a collection of scientific literature which is easy to access and of high quality. Especially the [[gray literature]] is often difficult to find and retrieve for the average user so Greenpilot also aims to make access to these sources easier. The service addresses itself not only to scientists and students but also to the broadly interested public. Greenpilot has been modelled after the corresponding digital library for Medicine, Medpilot,<ref>[http://www.medpilot.de/ Medpilot portal]</ref> also a project of the German National Library of Medicine. The ZB MED has chosen the slogan 'Greenpilot - all about life and science' as a motto. In Greenpilot scientifically relevant databases, library catalogues and websites can be searched by entering a search term and the results are presented in a standardised web interface.\u000a\u000a==Technical Background==\u000aGreenpilot is a search engine based on intuitive search engine technology. The portal's software was developed in the programming language [[Perl]]. The search engine technology is based upon the 'Averbis Search Platform' software developed by the Averbis Ltd. and uses the [[open source]] software [[Lucene]]. Functionally this is an expert search engine which centres around the intelligent semantic connection of search terms by means of a standardised vocabulary. This is made possible by Averbis's MSI software which provides:\u000a\u000a* semantic search optimised for the fields of Medicine and Life Sciences\u000a* a contextual analysis of texts taking synonyms and compounds into account\u000a* multilingual and cross-language search\u000a* linking of lay and expert vocabulary\u000aThe search results are generated from a search index.\u000a\u000aAdditionally a [[metasearch]] can be conducted in order to search other databases not contained in the index. This search is based upon individual results from the specific database searched.\u000a\u000a==Contents==\u000aThe Greenpilot portal integrates various scientifically relevant information resources under a uniform search interface. These resources are diverse and encompass national and international expert databases, library catalogues of national libraries with a focus on specific topics, full text documents from [[open access (publishing)|open access]] journals as well as information contained on about one thousand scientifically relevant websites selected for Greenpilot.\u000aThe following is a list of sources from November 2009:<ref>[http://www.greenpilot.de/beta2/app/misc/help/8cafcf93601eb861aaef86b5ce99ecdc/Datenbanken List of databases in Greenpilot]</ref>\u000a\u000a===Library Catalogues===\u000a* Catalogue of the German National Library of Medicine (ZB MED Nutrition. Environment. Agriculture)\u000a* Catalogue of the German National Library of Medicine (ZB MED Medicine. Health)\u000a* Catalogue of the Bonn University Library\u000a* Library catalogues of scientifically relevant departments within the collective library network (GBV)\u000a* Catalogue of the Federal Ministry of Food, Agriculture and Consumer Protection (BMELV)\u000a* Catalogue of the Johann Heinrich von Thünen-Institut (vTI), Federal Research Institute for Rural Areas, Forestry and Fisheries\u000a* Catalogue of the Julius Kühn-Institut, Federal Research Centre for Cultivated Plants\u000a* Catalogue of the Friedrich Löffler-Institut, Federal Research Institute for Animal Health\u000a* Catalogue of the Max Rubner-Institut, Federal Research Institute for Nutrition and Food\u000a* Catalogue of the Federal Institute for Risk Assessment\u000a* Catalogue of the Leibniz Institute for Marine Science (IFM-GEOMAR)\u000a* Catalogue of the Leibniz Institute for Plant Genetics and Crop Plant Research (IPK-Plant Genetics and Crop Plant)\u000a* Catalogue of the Leibniz Institute for Plant Biochemistry (IPB-Plant Chemistry)\u000a* Catalogue of the special collection inshore and deep-sea fishery\u000a* Catalogue of the University of Veterinary Medicine Hannover (TiHo-Veterinary Sciences)\u000a* Catalogue of the German National Library of Economics (ZBW)\u000a\u000a===Bibliographic databases===\u000a* AGRIS (1975\u20132008), FAO ( Food and Agriculture Organization of the United Nations)\u000a* VITIS-VEA, Viticulture and Enology Abstracts\u000a* Medline (2004\u20132009)\u000a* UFORDAT, Environmental Research Database (UBA)\u000a* ULIDAT, Environmental Literature Database (UBA)\u000a* ELFIS, International Information System for the Agricultural Sciences and Technology\u000a\u000a===Relevant Internet Sources===\u000a* Reviewed list of [[URL]]s selected by the ZB MED Nutrition. Environment. Agriculture\u000a* Open Access journals with full text documents\u000a\u000a===Metasearch===\u000a* GetInfo, the knowledge portal for Technical Science provided by the Library for Technical Sciences (TIB) and the professional information centres FIZ Technik Frankfurt, FIZ Karlsruhe and FIZ CHEMIE Berlin.\u000a* ECONIS, Catalogue of the German National Library of Economics (ZBW).\u000a\u000a==Other Features==\u000a\u000a===Search and results page===\u000a* Search and advanced search\u000a* Context sensitive help function\u000a* [[Truncation]] and [[Boolean function]]s\u000a* Personalised refining of search results by filtering for a specific document type, language or database\u000a* [[Bookmark]]s\u000a\u000a===Document ordering===\u000a* Ordering directly from the results page is made possible by using the document delivery service of the ZB MED or the Electronic Journals Library ([[Elektronische Zeitschriftenbibliothek]]).\u000a\u000a===Personalisation===\u000a* My Greenpilot: a feature requiring the user to sign up for an account. The service is free of charge and offers an overview of ordered documents as well as enabling individual managing of customer data.\u000a\u000a==See also==\u000a*[[List of digital library projects]]\u000a*[[vascoda]]\u000a\u000a==References==\u000a<references />\u000a\u000a==External links==\u000a* [http://www.greenpilot.de Greenpilot website]\u000a* [http://www.zbmed.de/home.html?lang=en Website of the German National Library of Medicine, ZB MED]\u000a* [http://www.land-of-ideas.org Germany - Land of Ideas website]\u000a\u000a{{coord missing|Germany}}\u000a\u000a[[Category:Libraries in Germany]]\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines]]
p234
sg6
S'Greenpilot'
p235
ssI215
(dp236
g2
S'http://en.wikipedia.org/wiki/Simple matching coefficient'
p237
sg4
S"The '''Simple Matching Coefficient (SMC)''' is a [[statistic]] used for comparing the [[Similarity measure|similarity]] and [[diversity index|diversity]] of [[Sample (statistics)|sample]] sets.<ref>http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sdaugherty/similarity.html</ref>\n\nGiven two objects, A and B, each with n binary attributes, SMC is defined as:\n:<math> SMC = {\\text{Number of Matching Attributes}\\over \\text{Number of Attributes}} = {{M_{00}+M_{11}}\\over{M_{00}+M_{01}+M_{10}+M_{11}}}</Math>\n\nWhere:\n:<math>M_{11}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 1.\n:<math>M_{01}</math> represents the total number of attributes where the attribute of ''A'' is 0 and the attribute of ''B'' is 1.\n:<math>M_{10}</math> represents the total number of attributes where the attribute of ''A'' is 1 and the attribute of ''B'' is 0.\n:<math>M_{00}</math> represents the total number of attributes where ''A'' and ''B'' both have a value of 0.\n\n== See also ==\n* [[Jaccard index]]\n\n==Notes==\n{{reflist}}\n\n[[Category:Index numbers]]\n[[Category:Measure theory]]\n[[Category:Clustering criteria]]\n[[Category:String similarity measures]]"
p238
sg6
S'Simple matching coefficient'
p239
ssI89
(dp240
g2
S'http://en.wikipedia.org/wiki/Statistical semantics'
p241
sg4
V{{linguistics}}\u000a'''Statistical semantics''' is the study of "how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access" {{citation needed|date=July 2012}}<!--([[George Furnas|Furnas]], 2006)--this page has been moved and the new version no longer contains this quotation-->. How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?\u000a\u000a==History==\u000a\u000aThe term ''Statistical Semantics'' was first used by [[Warren Weaver]] in his well-known paper on [[machine translation]].<ref>{{harvnb|Weaver|1955}}</ref> He argued that [[word sense disambiguation]] for machine translation should be based on the [[co-occurrence]] frequency of the context words near a given target word. The underlying assumption that "a word is characterized by the company it keeps" was advocated by [[J. R. Firth|J.R. Firth]].<ref>{{harvnb|Firth|1957}}</ref> This assumption is known in [[Linguistics]] as the [[Distributional hypothesis|Distributional Hypothesis]].<ref>{{harvnb|Sahlgren|2008}}</ref> Emile Delavenay defined ''Statistical Semantics'' as "Statistical study of meanings of words and their frequency and order of recurrence."<ref>{{harvnb|Delavenay|1960}}</ref> "[[George Furnas|Furnas]] ''et al.'' 1983" is frequently cited as a foundational contribution to Statistical Semantics.<ref>{{harvnb|Furnas|Landauer|Gomez|Dumais|1983}}</ref>  An early success in the field was [[Latent semantic analysis|Latent Semantic Analysis]].\u000a\u000a==Applications of statistical semantics==\u000a\u000aResearch in Statistical Semantics has resulted in a wide variety of algorithms that use the Distributional Hypothesis to discover many aspects of [[semantics]], by applying statistical techniques to [[Text corpus|large corpora]]:\u000a* Measuring the [[Semantic similarity|similarity in word meanings]] <ref>{{harvnb|Lund|Burgess|Atchley|1995}}</ref><ref>{{harvnb|Landauer|Dumais|1997}}</ref><ref>{{harvnb|McDonald|Ramscar|2001}}</ref><ref>{{harvnb|Terra|Clarke|2003}}</ref>\u000a* Measuring the similarity in word relations <ref>{{harvnb|Turney|2006}}</ref>\u000a* Modeling [[similarity-based generalization]] <ref>{{harvnb|Yarlett|2008}}</ref>\u000a* Discovering words with a given relation <ref>{{harvnb|Hearst|1992}}</ref>\u000a* Classifying relations between words <ref>{{harvnb|Turney|Littman|2005}}</ref>\u000a* Extracting keywords from documents <ref>{{harvnb|Frank|Paynter|Witten|Gutwin|1999}}</ref><ref>{{harvnb|Turney|2000}}</ref>\u000a* Measuring the cohesiveness of text <ref>{{harvnb|Turney|2003}}</ref>\u000a* Discovering the different senses of words <ref>{{harvnb|Pantel|Lin|2002}}</ref>\u000a* Distinguishing the different senses of words <ref>{{harvnb|Turney|2004}}</ref>\u000a* Subcognitive aspects of words <ref>{{harvnb|Turney|2001}}</ref>\u000a* Distinguishing praise from criticism <ref>{{harvnb|Turney|Littman|2003}}</ref>\u000a\u000a==Related fields==\u000a\u000aStatistical Semantics focuses on the meanings of common words and the relations between common words, unlike [[text mining]], which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical Semantics is a subfield of [[computational semantics]], which is in turn a subfield of [[computational linguistics]] and [[natural language processing]].\u000a\u000aMany of the applications of Statistical Semantics (listed above) can also be addressed by [[lexicon]]-based algorithms, instead of the [[text corpus|corpus]]-based algorithms of Statistical Semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.<ref>{{harvnb|Turney|Littman|Bigham|Shnayder|2003}}</ref>\u000a\u000a==See also==\u000a{{Portal|Linguistics}}\u000a*[[Latent semantic analysis]]\u000a*[[Latent semantic indexing]]\u000a*[[Text mining]]\u000a*[[Information retrieval]]\u000a*[[Natural language processing]]\u000a*[[Computational linguistics]]\u000a*[[Web mining]]\u000a*[[Semantic similarity]]\u000a*[[Co-occurrence]]\u000a*[[Text corpus]]\u000a*[[Semantic Analytics]]\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a===Sources===\u000a{{refbegin}}\u000a* {{cite book | last = Delavenay | first = Emile | year = 1960 | title = An Introduction to Machine Translation | location = New York, NY | publisher = [[Thames and Hudson]] | oclc = 1001646 | ref = harv }}\u000a\u000a* {{cite journal | last = Firth | first = John R. | authorlink = John Rupert Firth | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = [[Studies in Linguistic Analysis]] | pages = 1\u201332 | location = Oxford | publisher = [[Philological Society]] | ref = harv }}\u000a*: Reprinted in {{cite book | editor1-first = F.R. | editor1-last = Palmer | title = Selected Papers of J.R. Firth 1952-1959 | location = London | publisher = Longman | year = 1968 | oclc = 123573912 }}\u000a\u000a* {{cite conference | last1 = Frank | first1 = Eibe | last2 = Paynter | first2 = Gordon W. | last3 = Witten | first3 = Ian H. | last4 = Gutwin | first4 = Carl | last5 = Nevill-Manning | first5 = Craig G. | year = 1999 | title = Domain-specific keyphrase extraction | booktitle = Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence | conference = [[International Joint Conference on Artificial Intelligence|IJCAI-99]] | volume = 2 | pages = 668\u2013673 | location = California | publisher = Morgan Kaufmann | isbn = 1-55860-613-0 | id = {{citeseerx|10.1.1.43.9100}} {{citeseerx|10.1.1.148.3598}} | ref = harv }}\u000a\u000a* {{cite journal | last1 = Furnas | first1 = George W. | authorlink = George Furnas | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | year = 1983 | title = Statistical semantics: Analysis of the potential performance of keyword information systems | url = http://furnas.people.si.umich.edu/Papers/FurnasEtAl1983_BSTJ_p1753.pdf | journal = [[Bell System Technical Journal]] | volume = 62 | issue = 6 | pages = 1753\u20131806 | ref = harv | doi=10.1002/j.1538-7305.1983.tb03513.x}}\u000a\u000a* {{cite conference | last = Hearst | first = Marti A. | year = 1992 | title = Automatic Acquisition of Hyponyms from Large Text Corpora | booktitle = Proceedings of the Fourteenth International Conference on Computational Linguistics | conference = [[COLING|COLING '92]] | pages = 539\u2013545 | location = Nantes, France | url = http://acl.ldc.upenn.edu/C/C92/C92-2082.pdf | doi = 10.3115/992133.992154 | id = {{citeseerx|10.1.1.36.701}} | ref = harv }}\u000a\u000a* {{cite journal | last1 = Landauer | first1 = Thomas K. | last2 = Dumais | first2 = Susan T. | year = 1997 | title = A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge | journal = [[Psychological Review]] | volume = 104 | issue = 2 | pages = 211\u2013240 | url = http://lsa.colorado.edu/papers/plato/plato.annote.html | id = {{citeseerx|10.1.1.184.4759}} | ref = harv | doi=10.1037/0033-295x.104.2.211}}\u000a\u000a* {{cite conference | last1 = Lund | first1 = Kevin | last2 = Burgess | first2 = Curt | last3 = Atchley | first3 = Ruth Ann | year = 1995 | title = Semantic and associative priming in high-dimensional semantic space | booktitle = Proceedings of the 17th Annual Conference of the Cognitive Science Society | publisher = [[Cognitive Science Society]] | pages = 660\u2013665 | url = http://locutus.ucr.edu/reprintPDFs/lba95csp.pdf | ref = harv }}\u000a\u000a* {{cite conference | last1 = McDonald | first1 = Scott | last2 = Ramscar | first2 = Michael | year = 2001 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611\u2013616 | url = http://homepages.inf.ed.ac.uk/smcdonal/cogsci2001.pdf | id = {{citeseerx|10.1.1.104.7535}} | ref = harv }}\u000a\u000a* {{cite conference | last1 = Pantel | first1 = Patrick | last2 = Lin | first2 = Dekang | year = 2002 | title = Discovering word senses from text | booktitle = Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining | isbn = 1-58113-567-X | conference = [[KDD Conference|KDD '02]] | pages = 613\u2013619 | id = {{citeseerx|10.1.1.12.6771}} | doi = 10.1145/775047.775138 | ref = harv }}\u000a\u000a* {{cite journal | last1 = Sahlgren | first1 = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33\u201353 | ref = harv}}\u000a\u000a* {{cite conference | last1 = Terra | first1 = Egidio L. | last2 = Clarke | first2 = Charles L. A. | year = 2003 | title = Frequency estimates for statistical word similarity measures | booktitle = Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 | conference = HLT/NAACL 2003 | pages = 244\u2013251 | url = http://acl.ldc.upenn.edu/N/N03/N03-1032.pdf | id = {{citeseerx|10.1.1.12.9041}} | doi = 10.3115/1073445.1073477 | ref = harv }}\u000a\u000a* {{cite journal | last = Turney | first = Peter D. |date=May 2000 | title = Learning algorithms for keyphrase extraction | journal = [[Information Retrieval (journal)|Information Retrieval]] | volume = 2 | issue = 4 | pages = 303\u2013336 | arxiv = cs/0212020 | id = {{citeseerx|10.1.1.11.1829}} | doi = 10.1023/A:1009976227802 | ref = harv }}\u000a\u000a* {{cite journal | last = Turney | first = Peter D. | year = 2001 | title = Answering subcognitive Turing Test questions: A reply to French | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]] | volume = 13 | issue = 4 | pages = 409\u2013419 | arxiv = cs/0212015 | id = {{citeseerx|10.1.1.12.8734}} | ref = harv | doi=10.1080/09528130110100270}}\u000a\u000a* {{cite conference | last = Turney | first = Peter D. | year = 2003 | title = Coherent keyphrase extraction via Web mining | booktitle = Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence | conference = IJCAI-03 | location = Acapulco, Mexico | pages = 434\u2013439 | arxiv = cs/0308033 | id = {{citeseerx|10.1.1.100.3751}} | ref = harv }}\u000a\u000a* {{cite conference | last = Turney | first = Peter D. | year = 2004 | title = Word sense disambiguation by Web mining for word co-occurrence probabilities | booktitle = Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | conference = SENSEVAL-3 | location = Barcelona, Spain | pages = 239\u2013242 | arxiv = cs/0407065 | url = http://cogprints.org/3732/ | ref = harv }}\u000a\u000a* {{cite journal | last = Turney | first = Peter D. | year = 2006 | title = Similarity of semantic relations |journal = [[Computational Linguistics (journal)|Computational Linguistics]] | volume = 32 | issue = 3 | pages = 379\u2013416 | arxiv = cs/0608100 | url = http://cogprints.org/5098/ | doi = 10.1162/coli.2006.32.3.379 | id = {{citeseerx|10.1.1.75.8007}} | ref = harv }}\u000a\u000a* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. |date=October 2003 | title = Measuring praise and criticism: Inference of semantic orientation from association | journal = [[ACM Transactions on Information Systems]] (TOIS) | volume = 21 | issue = 4 | pages = 315\u2013346 | arxiv = cs/0309034 | url = http://cogprints.org/3164/ | id = {{citeseerx|10.1.1.9.6425}} | doi = 10.1145/944012.944013 | ref = harv }}\u000a\u000a* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | year = 2005 | title = Corpus-based Learning of Analogies and Semantic Relations | journal = [[Machine Learning (journal)|Machine Learning]] | volume = 60 | issue = 1\u20133 | pages = 251\u2013278 | arxiv = cs/0508103 | id = {{citeseerx|10.1.1.90.9819}} | doi = 10.1007/s10994-005-0913-1 | url = http://cogprints.org/4518/ | ref = harv }}\u000a\u000a* {{cite conference | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | last3 = Bigham | first3 = Jeffrey | last4 = Shnayder | first4 = Victor | year = 2003 | title = Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems | booktitle = Proceedings of the International Conference on Recent Advances in Natural Language Processing | conference = RANLP-03 | location = [[Borovets]], Bulgaria | pages = 482\u2013489 | arxiv = cs/0309035 | id = {{citeseerx|10.1.1.5.2939}} | url = http://cogprints.org/3163/ | ref = harv }}\u000a\u000a* {{cite book | last = Weaver | first = Warren | authorlink = Warren Weaver | year = 1955 | chapter = Translation | chapter-url = http://www.mt-archive.info/Weaver-1949.pdf | editor1-first = W.N. | editor1-last = Locke | editor2-first = D.A. | editor2-last = Booth | title = Machine Translation of Languages | location = [[Cambridge, Massachusetts]] | publisher = [[MIT Press]] | isbn = 0-8371-8434-7 | pages = 15\u201323 | ref = harv }}\u000a\u000a* {{cite thesis | last = Yarlett | first = Daniel G. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv }}\u000a{{refend}}\u000a\u000a==External links==\u000a* {{cite web | url = http://www.si.umich.edu/people/george-furnas | work = Faculty Profile | title = George Furnas | publisher = University of Michigan, School of Information | accessdate = 2010-07-12 }}\u000a*[http://research.microsoft.com/%7Esdumais/ Susan Dumais]\u000a*[http://www.pearsonkt.com/bioLandauer.shtml Thomas Landauer]\u000a*[http://www.apperceptual.com/ Peter Turney]\u000a*[http://waldron.stanford.edu/~michael/papers/ Michael Ramscar]\u000a*[http://www.cs.ualberta.ca/~lindek/demos.htm Dekang Lin's Demos]\u000a*[http://www.isi.edu/~pantel/Content/demos.htm Patrick Pantel's Demos]\u000a*[http://www.nzdl.org/Kea/ Kea keyphrase extraction]\u000a*[http://seokeywordanalysis.com/seotools/ Online keyphrase extractor]\u000a\u000a{{DEFAULTSORT:Statistical Semantics}}\u000a[[Category:Artificial intelligence applications]]\u000a[[Category:Computational linguistics]]\u000a[[Category:Information retrieval]]\u000a[[Category:Semantics]]\u000a[[Category:Statistical natural language processing]]\u000a[[Category:Fields of application of statistics]]
p242
sg6
S'Statistical semantics'
p243
ssI218
(dp244
g2
S'http://en.wikipedia.org/wiki/Arts and Humanities Citation Index'
p245
sg4
S'{{ infobox bibliographic database\n| image       = \n| caption     = \n| producer    =Thomson Reuters \n| country     =United States \n| history     = \n| languages   = \n| providers   =Web of Science, Dialog Bluesheets \n| cost        =Subscription \n| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio \n| depth       =Index, abstract, citation indexing, author \n| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances \n| temporal    =1975 to present \n| geospatial  =global \n| number      = \n| updates     = \n| p_title     = \n| p_dates     = \n| ISSN        = \n| web         = \n| titles      =  \n}}\n\nThe \'\'\'\'\'Arts & Humanities Citation Index\'\'\'\'\' (\'\'\'A&HCI\'\'\'), also known as \'\'\'\'\'Arts & Humanities Search\'\'\'\'\', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore the print counterpart is Current Contents.\n\nSubjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio. \n\nAvailable citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances. \n\nThis database can be accessed online through \'\'[[Web of Science]]\'\'. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.\n\nAccording to Thomson Reuters, the \'\'Arts & Humanities Search\'\', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.<ref name=dialog-blue>\n{{Cite web\n  | title =Arts & Humanities Search (File 255) \n  | publisher =Dialog bluesheets  \n  | date = \n  | url =http://library.dialog.com/bluesheets/html/bl0439.html \n  | format =Online web page \n  | accessdate =2011-07-03}}</ref><ref name=Iowa>\nDescription of Arts & Humanities Search. \n{{Cite web\n  | title =e-Library catalog\n  | publisher =Iowas State University  \n  | year =2008 \n  | url =http://www.lib.iastate.edu/collections/db/artshm.html\n  | format =Online web page \n  | accessdate =2011-07-03}}</ref><ref name=Iowa-wos>\nDescription of Web of Science coverage.  \n{{Cite web\n  | title =e-Library catalog\n  | publisher =Iowas State University  \n  | year =2008 \n  | url =http://www.lib.iastate.edu/collections/db/websci.html\n  | format =Online web page \n  | accessdate =2011-07-03}}</ref><ref name=TR>\nSee the page entitled "Tech Specs" \n{{Cite web\n  | title =Database description\n  | publisher =Thomson Reuters  \n  | year = \n  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3\n  | format =Online web page \n  | accessdate =2011-07-03}}</ref>\n==History==\nThe index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]\' IP & Science division.\n\n==See also==\n* [[Science Citation Index]]\n* [[Social Sciences Citation Index]]\n\n==References==\n{{Reflist}}\n\n== External links ==\n* {{Official|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.\n* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.\n\n{{Thomson Reuters}}\n[[Category:Citation indices]]\n[[Category:Thomson Reuters]]\n\n{{DEFAULTSORT:Arts And Humanities Citation Index}}'
p246
sg6
S'Arts and Humanities Citation Index'
p247
ssI92
(dp248
g2
S'http://en.wikipedia.org/wiki/EXCLAIM'
p249
sg4
S'{{For|the Canadian magazine|Exclaim!}}\nThe \'\'\'EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)\'\'\' is an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006. It is currently in a beta stage of development, with some support for more than a dozen languages. The lead developers are Justin Nuger and Jesse Saba Kirchner.\n\nEarly work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.<ref>\n{{cite web\n|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web\n|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf\n|format=PDF|publisher=ACM-SIGIR 1999\n|accessdate=2006-12-02\n}}\n</ref>\n\nEXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).\n\nOne of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.\n\n==Current status==\n\nEXCLAIM is in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, is available for the following twenty-three languages:\n\n{| class="wikitable"\n| [[Albanian language|Albanian]]\n|-\n| [[Amharic]]\n|-\n| [[Bengali language|Bengali]]\n|-\n| [[Gothic language|Gothic]]\n|-\n| [[Greek language|Greek]]\n|-\n| [[Icelandic language|Icelandic]]\n|-\n| [[Indonesian language|Indonesian]]\n|-\n| [[Irish language|Irish]]\n|-\n| [[Javanese language|Javanese]]\n|-\n| [[Latvian language|Latvian]]\n|-\n| [[Malagasy language|Malagasy]]\n|-\n| [[Mandarin Chinese]]\n|-\n| [[Nahuatl]]\n|-\n| [[Navajo language|Navajo]]\n|-\n| [[Quechua languages|Quechua]]\n|-\n| [[Sardinian language|Sardinian]]\n|-\n| [[Swahili language|Swahili]]\n|-\n| [[Tagalog language|Tagalog]]\n|-\n| [[Standard Tibetan|Tibetan]]\n|-\n| [[Turkish language|Turkish]]\n|-\n| [[Welsh language|Welsh]]\n|-\n| [[Wolof language|Wolof]]\n|-\n| [[Yiddish]]\n|}\n\nSupport using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:\n\n{| class="wikitable"\n|-\n| [[Dutch language|Dutch]]\n|-\n| [[Spanish language|Spanish]]\n|}\n\nSignificant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.\n\nFuture versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.\n\nThe EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.\n\n==Further applications==\n\nEXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].<ref>{{cite web\n|title=A crosslinguistic readability framework\n|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf\n|format=PDF|publisher=ACL-IJNLP 2009\n|accessdate=2009-09-04\n}}\n</ref>\n\n==Notes and references==\n\n{{reflist}}\n\n==External links==\n*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website]\n*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]\n*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]\n*[http://ju-st.in/ Justin Nuger\'s professional webpage]\n*[http://people.ucsc.edu/~kirchner/ Jesse Saba Kirchner\'s professional webpage]\n\n{{DEFAULTSORT:Exclaim}}\n[[Category:Information retrieval]]'
p250
sg6
S'EXCLAIM'
p251
ssI221
(dp252
g2
S'http://en.wikipedia.org/wiki/Islamic World Science Citation Database'
p253
sg4
S"'''Islamic World Science Citation Database''' (ISC) is a [[citation index]] established by the Iranian [[Ministry of Science, Research and Technology]] after it was approved by the [[Organisation of the Islamic Conference]].  It only indexes journals from the [[Islamic world]].\n\nIt was announced in [[Baku]], Azerbaijan during the Fourth Islamic Conference of the Ministers of Higher Education and Scientific Research held in October 2008.<ref>{{cite news | url = http://www.scidev.net/en/science-communication/science-publishing/news/islamic-countries-to-get-own-science-citation-inde.html | title = Islamic countries to get own science citation index | author = Wagdy Sawahel | date = 17 October 2008 | publisher = [[SciDev.Net]] }}</ref>  It is managed by the Islamic World Science Citation Center, located in [[Shiraz]].\n\nIn 2009, ISC partnered with [[Scopus]] that allows ISC's publications to be indexed in Scopus.<ref>{{cite journal | journal = [[Library Connect]] | title = The Islamic World Science Citation Database partnership with Scopus brings greater visibility to Islamic researchers | url = http://libraryconnect.elsevier.com/lcn/0703/lcn070319.html | author = Ahmed Rostom | volume = 7 | issue = 3 | date = August 2009 | issn = 1549-3725 }}</ref>\n\n== References ==\n{{Reflist}}\n\n==See also==\n* [[Academic publishing]]\n* [[List of academic databases and search engines]]\n* [[Impact factor]]\n\n== External links ==\n* {{Official website|http://www.isc.gov.ir/isce.htm}}\n\n[[Category:Bibliographic databases]]\n[[Category:Online databases]]\n[[Category:Citation indices]]\n[[Category:Research management]]\n[[Category:Databases in Iran]]\n\n\n{{science-journal-stub}}\n{{islam-stub}}"
p254
sg6
S'Islamic World Science Citation Database'
p255
ssI95
(dp256
g2
S'http://en.wikipedia.org/wiki/Cross-language information retrieval'
p257
sg4
V{{refimprove|date=September 2014}}\u000a\u000a'''Cross-language information retrieval (CLIR)''' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.  CLIR techniques can be classified into different categories based on different translation resources: \u000a* Dictionary-based CLIR techniques\u000a* Parallel corpora based CLIR techniques\u000a* Comparable corpora based CLIR techniques\u000a* Machine translator based CLIR techniques\u000a\u000aThe first workshop on CLIR was held in Zürich during the SIGIR-96 conference.<ref>The proceedings of this workshop can be found in the book ''Cross-Language Information Retrieval'' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.</ref> Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).\u000a\u000aThe term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.\u000a\u000a==See also==\u000a*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)\u000a\u000a==References==\u000a<references />\u000a\u000a==External links==\u000a*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]\u000a\u000a{{DEFAULTSORT:Cross-Language Information Retrieval}}\u000a[[Category:Information retrieval]]\u000a[[Category:Natural language processing]]\u000a\u000a\u000a{{linguistics-stub}}
p258
sg6
S'Cross-language information retrieval'
p259
ssI224
(dp260
g2
S'http://en.wikipedia.org/wiki/CAB Direct (database)'
p261
sg4
S"{{Redirect|Global Health|other uses|Global health}}\n{{italic title}}\n{{Infobox Bibliographic Database\n|title =CAB Abstracts \n|image = \n|caption = \n|producer =[[CABI (organisation)|CABI]]\n|country =United Kingdom \n|history =1973 to present \n|languages =Fifty languages, English abstracts \n|providers =Datastar, Dialog bluesheets, STN International, CAB Direct (CABI's own platform), Thomson-Reuters [[Web of Knowledge]], EBSCO, OvidSP, Dimdi \n|cost = \n|disciplines =applied life sciences - agriculture, environment, veterinary sciences, applied economics, food science and nutrition \n|depth =bibliographic, abstracting and indexing \n|formats =journal articles, abstracts, proceedings, books, book chapters, monographs, annual reports, handbooks, bulletins, newsletters, discussion papers, field notes, technical information, thesis papers  \n|temporal =1973-Present \n|geospatial =Global - international \n|number =6 million + \n|updates =\n|p_title = \n|p_dates = \n|ISSN =\n|web = \n|titles =http://www.cabi.org/default.aspx?site=170&page=1028  \n}}\n{{Infobox Bibliographic Database\n|title =Global Health bibliographic database \n|image = \n|caption = \n|producer = \n|country = \n|history = \n|languages = 50 languages (158 countries)\n|providers =CAB Direct, SilverPlatter, Web of Knowledge, EBSCO, OvidSP, Dialog, Dimdi \n|cost = \n|disciplines =international health research (medical and public)\n|depth =bibliographic, abstracting and indexing \n|formats =scientific journals, reports, books and conferences \n|temporal =1973 to present \n|geospatial =global-international \n|number =1.2 million scientific records \n|updates = \n|p_title = \n|p_dates = \n|ISSN =\n|web = \n|titles =  \n}}\n\n'''CAB Direct''' is a source of references for the ''[[life sciences|applied life sciences]]'' It incorporates two  bibliographic databases: '''''CAB Abstracts''''' and '''''Global Health'''''. CAB Direct is an access point for multiple [[bibliographic databases]] produced by ''CABI''. This database contains 8.8 million [[bibliographic record]]s, which includes  85,000 full text articles. It also includes noteworthy literature reviews. News articles and reports are also part of this combined database.<ref name=direct>{{cite web\n  | title =CAB Direct \n  | work = \n  | publisher =CABI  \n  | date =July 2010 \n  | url =http://www.cabdirect.org/ \n  | accessdate =2010-07-18}}</ref>\n\nIn the U.K., in 1947, the ''Imperial Agricultural Bureaux'' became the ''Commonwealth Agricultural Bureaux'' or ''CAB''. In 1986 the ''Commonwealth Agricultural Bureaux'' became ''[[CAB International]]'' or ''CABI''  <ref name=history-cabi>{{cite web\n  | title =Our history \n  | work =Bulleted history \n  | publisher =CABI  \n  | date =July 2010 \n  | url =http://www.cabi.org/default.aspx?site=170&page=1388 \n  | accessdate =2010-07-18}}</ref>\n\n==CAB Abstracts==\n'''CAB Abstracts''' is an applied life sciences bibliographic database emphasising [[agricultural]] literature, which is international in scope. It contains 6 million records, with coverage from 1973 to present day, adding 300,000 abstracts per year. Subject coverage includes [[agriculture]], [[environmental science|environment]], [[veterinary]] sciences, [[applied economics]], [[food science]] and nutrition. Database covers international issues in agriculture, [[forestry]], and allied disciplines in the life sciences. Indexed publications are from 150 countries in 50 languages, including English abstracts for most articles. Literature coverage includes journals, proceedings,  books, and a large collection of agricultural serials. Other non-journal formats are also indexed.<ref name=cabAb>\n{{cite web\n  | title =CAB Abstracts \n  | work = \n  | publisher =CABI  \n  | date =July 2010 \n  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=125 \n  | accessdate =2010-07-18}}</ref><ref name=TRcababse>{{cite web\n  | title =CAB Abstracts (Web of Knowledge) \n  | work = \n  | publisher =Thomson Reuters \n  | date =July 2010 \n  | url =http://science.thomsonreuters.com/training/cab/#overview \n  | accessdate =2010-07-18}}</ref><ref name=ovidCABabs>{{cite web\n  | title =CAB Abstracts \n  | work =Coverage is 1973-Present \n  | publisher =Ovid Technologies, Inc  \n  | date =December 2010 \n  | url =http://www.ovid.com/site/catalog/DataBase/31.jsp?top=2&mid=3&bottom=7&subsection=10  \n  | accessdate =2010-12-10}}</ref> \n===CAB Abstracts Archive===\n'''CAB Abstracts Archive''' is a searchable database produced by ''CABI''. It is created from 600 volumes of printed abstracts,  which are the collected and published [[scientific research]] from 1910 to 1972, and then digitized to form the archive. This archive database contains more than 1.8 million records which covers agriculture, [[veterinary]] science, nutrition and the environment. Subject coverage also includes [[biodiversity]], [[pest control]], [[environmental pollution]], [[animal disease]] (including [[zoonotic disease]]s), [[nutrition]], and [[food production]]. [[Natural resource management]] includes plant and [[animal breeding]]. CAB Abstracts Archive is also  indexed in other databases, which also serve as access points. These other databases are ''CAB Direct'', [[Web of Knowledge]], [[EBSCOhost]], [[Ovid Technologies|OvidSP]], and [[Dialog]].\n\nThe following print journals (digitized) comprise CAB Abstracts Archive:\n                                                \n:Animal Breeding Abstracts, Dairy Science Abstracts, Field Crop Abstracts, \n:Forestry Abstracts, Horticultural Science Abstracts, Nematological Abstracts, \n:Nutrition Abstracts and Reviews Series A: Human and Experimental, \n:Nutrition Abstracts and Reviews Series B: Livestock Feeds and Feeding,  \n:Plant Breeding Abstracts, Review of Agricultural Entomology, \n:Review of Medical and Veterinary Mycology, Review of Plant Pathology, \n:Review of Medical and Veterinary Entomology, Review of Plant Pathology, \n:Soils and Fertilizers, Tropical Veterinary Bulletin, Veterinary Bulletin  \n:and Weed Abstracts.\n\n===Weed Abstracts===\n'''''Weed Abstracts''''', derived from CAB Abstracts, is an abstracts database focused on [[scientific journal|published research]] regarding [[weed]]s and [[herbicides]]. This includes [[plant biology|weed biology]], encompassing [[research|research areas]] from [[genetics]] to [[ecology]], including [[parasitic]], [[poisonous]], [[allergenic]] and [[aquatic plant|aquatic]] weeds. Further coverage includes all topics related to [[weed control]], in both [[farming|crop]] and non-crop situations. Research on herbicides, includes formulations, [[herbicide resistance]] and the effects of [[herbicide residues]] in the environment. 10,000 records are add to this database per year. \n\n'''''Weed Abstracts''''' is updated weekly with summaries from notable English and foreign language journal articles, reports, conferences and books about weeds and herbicides. With the back-file, coverage is from 1990 to present day bringing the total of available research summaries to 130,000 records.<ref name=weedAb>{{cite web\n  | title =Weed Abstracts \n  | work = \n  | publisher =CABI  \n  | date =July 2010 \n  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2203\n  | accessdate =2010-07-20}}</ref>\n\n==Global Health database==\n'''''Global Health''''' is a bibliographic database which focuses on [[scientific literature|research literature]] in [[public health]] and [[Health science|medical health]] science sectors (including practice). Information (see infobox above) in indexed in more than 5000 [[academic journals]], and indexed from other sources such as reports, books and conferences.  Global Health contains over 1.2 million [[scientific]] records from 1973 to the present, with an addition of  90,000 indexed and abstracted records per year. Sources are abstracted from publications in 158 countries written in 50 languages. Any relevant non-English-language papers are translated into English. Proceedings, patents, thesis papers, electronic publications and relevant but difficult-to-find literature sources are also part of this database.<ref name=glbl-hlth-cabi>{{cite web\n  | title =Global Health overview \n  | work = \n  | publisher =CABI  \n  | date =July 2010 \n  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=328 \n  | accessdate =2010-07-18}}</ref><ref name=TRglblhlth>{{cite web\n  | title =Global Health (Web of Knowledge) \n  | work = \n  | publisher =Thomson Reuters \n  | date =July 2010 \n  | url =http://thomsonreuters.com/content/PDF/scientific/globalhealth_fs.pdf \n  | format =Free PDF download \n  | accessdate =2010-07-18}}</ref><ref name=ovidGH>{{cite web\n  | title =Global Health (Ovid) \n  | work = \n  | publisher =Ovid Technologies Inc. \n  | date =July 2010 \n  | url =http://www.ovid.com/site/catalog/DataBase/30.jsp?top=2&mid=3&bottom=7&subsection=10 \n  | accessdate =2010-07-18}}</ref> \n\n===Global Health Archive===\n'''''Global Health Archive''''' is a searchable database produced by CABI. It is created from 800,000 records, from six printed abstract journals,  which are collected published scientific research from 1910 to 1972, digitized to form the archive. Global Health Archive is also  indexed in other databases, which also serve as access points. These other databases are ''CAB Direct'', [[Web of Knowledge]], [[EBSCOhost]], [[Ovid Technologies|OvidSP]], and [[Dialog]].<ref name=ghArchive/>\n\nWhen combined with the ''Global Health'' database indexing coverage can be from 1910 to present day. Hence, coverage is made up of past [[epidemics]], from rates and patterns of disease [[Transmission (medicine)|transmission]], duration of [[pandemics]], timing of epidemiological peaks, [[geographic distribution]] of diseases, and [[World Health Organization|government preparedness]] and [[quarantine]] provisions.  The following can also be taken  into account:  effects on different age and [[social groups]], severity in developing vs. developed countries, [[symptoms]], causes of [[Human|mortality]] - such as secondary problems like [[pneumonia]] - and mortality rates.<ref name=ghArchive>{{cite web\n  | title =Global Health Archive \n  | work = \n  | publisher =CABI \n  | date =March 2010  \n  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2221 \n  | accessdate =2010-07-18}}</ref><ref name=ovidGHA>{{cite web\n  | title =Global Health Archive (Ovid)\n  | work = \n  | publisher =Ovid Technologies Inc. \n  | date =July 2010 \n  | url =http://www.ovid.com/site/catalog/DataBase/1748.jsp?top=2&mid=3&bottom=7&subsection=10 \n  | accessdate =2010-07-18}}</ref> \n\n====Journal and topic coverage====\nRecords for this database are derived from the following journals throughout certain years:<ref name=ghArchive/><ref name=ovidGHA/>\n\n:Tropical Diseases Bulletin (1912-83),\n:Abstracts on Hygiene and Communicable Diseases (1926-83), \n:Review of Veterinary and Medical Entomology (1913-72), \n:Review of Veterinary and Medical Mycology (1943-72) \n:Nutrition Abstracts and Reviews (1931-72), and Helminthological Abstracts (1932-72).\n\nSubject coverage includes [[Public health]], [[tropical disease|Tropical]] and [[Communicable disease]]s, Nutrition, [[Parasitology]], [[Entomology]], and [[Mycology]].\n\n===Tropical Diseases Bulletin===\n'''''Tropical Diseases Bulletin''''' is a bibliographic and abstracts database which focuses on research published regarding [[infectious disease]]s and [[public health]] in [[developing countries]] and the [[tropics]] and [[subtropics]]. This includes research areas from [[epidemiology]] to [[diagnosis]], [[therapy]] to [[disease prevention]], [[tropical medicine]], and related aspects of [[travel medicine]]. Published research coverage on [[patients]] and populations encompasses the health of marginalized populations: [[immigrant]]s, [[refugee]]s, and [[indigenous peoples]].<ref name=tropical/>\n\nBack-file coverage is from 1990 to present day, with an accessible base of 195,000 abstracts and the addition of 11,000 records per year. As a monthly journal '''''Tropical Diseases Bulletin''''' is also available in print. This print journal has author, subject and serials cited indexes.  Coverage of the print back-file is to 1912. A searchable, electronic database version of this journal is part of the ''Global Health Archive'' (see above).<ref name=tropical>\n{{cite web\n  | title =Tropical Diseases Bulletin\n  | work = \n  | publisher =CABI \n  | year =2010  \n  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2201\n  | accessdate =2010-07-18}}</ref>\n\n==Organic Research Database==\nThis indexing database focuses on scientific literature pertaining to all topics in  [[organic farming]], in both the [[temperate zone|temperate]] and [[tropical zone]]s. This includes [[sustainability|sustainability issues]] and [[soil science|soil fertility]]. Coverage is global; literature is obtained from 125 countries. The temporal coverage spans 30 years, 180,000 organic research abstracts, along with the addition of 8000 records per year. Linking to full text articles, guided searches, broad subject categorization along with subject refinement are also provided. The editorial advisory board of this database also commission reviews pertaining to organic farming.<ref name=organic>\n{{cite web\n  | title =Organic Research Database\n  | work =Description and bibliographic information \n  | publisher =CABI \n  | year =2011  \n  | url =http://www.cabi.org/organicresearch/default.aspx?site=154&page=932\n  | accessdate =2011-01-03}}</ref><ref name=usda>\n{{cite web\n  | title =Primary Research and Literature Databases\n  | work = focus on sustainable and alternative agricultural topics\n  | publisher =[[USDA]] - [[National Agriculture Library]] - [[AFSIC]] \n  | year =2011  \n  | url =http://afsic.nal.usda.gov/nal_display/index.php?info_center=2&tax_level=2&tax_subject=288&level3_id=0&level4_id=0&level5_id=0&topic_id=1597&&placement_default=0\n  | accessdate =2011-01-03}}</ref>\n\n==CABI full text repository==\n'''''CABI full text repository''''' is integrated into all ''CABI databases'' including CAB Abstracts, and Global Health. Both of these are online and print journals. Coverage includes 70,000 full text articles, through agreements with third party publishers. Eighty percent of the content is exclusive to CABI.<ref name=full-text/>  \n\nThe full text repository is made up of fifty percent journal articles, and equal percentage of conference (proceeding) papers, and other accessible literature is also included. Eighty percent of the articles are in English and coverage includes 56 countries. Also included in this database are relevant but hard to find materials which crosses disciplines consisting of [[agriculture]], [[health sciences|health]] and the [[life sciences]]. Main stream literature and hard to find materials of equal relevance are given equal access.<ref name=full-text>{{cite web\n  | title =CABI full text \n  | work = \n  | publisher =CABI \n  | date =March 2010  \n  | url =http://www.cabi.org/default.aspx?site=170&page=1016&pid=2227 \n  | accessdate =2010-07-18}}</ref>\n\n''CABI full text repository'' is indexed in other databases, which also serve as access points, consisting of ''Web of Knowledge (Thomson Reuters)'', ''CAB Direct'', ''OvidSP, Dialog, Dimdi, and EBSCOhost''.\n\n==References==\n{{Reflist}}\n\n[[Category:Bibliographic database providers]]\n[[Category:Bibliographic indexes]]\n[[Category:Citation indices]]\n[[Category:Environmental science]]\n[[Category:Global health]]"
p262
sg6
S'CAB Direct (database)'
p263
ssI98
(dp264
g2
S'http://en.wikipedia.org/wiki/Cluster labeling'
p265
sg4
VIn [[natural language processing]] and [[information retrieval]], '''cluster labeling''' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find labeling that summarize the topic of each cluster and distinguish the clusters from each other.\u000a\u000a==Differential cluster labeling==\u000aDifferential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson's chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Cluster Labeling''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html>.</ref>\u000a\u000a===Pointwise mutual information===\u000a\u000a{{Main|Pointwise mutual information}}\u000a\u000aIn the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:\u000a\u000a<math>I(X, Y) = \u005csum_{x\u005cin X}{ \u005csum_{y\u005cin Y} {p(x, y)log_2\u005cleft(\u005cfrac{p(x, y)}{p_1(x)p_2(y)}\u005cright)}}</math>\u000a\u000awhere ''p(x, y)'' is the [[joint probability|joint probability distribution]] of the two variables, ''p<sub>1</sub>(x)'' is the probability distribution of X, and ''p<sub>2</sub>(y)'' is the probability distribution of Y.\u000a\u000aIn the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Mutual Information''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html>.</ref>  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:\u000a\u000a<math>I(C, T) = \u005csum_{c\u005cin {0, 1}}{ \u005csum_{t\u005cin {0, 1}} {p(C = c, T = t)log_2\u005cleft(\u005cfrac{p(C = c, T = t)}{p(C = c)p(T = t)}\u005cright)}}</math>\u000a\u000aIn this case, ''p(C = 1)'' represents the probability that a randomly selected document is a member of a particular cluster, and ''p(C = 0)'' represents the probability that it isn't.  Similarly, ''p(T = 1)'' represents the probability that a randomly selected document contains a given term, and ''p(T = 0)'' represents the probability that it doesn't.  The [[joint probability|joint probability distribution function]] ''p(C, T)'' represents the probability that two events occur simultaneously.  For example, ''p(0, 0)'' is the probability that a document isn't a member of cluster ''c'' and doesn't contain term ''t''; ''p(0, 1)'' is the probability that a document isn't a member of cluster ''c'' and does contain term ''t''; and so on.\u000a\u000a===Chi-Squared Selection===\u000a{{Main|Pearson's chi-squared test}}\u000aThe Pearson's chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:\u000a\u000a<math>X^2 = \u005csum_{a \u005cin A}{\u005csum_{b \u005cin B}{\u005cfrac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>\u000a\u000awhere ''O<sub>a,b</sub>'' is the ''observed'' frequency of a and b co-occurring, and ''E<sub>a,b</sub>'' is the ''expected'' frequency of co-occurrence.\u000a\u000aIn the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:\u000a\u000a<math>X^2 = \u005csum_{a \u005cin {0,1}}{\u005csum_{b \u005cin {0,1}}{\u005cfrac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>\u000a\u000aFor example, ''O<sub>1,0</sub>'' is the observed number of documents that are in a particular cluster but don't contain a certain term, and ''E<sub>1,0</sub>'' is the expected number of documents that are in a particular cluster but don't contain a certain term.\u000aOur initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Chi2 Feature Selection''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html>.</ref>\u000a\u000a''E<sub>1,0</sub> = N * P(C = 1) * P(T = 0)''\u000a\u000awhere N is the total number of documents in the collection.\u000a\u000a==Cluster-Internal Labeling==\u000aCluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.\u000aCluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.\u000a\u000a===Centroid Labels===\u000a{{Main|Vector space model}}\u000aA frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.\u000aOne downside to using centroid labeling is that it can pick up words like "place" and "word" that have a high frequency in written text, but have little relevance to the contents of the particular cluster.\u000a\u000a===Contextualized centroid labels===\u000aA simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection. <ref>Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters\u2019 contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155</ref>\u000aIn this approach, a term-term co-occurrence matrix referred as <math>T_k</math> is first built for each cluster <math>S_k</math>. Each cell represents the number of times term <math>i</math> co-occurs with term <math>j</math> within a certain window of text (a sentence, a paragraph, etc.)\u000aIn a second stage, a similarity matrix <math>T_k^{sim}</math> is obtained by multiplying <math>T_k</math> with its transpose. We have <math>T_k^{sim}=T_k' T_k=(t_{{sim}_{ij}})</math>. Being the dot product of two normalized vectors <math>\u005ctilde{t}_{i}</math> and <math>\u005ctilde{t}_{j}</math>, <math>t_{{sim}_{ij}}</math> denotes the cosine similarity between terms <math>i</math> and <math>j</math>. The so obtained <math>T_k^{sim}</math> can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.\u000a\u000a===Title labels===\u000aAn alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.\u000a\u000a===External knowledge labels===\u000aCluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.<ref>David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146</ref> In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.\u000a\u000a==External links==\u000a* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]\u000a* [http://erulemaking.ucsur.pitt.edu/doc/papers/dgo06-labeling.pdf Automatically Labeling Hierarchical Clusters]\u000a\u000a==References==\u000a<references/>\u000a\u000a{{DEFAULTSORT:Cluster Labeling}}\u000a[[Category:Information retrieval]]
p266
sg6
S'Cluster labeling'
p267
ssI227
(dp268
g2
S'http://en.wikipedia.org/wiki/Russian Science Citation Index'
p269
sg4
S'{{primary sources|date=March 2012}}\n\'\'\'Russian Science Citation Index\'\'\' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.\n\n== Purpose ==\nFrom 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain "invisible" and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.\n\n== Functionality ==\nIn Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: \n* Scientific capacity and effectiveness of research, and\n* Publication activity\nthrough the following indicators:\n* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,\n* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and\n* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.\n\n== See also ==\n*[[List of academic databases and search engines]]\n*[[Science Citation Index]]\n*[[Scopus]]\n\n==External links==\n* [http://elibrary.ru/ Scientific Electronic Library]\n\n\n[[Category:Citation indices]]'
p270
sg6
S'Russian Science Citation Index'
p271
ssI101
(dp272
g2
S'http://en.wikipedia.org/wiki/Adversarial information retrieval'
p273
sg4
S'\'\'\'Adversarial information retrieval\'\'\' (\'\'\'adversarial IR\'\'\') is a topic in [[information retrieval]] related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.\n\nOn the Web, the predominant form of such manipulation is [[spamdexing|search engine spamming]] (also known as spamdexing), which involves employing various techniques to disrupt the activity of [[web search engines]], usually for financial gain. Examples of spamdexing are [[Google bomb|link-bombing]], [[comment spam (disambiguation)|comment]] or [[referrer spam]], [[spam blog]]s (splogs), malicious tagging.  [[Reverse engineering]] of [[ranking function|ranking algorithms]], [[Ad filtering|advertisement blocking]], and [[web content filtering]] may also be considered forms of adversarial [[data manipulation]].<ref>B. Davison, M. Najork, and T. Converse (2006), [http://wayback.archive.org/web/20090320173324/http://www.acm.org/sigs/sigir/forum/2006D/2006d_sigirforum_davison.pdf SIGIR Worksheet Report: Adversarial Information Retrieval on the Web (AIRWeb 2006)]</ref>\n\nActivities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.\n\n== Topics ==\nTopics related to Web spam (spamdexing):\n\n* [[Link spam]]\n* [[Keyword spamming]]\n* [[Cloaking]]\n* Malicious tagging\n* Spam related to blogs, including [[spam in blogs|comment spam]], [[spam blog|splogs]], and [[sping|ping spam]]\n\nOther topics:\n* [[Click fraud]] detection\n* Reverse engineering of  [[search engine]]\'s [[ranking]] algorithm\n* Web [[content filtering]]\n* [[Ad filtering|Advertisement blocking]]\n* Stealth [[web crawling|crawling]]\n*[[Troll (Internet)]]\n* Malicious tagging or voting in [[social networks]]\n* [[Astroturfing]]\n* [[Sockpuppetry]]\n\n== History ==\nThe term "adversarial information retrieval" was first coined in 2000 by [[Andrei Broder]] (then Chief Scientist at [[Alta Vista]]) during the Web plenary session at the [[Text Retrieval Conference|TREC]]-9 conference.<ref>D. Hawking and N. Craswell (2004), [http://es.csiro.au/pubs/trecbook_for_website.pdf Very Large Scale Retrieval and Web Search (Preprint version)]</ref>\n\n== See also ==\n*[[Spamdexing]]\n*[[Information retrieval]]\n\n== References ==\n{{reflist}}\n\n== External links ==\n*[http://airweb.cse.lehigh.edu/ AIRWeb]: series of workshops on Adversarial Information Retrieval on the Web\n*[http://webspam.lip6.fr/ Web Spam Challenge]: competition for researchers on Web Spam Detection\n*[http://wayback.archive.org/web/20100217125910/http://barcelona.research.yahoo.net/webspam/ Web Spam Datasets]: datasets for research on Web Spam Detection\n\n{{DEFAULTSORT:Adversarial Information Retrieval}}\n[[Category:Information retrieval]]\n[[Category:Internet fraud]]\n[[Category:Searching]]'
p274
sg6
S'Adversarial information retrieval'
p275
ssI230
(dp276
g2
S'http://en.wikipedia.org/wiki/Web of Science'
p277
sg4
S'{{Merge from|Web of Knowledge|date=December 2014|discuss=Talk:Web of Science#Merge}}\n{{Infobox Bibliographic Database\n|title = Web of Science\n|image = [[File:Web of science next generation.png|thumb|350px|Web of Science]]\n|caption = \n|producer = Thomson Reuters \n|country = United States \n|history = \n|languages = \n|providers = Various institutions and commercial organizations \n|cost = \n|disciplines = Science, social science, arts, humanities (supports 256 disciplines) \n|depth = citation indexing,  author, topic title, subject keywords, abstract,  periodical title, author\'s address, publication year \n|formats = full text articles, reviews, editorials, chronologies, abstracts, proceedings (journals and book-based ), technical papers \n|temporal = 1900 to present \n|geospatial = \n|number = 90 million + \n|updates = \n|p_title = \n|p_dates = \n|ISSN =\n|web =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science \n|titles =http://thomsonreuters.com/content/science/pdf/Web_of_Science_factsheet.pdf\n}}\n\'\'\'Web of Science\'\'\' (WoS, previously known as [[Web of Knowledge]]) is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] that provides a comprehensive citation search. It gives access to multiple databases that reference cross-disciplinary research, which allows for in-depth exploration of specialized sub-fields within an [[academic discipline|academic or scientific discipline]].<ref>Drake, Miriam A. Encyclopedia of Library and Information Science. New York, N.Y.: Marcel Dekker, 2004.</ref>\n\n==Background==\nA citation index is built on the fact that citations in science serve as linkages between similar research items, and lead to matching or related scientific literature, such as [[academic journal|journal articles]], [[conference proceedings]], abstracts, etc. In addition, literature which shows the greatest impact in a particular field, or more than one discipline, can be easily located through a citation index. For example, a paper\'s influence can be determined by linking to all the papers that have cited it. In this way, current trends, patterns, and emerging fields of research can be assessed. [[Eugene Garfield]], the "father of citation indexing of academic literature,"<ref>Jacso, Peter. The impact of Eugene Garfield through the prizm of Web of Science. Annals of Library and Information Studies, Vol. 57, September 2010, P. 222. [http://nopr.niscair.res.in/bitstream/123456789/10235/4/ALIS%2057%283%29%20222-247.pdf PDF]</ref> who launched the [[Science Citation Index]] (SCI), which in turn led to the Web of Science,<ref>Garfield, Eugene, Blaise Cronin, and Helen Barsky Atkins. The Web of Knowledge: A Festschrift in Honor of Eugene Garfield. Medford, N.J.: Information Today, 2000.</ref> wrote: \n\n{{Quote|Citations are the formal, explicit linkages between papers that have particular points in common. A citation index is built around these linkages. It lists publications that have been cited and identifies the sources of the citations. Anyone conducting a literature search can find from one to dozens of additional papers on a subject just by knowing one that has been cited. And every paper that is found provides a list of new citations with which to continue the search.\nThe simplicity of citation indexing is one of its main strengths. <ref>Garfield, Garfield, Eugene. Citation indexing: Its theory and application in science, technology, and humanities. New York: Wiley, 1979, P. 1. [http://garfield.library.upenn.edu/ci/chapter1.PDF PDF]</ref>}}\n\n==Coverage==\n[[File:Web_of_Science_Core_Collection.png|thumb|200px|Accessing the Web of Science via the [[Web of Knowledge]]]]\nExpanding the coverage of Web of Science, in November 2009 Thomson Reuters introduced \'\'Century of Social Sciences\'\'. This service contains files which trace social science research back to the beginning of the 20th century,<ref name=InfoToNov2009>"\'\'Thomson Reuters introduces century of social sciences\'\'". Information Today 26.10 (2009): 10. General OneFile. Web. 23 June 2010.  [http://find.galegroup.com/gps/infomark.do?&contentSet=IAC-Documents&type=retrieve&tabID=T003&prodId=IPS&docId=A211794482&source=gale&srcprod=ITOF&userGroupName=mlin_c_marlpl&version=1.0  Document URL].</ref><ref name=ComLibNov2009>Thomson Reuters introduces century of social sciences." Computers in Libraries 29.10 (2009): 47. General OneFile. Internet. 23 June 2010. [http://find.galegroup.com/gps/infomark.do?&contentSet=IAC-Documents&type=retrieve&tabID=T003&prodId=IPS&docId=A211236981&source=gale&srcprod=ITOF&userGroupName=mlin_c_marlpl&version=1.0 Document URL]</ref> and Web of Science now has indexing coverage from the year 1900 to the present.<ref name=oview>\n{{Cite web |title =Overview - Web of Science| publisher =Thomson Reuters| year = 2010\n  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science\n  | format =Overview of coverage gleaned from promotional language.  \n  | accessdate =2010-06-23}}</ref><ref name=UoOL>\n{{Cite web| last = Lee| first =Sul H.| title =Citation Indexing and ISI\'s Web of Science \n  | publisher =The University of Oklahoma Libraries| year =2010\n  | url =http://www.ou.edu/webhelp/librarydemos/isi/ | format =Discussion of finding literature manually. Description of [[citation index]]ing, and Web of Science.| accessdate =2010-06-23}}</ref> The multidisciplinary coverage of the Web of Science encompasses over 50,000 scholarly books, 12,000 journals and 160,000 conference proceedings<ref name="Web of Science">[http://wokinfo.com/citationconnection/realfacts/#regional Web of Science. Thomson Reuters, 2014]</ref> (as of September 3, 2014). The selection is made on the basis of [[impact factor|impact evaluations]] and comprise [[open-access journal]]s, spanning multiple [[academic discipline]]s. The coverage includes: the [[science]]s, [[social science]]s, [[the arts|arts]], and humanities, and goes across disciplines.<ref name=oview/><ref name=facts/> However, Web of Science does not index all journals, and its coverage in some fields is less complete than in others.\n\nFurthermore, as of September 3, 2014 the total file count of the Web of Science was 90 million records, which included over a billion cited references. This citation service on average indexes around 65 million items per year, and it is described as the largest accessible citation database.<ref name=facts>[http://wokinfo.com/citationconnection/  Bulleted fact sheet]. Thomson Reuters. 2014.</ref>\n\nTitles of foreign-language publications are translated into English and so cannot be found by searches in the original language.<ref name=harvard-search>{{Cite web\n  |title =Some Searching Conventions\n  | publisher =President and Fellows of Harvard College   | date = December 3, 2009   | url =http://hcl.harvard.edu/research/guides/citationindex/#some   | format =    | accessdate =2010-06-23}}</ref>\n\n==Citation databases==\nWeb of Science consist of seven online databases:<ref name=included/><ref name="Web of Science">[http://wokinfo.com/media/pdf/WoSFS_08_7050.pdf Jo Yong-Hak. Web of Science. Thomson Reuters, 2013]</ref>\n*[[Conference Proceedings Citation Index]] covers more than 160,000 conference titles in the Sciences starting from 1990 to the present day\n*[[Science Citation Index Expanded]] covers more than 8,500 notable journals encompassing 150 disciplines. Coverage is from the year 1900 to the present day.\n*[[Social Sciences Citation Index]] covers more than 3,000 journals in social science disciplines.  Range of coverage is from the year 1900 to the present day.\n*[[Arts & Humanities Citation Index]] covers more than  1,700 arts and humanities journals starting from 1975. In addition, 250 major scientific and social sciences journals are also covered. \n*[[Index Chemicus]] lists more than 2.6 million compounds. The time of coverage is from 1993 to present day.\n*[[Current Chemical Reactions]] indexes over one million reactions, and the range of coverage is from 1986 to present day. The \'\' INPI \'\' archives from 1840 to 1985 are also indexed in this database.\n*[[Book Citation Index]] covers more than 60,000 editorially selected books starting from 2005.\n\n=== Contents ===\nThe seven [[citation index|citation indices]] listed above contain references which have been cited by other articles. One may use them to undertake cited reference search, that is, locating articles that cite an earlier, or current publication. One may search citation databases by topic, by author, by source title, and by location. Two chemistry databases,  \'\'Index Chemicus\'\' and  \'\'Current Chemical Reactions\'\' allow for the creation of structure drawings, thus enabling users to locate [[chemical compound]]s and reactions. Institutions such as universities and research departments generally access the Web of Science through the [[Web of Knowledge]] platform. (An example of a typical search.<ref>[http://cires.colorado.edu/~jjose/P-Cited/DeCarlo06_ISI.pdf A typical Web of Science search example.]</ref>)\n\n===Abstracting and indexing===\nThe following  types of literature are indexed: scholarly books, [[peer review]]ed journals, original research articles, reviews, editorials, chronologies, abstracts, as well as other items. Disciplines included in this index are  [[agriculture]], [[biological sciences]], [[engineering]], medical and [[life sciences]], [[physics|physical]] and [[chemical sciences]], [[anthropology]], law, [[library science]]s, [[architecture]], dance, music, film, and theater. Seven citation databases encompasses coverage of the above disciplines.<ref name=UoOL/><ref name=included>\n{{Cite web |title =Coverage - Web of Science| publisher =Thomson Reuters| year = 2010\n  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/web_of_science\n  | format =Overview of coverage gleaned from promotional language.  \n  | accessdate =2010-06-23}}</ref><ref name="Web of Science" />\n\n==Limitations in the use of citation analysis==\nAs with other scientific approaches, scientometrics and bibliometrics have their own limitations. Recently, a criticism was voiced pointing toward certain deficiencies of the journal impact factor (JIF) calculation process, based on Thomson Reuters Web of Science, such as: journal citation distributions usually are highly skewed towards established journals; journal impact factor properties are field-specific and can be easily manipulated by editors, or even by changing the editorial policies; this makes the entire process essentially nontransparent.<ref name="Declaration">[http://am.ascb.org/dora/ San Francisco Declaration on Research Assessment: Putting science into the assessment of research, December 16, 2012]</ref>\n\nRegarding the more objective journal metrics, there is a growing view that for greater accuracy it must be supplemented with an article-based assessment and peer-review.<ref name="Declaration" /> Thomson Reuters replied to criticism in general terms by stating that "no one metric can fully capture the complex contributions scholars make to their disciplines, and many forms of scholarly achievement should be considered."<ref>Thomson Reuters Statement Regarding the San Francisco Declaration on Research Assessment [http://researchanalytics.thomsonreuters.com/]</ref>\n\n== See also ==\n\n{{Div col|3}}\n*[[List of academic journal search engines]]\n*[[CSA (database company)|CSA databases]]\n*[[Dialog (online database)]]\n*[[Energy Citations Database]]\n*[[Energy Science and Technology Database]]\n*[[ETDEWEB]]\n*[[Geographic Names Information System]]\n*[[Materials Science Citation Index]]\n*[[PASCAL (database)|PASCAL database]]\n* [[PubMed Central]]\n* [[SciELO]]\n* [[VINITI Database RAS]]\n* [[Web development tools]]\n{{Div col end}}\n\n== References ==\n{{Reflist}}\n\n==External links==\n* [http://scientific.thomson.com/products/wos/ Web of Science]\n* [http://www.webofknowledge.com/ Web of Knowledge]\n* [http://web.archive.org/web/20110521161422/http://hcl.harvard.edu/research/guides/citationindex/ Searching the Citation Indexes (Web of Science)] Harvard College Library. 2010. (archive)\n* [http://video.mit.edu/watch/web-of-science-12339/ MIT Web of Science video tutorial]. 2008.\n\n{{Thomson Reuters}}\n{{DEFAULTSORT:Web Of Science}}\n[[Category:Bibliographic databases]]\n[[Category:Full text scholarly online databases]]\n[[Category:Thomson family]]\n[[Category:Thomson Reuters]]\n[[Category:Citation indices]]\n[[Category:Scholarly search services]]'
p278
sg6
S'Web of Science'
p279
ssI104
(dp280
g2
S'http://en.wikipedia.org/wiki/Extended Boolean model'
p281
sg4
VThe '''Extended Boolean model''' was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in [[information retrieval]]. The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the [[Vector Space Model]] with the properties of [[Boolean algebra (logic)|Boolean algebra]] and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the [[Standard Boolean model]] it wasn't.<ref>	\u000a{{citation | url=http://portal.acm.org/citation.cfm?id=358466 | last=Salton | first=Gerard | coauthors=Edward A. Fox, Harry Wu | title=Extended Boolean information retrieval | publisher=Communications of the ACM, Volume 26,  Issue 11 | year=1983 }}</ref>\u000a\u000aThus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that [[relevance feedback]] and [[query expansion]] can be integrated with extended Boolean query processing.\u000a\u000a==Definitions==\u000aIn the '''Extended Boolean model''', a document is represented as a vector (similarly to in the vector model). Each ''i'' [[Dimension (vector space)|dimension]] corresponds to a separate term associated with the document.\u000a\u000aThe weight of term {{math|''K<sub>x</sub>''}} associated with document {{math|''d<sub>j</sub>''}} is measured by its normalized [[Term frequency]] and can be defined as:\u000a\u000a<math>\u000aw_{x,j}=f_{x,j}*\u005cfrac{Idf_{x}}{max_{i}Idf_{i}}\u000a</math>\u000a\u000awhere {{math|''Idf<sub>x</sub>''}} is [[inverse document frequency]].\u000a\u000aThe weight vector associated with document {{math|''d<sub>j</sub>''}} can be represented as:\u000a\u000a<math>\u005cmathbf{v}_{d_j} = [w_{1,j}, w_{2,j}, \u005cldots, w_{i,j}]</math>\u000a\u000a==The 2 Dimensions Example==\u000a{{multiple image\u000a | width     = 150\u000a | image1    = 2D_Extended_Boolean_model_OR_example.png\u000a | alt1      = Figure 1\u000a | caption1  = '''Figure 1:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.\u000a | image2    = 2D_Extended_Boolean_model_AND_example.png\u000a | alt2      = Figure 2\u000a | caption2  = '''Figure 2:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.\u000a}}\u000a\u000aConsidering the space composed of two terms {{math|''K<sub>x</sub>''}} and {{math|''K<sub>y</sub>''}} only, the corresponding term weights are {{math|''w''<sub>1</sub>}} and {{math|''w''<sub>2</sub>}}.<ref>[http://www.cs.cityu.edu.hk/~cs5286/Lectures/Lwang.ppt Lusheng Wang]</ref>  Thus, for query {{math|''q<sub>or</sub>'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}}, we can calculate the similarity with the following formula:\u000a \u000a<math>sim(q_{or},d)=\u005csqrt{\u005cfrac{w_1^2+w_2^2}{2}}</math>\u000a\u000aFor query {{math|''q<sub>and</sub>'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}}, we can use:\u000a\u000a<math>sim(q_{and},d)=1-\u005csqrt{\u005cfrac{(1-w_1)^2+(1-w_2)^2}{2}}</math>\u000a\u000a==Generalizing the idea and P-norms==\u000aWe can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances.\u000a\u000aThis can be done using [[P-norm]]s which extends the notion of distance to include p-distances, where {{math|1 &le; ''p'' &le; &infin;}} is a new parameter.<ref>{{ citation | last=Garcia | first= Dr. E. | url=http://www.miislita.com/term-vector/term-vector-6-boolean-model.html | title=The Extended Boolean Model - Weighted Queries: Term Weights, p-Norm Queries and Multiconcept Types. Boolean OR Extended? AND that is the Query }}</ref>\u000a\u000a*A generalized conjunctive query is given by:\u000a:<math>q_{or}=k_1 \u005clor^p k_2 \u005clor^p .... \u005clor^p k_t  </math>\u000a\u000a*The similarity of <math>q_{or}</math> and <math>d_j</math> can be defined as:\u000a''':<math>sim(q_{or},d_j)=\u005csqrt[p]{\u005cfrac{w_1^p+w_2^p+....+w_t^p}{t}}</math>'''\u000a\u000a*A generalized disjunctive query is given by:\u000a:<math>q_{and}=k_1 \u005cland^p k_2 \u005cland^p .... \u005cland^p k_t  </math>\u000a\u000a*The similarity of <math>q_{and}</math> and <math>d_j</math> can be defined as:\u000a:<math>sim(q_{and},d_j)=1-\u005csqrt[p]{\u005cfrac{(1-w_1)^p+(1-w_2)^p+....+(1-w_t)^p}{t}}</math>\u000a\u000a==Examples==\u000aConsider the query {{math|''q'' {{=}} (''K''<sub>1</sub> &and; ''K''<sub>2</sub>) &or; ''K''<sub>3</sub>}}. The similarity between query {{math|''q''}} and document {{math|''d''}} can be computed using the formula:\u000a\u000a<math>sim(q,d)=\u005csqrt[p]{\u005cfrac{(1-\u005csqrt[p]{(\u005cfrac{(1-w_1)^p+(1-w_2)^p}{2}}))^p+w_3^p}{2}}</math>\u000a\u000a==Improvements over the Standard Boolean Model==\u000a\u000aLee and Fox<ref>{{citation | last=Lee | first=W. C. | coauthors=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}</ref> compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.\u000aUsing P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively.<br>\u000aThe P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even [[Fuzzy retrieval]] techniques. The [[Standard Boolean model]] is still the most efficient.\u000a\u000a==Further reading==\u000a* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.1997 Adaptive Feedback Methods in an Extended Boolean Model  by Dr.Jongpill Choi]\u000a* [http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VC8-454T5MS-2&_user=513551&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1117914301&_rerunOrigin=google&_acct=C000025338&_version=1&_urlVersion=0&_userid=513551&md5=4eab0da46bfe361afa883e48f2060feb Interpolation of the extended Boolean retrieval model ]\u000a* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last=Fox | first=E. | coauthors=S. Betrabet , M. Koushik , W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}\u000a* {{citation | title=Experiments with Automatic Query Formulation in the Extended Boolean Model | url=http://www.springerlink.com/content/tk1t141253257613/ | first= Lucie | last= Skorkovská | coauthors=Pavel Ircing | year=2009 | publisher= Springer Berlin / Heidelberg}}\u000a\u000a==See also==\u000a*[[Information retrieval]]\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{DEFAULTSORT:Extended Boolean Model}}\u000a[[Category:Information retrieval]]
p282
sg6
S'Extended Boolean model'
p283
ssI233
(dp284
g2
S'http://en.wikipedia.org/wiki/Tunebot'
p285
sg4
S"'''Tunebot''' is a music search engine developed by the Interactive Audio Lab at [[Northwestern University]]. Users can search the database by humming or singing a melody into a microphone, playing the melody on a virtual keyboard, or by typing some of the lyrics. This allows users to finally identify that song that was stuck in their head.\n\n==Searching Techniques==\n\nTunebot is a [[Query by humming]] system. It compares a sung query to a database of musical themes by using the intervals between each note. This allows a user to sing in a different key than the target recording and still produce a match. The intervals are also unquantized to allow for other tunings besides the standard A=440Hz, since not many people in the world have [[perfect pitch]].\n\nIn addition to note intervals, Tunebot compares a query with potential targets by using rhythmic ratios between notes. Since ratios between note lengths are used, the tempo of the performance does not affect the rhythmic similarity measure. \n\nQueries and targets are then matched by a weighted string alignment algorithm between the note intervals and rhythmic ratios.\n<!--Note segmentation, then to pitches and then use Pitch intervals (instead of melodic contour - measured frequency at given times). Pitch intervals are relative (unquantized) to adjust for singing in the wrong key or wrong tempo. Faster and more reliable search.\n\nModel singer error: gaussian distribution because wider interval and lower intervals seem to be more prone to singer error. Combination of gaussians with 5 parameters to tweak: pitch weight, rhythm weight, sensitivity to distance for pitch, sensisitivity to distance for rhythm, octave decay\n\nDo we use rhythmic ratios? (LIR)\n\nGenetic algorithm to tune system parameters-->\n\n==The Database==\nThe database consists of unaccompanied melodies sung by contributors (a capella). Contributors log into the website and sing their examples to the system. Each of these recordings is associated with a corresponding song on [[Amazon.com|Amazon]]. A sung query is compared to these examples. A capella sung examples are used as search keys because it is much easier to compare one unaccompanied vocal (the sung query) to another (an example search key) than it is to compare an unaccompanied vocal to a full band recording, which may contain guitar, drums, other singers, sound effects, etc.\n\n==Distinguishing Features==\n\nTunebot learns from user input, and it improve its results as each user submits more queries. Since no human can sing perfectly in tune every time they sing, the search engine must take that into account. By choosing a song from a list of ranked results, users tell Tunebot which song was correct. Tunebot then pairs that song with the user's query, analyzes the differences, and runs a [[Genetic Algorithm]]. This process tweaks the parameters that control how the system compares the user's query to the targets. For instance, if a user has no sense of rhythm, that factor of the comparison is lowered for future queries.\n\n==References==\n\n* B. Pardo. [http://127.0.0.1/publications/pardo-IEEE-signal-processing-mag-06.pdf Finding Structure in Audio for Music Information Retrieval.] IEEE Signal Processing Magazine. vol. 49 (8), pp. 49-52, 2006\n* D. Little, D. Raffensperger, B. Pardo. [http://music.cs.northwestern.edu/files/ISMIR%202007%20v2.pdf A Query by Humming System that Learns from Experience.] Proceedings of the 8th International Conference on Music Information Retrieval, Vienna, Austria, September 23-27, 2007.\n* D. Little, D. Raffensperger and B. Pardo.[http://www.eecs.northwestern.edu/docs/techreports/2007_TR/NWU-EECS-07-03.pdf Online Training of a Music Search Engine.] Northwestern University, Evanston, IL, NWU-EECS-07-03, 2007\n\n==External links==\n*[http://tunebot.cs.northwestern.edu Tunebot @ Northwestern]\n\n\n[[Category:Music search engines]]\n[[Category:Acoustic fingerprinting]]"
p286
sg6
S'Tunebot'
p287
ssI107
(dp288
g2
S'http://en.wikipedia.org/wiki/Keyword optimization'
p289
sg4
S'#REDIRECT [[Search engine optimization]]\n\n{{DEFAULTSORT:Keyword Optimization}}\n[[Category:Information retrieval]]\n[[Category:Internet marketing]]'
p290
sg6
S'Keyword optimization'
p291
ssI236
(dp292
g2
S'http://en.wikipedia.org/wiki/Shazam (service)'
p293
sg4
V{{EngvarB|date=February 2014}}\u000a{{Use dmy dates|date=February 2014}}\u000a{{Infobox company\u000a| name             = Shazam Entertainment Ltd.\u000a| logo             = [[File:Shazam logo.png|160px]]\u000a| caption          = \u000a| type             = \u000a| traded_as        = \u000a| genre            = <!-- Only used with media and publishing companies -->\u000a| fate             = \u000a| predecessor      = \u000a| successor        = \u000a| foundation       = United States ({{Start date|1999}})\u000a| founder          = {{unbulleted list|Chris Barton|Philip Inghelbrecht|Dhiraj Mukherjee|Avery Wang}}\u000a| defunct          = <!-- {{End date|df=yes|YYYY|MM|DD}} -->\u000a| location_city    = London\u000a| location_country = United Kingdom\u000a| location         = \u000a| locations        = 7 offices (2014)\u000a| area_served      = Worldwide\u000a| key_people       = {{unbulleted list|Rich Riley (CEO)|Andrew Fisher (Executive chairman)}}\u000a| industry         = \u000a| products         = [[Application software|Apps]]\u000a| services         = \u000a| revenue          = \u000a| operating_income = \u000a| net_income       = \u000a| aum              = <!-- Only used with financial services companies -->\u000a| assets           = \u000a| equity           = \u000a| num_employees    = \u000a| divisions        = \u000a| subsid           = \u000a| homepage         = {{URL|//www.shazam.com/}} \u000a| footnotes        = \u000a| intl             = \u000a}}\u000a'''Shazam''' is a British app for smartphones, PCs<ref>{{Cite web|url = http://apps.microsoft.com/windows/en-au/app/shazam/5593d150-02c7-4714-ab8f-007d5d251688|title = Shazam|date = |accessdate = 7 January 2015|website = Shazam app for Windows in the Windows Store|publisher = Microsoft Corporation|last = |first = }}</ref> and Macs, which is best known for its music identification capabilities. Shazam Entertainment Limited was founded in 1999 by Chris Barton, Philip Inghelbrecht, Avery Wang and Dhiraj Mukherjee.<ref name="DirectorDec2009">{{cite news | url=http://www.director.co.uk/magazine/2009/11%20December/shazam_63_04.html | title=Shazam names that tune | date=December 2009 | accessdate=26 September 2012 | last=Woodward | first=David | newspaper=Director}}</ref> The company is best known for its music identification technology, but has expanded to integrations with cinema, advertising, TV and retail environments.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6207061/shazam-launches-resonate-tv-sales-platform</ref>\u000a\u000aShazam uses a smartphone or Mac's built-in microphone to gather a brief sample of audio being played.  It creates an [[acoustic fingerprint]] based on the sample, and compares it against a central database for a match.  If it finds a match, it sends information such as the artist, song title, and album back to the user. Some implementations of Shazam incorporate relevant links to services such as [[iTunes]], [[YouTube]], [[Spotify]] or [[Zune]]. In December of 2013, Shazam was one of the top ten apps in the world, according to its CEO.<ref>http://video.cnbc.com/gallery/?video=3000222563#.</ref> The Shazam app has more than 100 million monthly active users and has been used on more than 500 million mobile devices.<ref>http://thenextweb.com/insider/2014/08/20/shazam-now-100-million-monthly-active-users-mobile/</ref> In October of 2014, Shazam announced its technology has been used to identify 15 billion songs.<ref>http://www.siliconrepublic.com/digital-life/item/38714-15-billion-songs-have-been</ref>\u000a==Features==\u000aShazam offers two types of applications; a free app simply called Shazam and a paid app called Shazam Encore. The service was expanded in September 2012 to enable TV users in the US to identify featured music, access cast information and get links to show information online, as well as adding social networking capabilities.<ref name="TV tags"/>\u000a\u000aIn February of 2014, Shazam announced a redesign of the app, which included a new look and additional features, including lyric-viewing options, access to music videos and related videos, unique recommendations, improved biographies and discographies and additional functionality for use with TV shows. The update also featured a News Feed, and Auto-Shazam, a feature introduced in December of 2013, which runs in the background of users\u2019 mobile devices to automatically identify media.<ref>http://www.digitaltveurope.net/151662/shazam-unveils-app-redesign/</ref>  \u000a\u000aIn July of 2014, Shazam announced the launch of Shazam for Mac, a desktop version of the app, which when enabled, runs in the background and automatically recognizes any song played on or near the computer, including songs playing in the background of TV shows or YouTube videos.<ref>http://mashable.com/2014/07/31/shazam-mac-app/</ref> Apple\u2019s launch of iOS 8 in September of 2014 came with the seamless integration of Shazam into Apple\u2019s intelligent personal assistant Siri function.<ref>http://www.jbgnews.com/2014/09/shazam-partners-with-apple-to-bring-music-recognition-to-siri/504608.html</ref> \u000a\u000a==Devices==\u000aShazam is a free or low-cost application that runs on [[Android (operating system)|Android]], [[Apple Inc.|Apple]] [[iPhone]] iOS, [[BlackBerry]] OS, and [[Windows Phone|Windows]] systems. The application is similar on most phones and the result is shown on the screen complete with details on Artist, Album, Title, Genre, Music label, lyrics, a thumbnail image of the song/album artwork, links to download the song on [[iTunes]] or the [[Amazon MP3]] store and, where relevant, show the song's video on YouTube and give the option of playing the song on [[Rdio]]. Shazam is also available for Mac, as a desktop application.<ref>http://mashable.com/2014/07/31/shazam-mac-app/ </ref> \u000a\u000a== Function ==\u000aShazam works by analyzing the captured sound and seeking a match based on an [[acoustic fingerprint]] in a database of more than 11 million songs.<ref>[//www.shazam.com/music/web/about.html Shazam \u2013 About Shazam<!-- Bot generated title -->]</ref>\u000a\u000a[[File:Spectrogram of violin.png|thumb|A spectrogram of the sound of a violin.]]\u000a[[File:Target zone2.png|thumb|The target zone of a song scanned by Shazam.{{clarify|date=September 2012}}]]\u000aShazam identifies songs based on an audio fingerprint based on a time-frequency graph called a [[spectrogram]].\u000a\u000aShazam stores a catalogue of audio fingerprints in a database. The user tags a song for 10 seconds and the application creates an audio fingerprint.\u000a\u000aOnce it creates the fingerprint of the audio, Shazam starts the search for matches in the database. If there is a match, it returns the information to the user; otherwise it returns a "song not known" dialogue.<ref>[http://soyoucode.com/2011/how-does-shazam-recognize-song How does Shazam work to recognize a song ? | So, you code ?<!-- Bot generated title -->]</ref>\u000a\u000aShazam can identify prerecorded music being broadcast from any source, such as a radio, television, cinema or music in a club, provided that the background noise level is not high enough to prevent an acoustic fingerprint being taken, and that the song is present in the software's database.\u000a\u000a==History==\u000aThe company was founded in 1999 by Barton and Inghelbrecht, who were students at [[University of California, Berkeley]], and Mukherjee, who worked at a London-based internet consulting firm called Viant.{{Citation needed|date=September 2013}} In need of a digital signal processing specialist, the founding team then hired Wang, who was a PhD student from [[Stanford University]]. {{as of|September 2012}}, Wang is the only member of the original team to remain in the company,<ref name="DirectorDec2009" /> and serves as Shazam's Chief Scientist.<ref name="Shazam Team">{{cite web|title=About Shazam \u2013 Team|url=//www.shazam.com/music/web/team.html|accessdate=27 September 2012}}</ref>\u000a\u000a[[Rich Riley]] joined Shazam as CEO in April 2013 to increase the company\u2019s growth,<ref>http://www.huffingtonpost.co.uk/2013/08/30/shazam-rich-riley_n_3762179.html</ref> after over 13 years at Yahoo!<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/1560025/shazam-names-rich-riley-new-ceo-aiming-for-eventual-ipo </ref> and with more than 17 years of experience as an entrepreneur and leading Internet executive.<ref>http://www.crunchbase.com/person/rich-riley </ref> "I look forward to extending our dominance in media engagement, from our roots in music to our leadership position in second-screen TV and want to ensure that Shazam is the company that helps people recognize and engage with the world around them,\u201d Riley said in a statement at the time.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/1560025/shazam-names-rich-riley-new-ceo-aiming-for-eventual-ipo </ref> Riley replaced Andrew Fisher, who was hired from [[Infospace]] into the CEO role in 2005 to strengthen industry partnerships and grow the userbase.<ref name=DirectorDec2009 /> Fisher is now executive chairman.\u000a\u000a===Partnerships===\u000aThe first partnership was with Entertainment UK, part of Woolworths, whom they approached to digitise their music catalogue of 1.5 million songs in return for permission to create a proprietary database. As the service grew to have a worldwide userbase, it needed to keep its database up-to-date, which it does by having relationships with labels globally.<ref name="DirectorDec2009" /> By December 2008, the database had grown to 8 million songs.<ref>{{cite news|last=Reisinger|first=Don|title=Shazam adds 2 million tracks to music library|url=http://news.cnet.com/8301-17939_109-10113274-2.html|accessdate=29 September 2012|newspaper=CNET|date=4 December 2008}}</ref>\u000a\u000aIn February 2013, Shazam announced a partnership with the music store [[Beatport]], adding its library of [[electronic music]] to the service.<ref name=bb-shazambeatport>{{cite web|title=Beatport's Matthew Adell on Shazam Deal, Why Music Biz Is a 'Disaster Model'|url=http://www.billboard.com/biz/articles/news/digital-and-mobile/1538517/beatports-matthew-adell-on-shazam-deal-why-music-biz-is|work=Billboard.biz|accessdate=21 September 2013}}</ref> On 3 April 2013, Shazam announced an exclusive partnership with [[Saavn]], an Indian online music streaming service. The deal will add nearly 1 million songs in [[Languages of India|Indian languages]] to Shazam's database.<ref>[http://www.financialmirror.com/newsml_story.php?id=5458 Shazam Forms Exclusive New Partnership with Saavn for the Best Indian Music Discovery Experience<!-- Bot generated title -->]</ref><ref>{{cite news| url=http://blogs.wsj.com/speakeasy/2013/04/03/shazam-broadens-its-horizons/ | work=The Wall Street Journal | title=Shazam Broadens Its Horizons \u2013 Speakeasy \u2013 WSJ}}</ref><ref>[http://techcrunch.com/2013/04/03/shazam-partners-with-the-spotify-of-india-saavn-to-improve-its-south-asian-music-recognition/ Shazam Partners With The \u2018Spotify Of India\u2019, Saavn, To Improve Its South Asian Music Recognition | TechCrunch<!-- Bot generated title -->]</ref><ref>[http://www.medianama.com/2013/04/223-shazam-saavn-tieup/ Updated: Shazam Ties Up With Saavn To Identify Hindi & Regional Music; Implications \u2013 MediaNama<!-- Bot generated title -->]</ref> In July 2014, Shazam announced a partnership with Rdio that allows Shazam users to stream full songs within the app.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6157583/shazam-partners-with-rdio-to-stream-full-songs-inside </ref>\u000a\u000aIn addition to music, Shazam has announced collaborations with partners across television, advertising and cinema. In May of 2014, NCM Media Networks announced a partnership with Shazam to incorporate Shazam into FirstLook pre-show segments that run in Regal, AMC and Cinemark theaters.<ref>http://techcrunch.com/2014/05/14/shazam-partners-with-ncm/ </ref> In November of 2014, NCM and Shazam announced that NCM FirstLook pre-shows are now Shazam enabled on over 20,000 movie screens across the United States.<ref>http://mashable.com/2014/11/07/shazam-firstlook/ </ref>\u000a\u000aIn August of 2014, Shazam announced the launch of Resonate, a sales product that allows TV networks to access its technology and user base. The news included the announcement of partnerships with AMC, A+E, dick clark productions and FUSE.<ref>http://www.billboard.com/biz/articles/news/digital-and-mobile/6207061/shazam-launches-resonate-tv-sales-platform </ref>\u000a\u000aShazam recently announced a partnership with Sun Broadcast Group on Shazam for Radio, a new offering that will allow radio stations to push customized content to listeners on Sun Broadcast\u2019s over 8,000 radio stations in the U.S.<ref>http://thenextweb.com/insider/2014/10/09/shazam-makes-big-move-interactive-radio-content/ </ref>\u000a\u000a===Early days of the service===\u000aInitially, in 2002, the service was launched only in the UK and was known as "2580", as the number was the [[shortcode]] that customers dialled from their mobile phone to get music recognised.<ref name=DirectorDec2009 /> The phone would automatically hang up after 30 seconds. A result was then sent to the user in the form of a text message containing the song title and artist name. At a later date, the service also began to add hyperlinks in the text message to allow the user to download the song online.<ref name=CNETUKApril06>{{cite news|last=Lim|first=Andrew|title=Shazam & AQA: The answer is on your mobile|url=http://crave.cnet.co.uk/mobiles/shazam-and-aqa-the-answer-is-on-your-mobile-49264359/|accessdate=29 September 2012|newspaper=CNET UK|date=24 April 2006}}</ref>\u000a\u000aShazam launched in the US on the AT&T Wireless network in 2004 in a joint offering with Musicphone, a now defunct San Francisco-based company. The service was free at launch with AT&T saying that it would charge USD0.99 for each use in future.<ref name="cnet040415">{{cite news | url=http://news.cnet.com/Dial-that-tune-comes-to-U.S./2110-1039_3-5192105.html | title=Dial-that-tune comes to U.S. | work=CNET | date=15 April 2004 | accessdate=29 September 2012 | author=Charny, Ben}}</ref>\u000a\u000aIn 2006, users were charged £0.60 per call or had unlimited use for £4.50 a month, as well as an online service to keep track of all tags.<ref name=CNETUKApril06/>\u000a\u000a===Smartphone app===\u000aShazam for iPhone 2.0 debuted on 10 July 2008, with the launch of Apple's App Store. The free app simplified the service by enabling the user to launch iTunes and buy the song directly if the user was on a Wi-Fi connection <ref name=CNET080710>{{cite news|last=Rosoff|first=Matt|title=Shazam on iPhone could change music discovery|url=http://news.cnet.com/8301-13526_3-9988219-27.html|accessdate=29 September 2012|newspaper=CNET|date=10 July 2008}}</ref> (at the time, iTunes did not allow music downloads over 3G). It was also possible to launch the iPhone YouTube app, if a video was available.<ref name=CNET080716>{{cite news|last=Dolcourt|first=Jessica|title=First Look video: Shazam for iPhone|url=http://download.cnet.com/8301-2007_4-9992639-12.html|accessdate=29 September 2012|newspaper=CNET|date=16 July 2008}}</ref>\u000a\u000aIn 2008, the service struggled to identify classical music.<ref>{{cite news|last=Ho|first=Kevin|title=iPhone apps: Testing Shazam's limits \u2013 classical music|url=http://news.cnet.com/8301-13544_3-9993320-35.html|accessdate=29 September 2012|newspaper=CNET|date=17 July 2008}}</ref>\u000a\u000aShazam launched on the [[Android operating system|Android platform]] in October 2008. The Android app connected to [[Amazon Appstore|Amazon's MP3 store]] instead of iTunes.<ref name=AndroidLaunch>{{cite news|last=Reisinger|first=Don|title=Shazam moves to Android, works with Amazon MP3 Store|url=http://news.cnet.com/8301-17939_109-10071167-2.html|accessdate=29 September 2012|newspaper=CNET|date=21 October 2008}}</ref>\u000a\u000aAlongside the iOS 3 update in July 2009, Shazam updated its app to include a number of new features: marking the tag with GPS coordinates; sending tags to others as 'postcards', enabling them to buy the song; and Twitter integration.<ref>{{cite news|last=Lee|first=Nicola|title=Latest Shazam lets you track musical journey in iPhone OS 3.0|url=http://download.cnet.com/8301-2007_4-10267205-12.html|accessdate=29 September 2012|newspaper=CNET|date=17 June 2009}}</ref>\u000a\u000aThe app launched on the [[Windows Marketplace for Mobile|Windows Mobile Marketplace]] in October 2009 as a [[freemium]] offering, with the first release of Shazam Encore. The free version was now limited to five tags per month: users typically tagged ten songs per month. Encore, priced at USD4.69, added several features such as song popularity charts and recommendations.<ref name=CNETWindowsLaunch>{{cite news|last=Dolcourt|first=Jessica|title=Shazam debuts in Windows Marketplace for Mobile|url=http://reviews.cnet.com/8301-12261_7-10368986-10356022.html|accessdate=30 September 2012|newspaper=CNET|date=7 October 2009}}</ref> Encore first appeared for iPhone in November 2009.<ref>{{cite news|last=Dolcourt|first=Jessica|title=Shazam iPhone app gets premium Encore|url=http://download.cnet.com/8301-2007_4-10393035-12.html|accessdate=30 September 2012|newspaper=CNET|date=9 November 2009}}</ref>\u000a\u000aBy December 2009, Shazam was downloaded 10 million times in 150 countries across 350 mobile operators. Around eight percent of users purchased a track after it was identified by the service.<ref name=DirectorDec2009 /> Its success led to a funding round from [[Kleiner Perkins Caufield & Byers]] in October 2009.<ref name=DirectorDec2009 /><ref>{{cite news|last=Saint|first=Nick|title=Shazam Draws Investment, Is Already Profitable|url=http://www.businessinsider.com/shazam-draws-investment-is-already-profitable-2009-10|accessdate=30 September 2012|newspaper=Business Insider|date=15 October 2009}}</ref> In January 2011, Apple announced that Shazam was the fourth most downloaded free app of all time on the App Store, while rival [[SoundHound]] had the top paid iPad app.<ref>{{cite news|last=Reisinger|first=Don|title=Apple reveals top apps of all time|url=http://news.cnet.com/8301-13506_3-20028889-17.html|accessdate=30 September 2012|newspaper=CNET|date=19 January 2011}}</ref>\u000a\u000aEarly adopters of the free application are still allowed unlimited tagging.<ref>[http://androidforums.com/android-applications/132182-shazam-how-preserve-unlimited-tagging-feature-after-reflash-root.html#post1488306 Shazam: How to preserve the "unlimited tagging" feature after REFLASH and Root? \u2013 Android Forums<!-- Bot generated title -->]</ref>\u000a\u000a[[GetJar]], an app store for Android, Blackberry and Symbian, added Shazam in November 2010.<ref>{{cite news|last=Reisinger|first=Don|title=AT&T ladles out GetJar apps \u2013 iPhone excluded|url=http://news.cnet.com/8301-13506_3-20022340-17.html|accessdate=30 September 2012|newspaper=CNET|date=10 November 2010}}</ref>\u000a\u000aIn January 2011, Shazam and [[Spotify]] announced a partnership for iOS and Android to help users identify music with Shazam and listen to tracks through Spotify.<ref>{{cite news|last=Morris|first=Natali|title=Space love|url=http://cnettv.cnet.com/8301-13991_53-20028388-10391624.html|accessdate=30 September 2012|newspaper=CNET|date=13 January 2011}}</ref>\u000a\u000aWhile Shazam already had Facebook and Twitter share buttons, deeper Facebook integration was released in March 2011. With Shazam Friends users can see what their Facebook friends have tagged, listen to the tracks and buy them.<ref>{{cite news|last=McCarthy|first=Caroline|title=Music app Shazam gets new Facebook features|url=http://news.cnet.com/8301-13577_3-20045965-36.html|accessdate=1 October 2012|newspaper=CNET|date=22 March 2011}}</ref>\u000a\u000aWith Shazam 5.0, released in April 2012, the app begins 'listening' as soon as it is launched and can take as little as one second to identify media. In addition to music, the app can identify TV programs and ads, if they are Shazam-enabled.<ref>{{cite news|last=Parker|first=Jason|title=Shazam for iOS adds TV to its list of media it can identify|url=http://reviews.cnet.com/8301-19512_7-57408964-233/shazam-for-ios-adds-tv-to-its-list-of-media-it-can-identify/|accessdate=1 October 2012|newspaper=CNET|date=3 April 2012}}</ref>\u000a\u000aIn August 2012, Shazam announced the service had been used to tag five billion songs, TV shows and advertisements. In addition, Shazam claimed to have over 225 million users across 200 countries.<ref>{{cite news|last=Sawers|first=Paul|title=Shazam: Five billion songs, TV shows and ads tagged|url=http://thenextweb.com/insider/2012/08/07/shazam-five-billion-songs-tv-shows-and-ads-tagged/|accessdate=30 September 2012|newspaper=The Next Web|date=7 August 2012}}</ref> A month later, the service claimed to have more than 250 million users with 2 million active users per week.<ref name="TV tags">{{cite news|last=Kinder|first=Lucy|title=Shazam hits 250 million users and adds TV tagging capability|url=http://www.telegraph.co.uk/technology/news/9547632/Shazam-hits-250-million-users-and-adds-TV-tagging-capability.html|accessdate=17 September 2012|newspaper=The Telegraph|date=17 September 2012|location=London}}</ref> The Shazam app currently has more than 100 million monthly active users and has been used on more than 500 million mobile devices.<ref>http://thenextweb.com/insider/2014/08/20/shazam-now-100-million-monthly-active-users-mobile/ </ref> In October of 2014, Shazam announced its technology has been used to identify 15 billion songs.<ref>http://www.siliconrepublic.com/digital-life/item/38714-15-billion-songs-have-been </ref>\u000a\u000aThe Shazam app was listed among Techland's 50 Best Android Applications for 2013.<ref>{{cite news |url=http://techland.time.com/2013/07/01/50-best-android-apps-for-2013/slide/pulse-news/ | title=50 Best Android Apps for 2013 | author=Jared Newman | work=Techland | accessdate=30 June 2013 | date=1 July 2013}}</ref>\u000a\u000aIn August 2014, Shazam announced there would be no more updates for Shazam(RED) after August 7.<ref>[https://support.shazam.com/hc/en-us/articles/202604996-Important-News-About-SHAZAM-RED Important News About Shazam(RED)] \u2014 Shazam Support</ref> Current users are advised to switch to the free version with tags transferred and ads removed (for free).\u000a\u000aApple\u2019s launch of iOS 8 in September of 2014 came with the seamless integration of Shazam into Apple\u2019s intelligent personal assistant Siri function.<ref>http://www.jbgnews.com/2014/09/shazam-partners-with-apple-to-bring-music-recognition-to-siri/504608.html </ref>\u000a\u000aIn October of 2014, Shazam introduced version 8.0 of the app, which features a new and improved News feed, as well as a section featuring Shazam charts and an \u201cexplore\u201d option which lets user explore Shazamed tracks near them and around the world.<ref>http://appadvice.com/appnn/2014/10/shazam-8-0-features-interactive-notifications-in-ios-8-revamped-news-feed-and-more </ref>\u000a\u000a===Desktop app=== \u000aShazam announced the launch of Shazam for Mac, a desktop application, in July of 2014. When enabled, the app runs in the background of a Mac and automatically recognizes any song played on or near the computer, including songs playing in the background of TV shows or YouTube videos.<ref>http://mashable.com/2014/07/31/shazam-mac-app/ </ref>\u000a\u000a==Similar apps==\u000a\u000a*[[SoundHound]], previously known as Midomi, uses [[Query by humming]] to identify songs.{{citation needed|date=October 2012}}\u000a*[[Gracenote]]'s MusicID-Stream has the main advantage of having the largest database of all music IDs (with more than 28 million songs).{{citation needed|date=October 2012}}\u000a*Musipedia is a music search engine that works differently from others because instead of using techniques to identify recorded music, it can identify pieces of music from a single melody or rhythm.{{citation needed|date=October 2012}}\u000a*Play by Yahoo Music.\u000a*Bing music identification.\u000a*Sony TrackID\u000a*Path also has a music-identification feature.<ref>{{cite news|last=Cabebe|first=Jaymar|title=Path: The smaller, simpler alternative to Facebook|url=http://news.cnet.com/8301-1035_3-57416066-94/path-the-smaller-simpler-alternative-to-facebook/|accessdate=1 October 2012|newspaper=CNET|date=18 April 2012}}</ref>\u000a*Stream That Song by Orange Innovation UK Ltd\u000a\u000a==Patent infringement lawsuit==\u000aIn May 2009, Tune Hunter accused Shazam of violating {{US Patent|6941275}}, which covers music identification and purchase in a portable device.<ref>{{cite news|last=Ogg|first=Erica|title=Apple, AT&T, Samsung, Verizon, and others sued over Shazam app|url=http://news.cnet.com/8301-13579_3-10241309-37.html|accessdate=29 September 2012|newspaper=CNET|date=14 May 2009}}</ref> Shazam settled the case in January 2010.<ref>{{cite news\u000a|title=Shazam Settles Patent Infringement Case With Tune Hunter\u000a|url=http://techcrunch.com/2010/01/06/shazam-tune-hunter-settlement/\u000a|date=Jan 6, 2010\u000a|first=Robin\u000a|last=Wauters\u000a}}</ref>\u000a\u000a==Funding==\u000a\u000aAs of September 2012, Shazam had raised $32 million in funding.<ref name="Techcrunch">{{cite news|last=Kincaid|first=Jason|title=Shazam Raises A Huge Round to the Tune of $32 Million|url=http://techcrunch.com/2011/06/22/shazam-raises-a-huge-round-to-the-tune-of-32-million/|accessdate=20 September 2012|newspaper=TechCrunch|date=22 June 2011}}</ref> In July 2013, [[Carlos Slim]] invested $40 million in Shazam for an undisclosed share.<ref>[http://www.ft.com/intl/cms/s/0/97d2c46a-e58d-11e2-8d0b-00144feabdc0.html#axzz2ag6DhVhD Carlos Slim invests $40m in music app Shazam \u2013 FT.com<!-- Bot generated title -->]</ref> And in March of 2014, Shazam confirmed another $20 million in new funding, raising the total value of the company to half a billion dollars.<ref>http://www.billboard.com/biz/articles/5930359/shazam-confirms-20m-in-new-funding-raising-value-to-500m </ref>\u000a\u000a==See also==\u000a* [[Query by humming]]\u000a* [[Acoustic fingerprint]]\u000a* [[Spectrogram]]\u000a* [[Sound recording copyright symbol]]\u000a\u000a==References==\u000a{{Reflist|30em}}\u000a\u000a==Further reading==\u000a* {{cite news |last=Dredge |first=Stuart |title=Shazam: 'TV advertising is going to become our primary revenue stream' |url=http://www.guardian.co.uk/media/appsblog/2013/feb/27/shazam-tv-advertising-future |accessdate=27 February 2013 |newspaper=[[The Guardian]] |date=27 February 2013|location=London}}\u000a\u000a==External links==\u000a* {{Official website|www.shazam.com/music/web/home.html}}\u000a\u000a[[Category:Companies based in London]]\u000a[[Category:Acoustic fingerprinting]]\u000a[[Category:Android (operating system) software]]\u000a[[Category:BlackBerry software]]\u000a[[Category:IOS software]]\u000a[[Category:Symbian software]]\u000a[[Category:Music search engines]]\u000a[[Category:Companies established in 1999]]\u000a[[Category:Windows Phone software]]
p294
sg6
S'Shazam (service)'
p295
ssI110
(dp296
g2
S'http://en.wikipedia.org/wiki/International Society for Music Information Retrieval'
p297
sg4
VThe '''International Society for Music Information Retrieval''' ('''ISMIR''') is an international forum for research on the organization of music-related data. It started as an informal group steered by an ''ad hoc'' committee in 2000<ref>[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: ''The History of ISMIR - A Short Happy Tale''. D-Lib Magazine, Vol. 8 No. 11 ISSN: 1082-9873.]</ref> which established a yearly symposium - whence "ISMIR", which meant '''International Symposium on Music Information Retrieval'''. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.<ref>[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]</ref>\u000a\u000a==Purpose==\u000aGiven the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.\u000a\u000aAs the term Music Information Retrieval (MIR) indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science and many others.\u000a\u000a==Annual Conference==\u000aSince its inception in 2000, ISMIR has been the world\u2019s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.\u000a* [http://ismir2012.ismir.net ISMIR 2012], 8\u201312 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2012'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2011.ismir.net ISMIR 2011], 24\u201328 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2011'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2010.ismir.net ISMIR 2010], 9\u201313 August 2010, Utrecht (Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2010'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2009.ismir.net ISMIR 2009], 26\u201330 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2009'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2008.ismir.net ISMIR 2008], 14\u201318 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2008'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2007.ismir.net ISMIR 2007], 23\u201330 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2007'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2006.ismir.net ISMIR 2006], 8\u201312 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2006'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2005.ismir.net ISMIR 2005], 11\u201315 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2005'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2004.ismir.net ISMIR 2004], 10\u201315 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2004'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2003.ismir.net ISMIR 2003], 26\u201330 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2003'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2002.ismir.net ISMIR 2002], 13\u201317 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2002'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2001.ismir.net ISMIR 2001], 15\u201317 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2001'&page=0&order=Authors&order_type=ASC proceedings]\u000a* [http://ismir2000.ismir.net ISMIR 2000], 23\u201325 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='200'&page=0&order=Authors&order_type=ASC proceedings]\u000a\u000aThe [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences.\u000a\u000a==MIREX==\u000aThe Music Information Retrieval Evaluation eXchange (MIREX)<ref>[http://www.music-ir.org/mirex MIREX Wiki]</ref> is an annual evaluation campaign for Music Information Retrieval (MIR) algorithms, coupled to the ISMIR conference.\u000a\u000aMIR tasks evaluated at past MIREXs include:\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Train/Test_Tasks Audio Train/Test Tasks]\u000a**Audio Artist Identification\u000a**Audio Genre Classification\u000a**Audio Music Mood Classification\u000a**Audio Classical Composer Identification\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Genre_Classification Symbolic Genre Classification]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Onset_Detection Audio Onset Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Key_Detection Audio Key Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Key_Detection Symbolic Key Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Tag_Classification Audio Tag Classification]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Cover_Song_Identification Audio Cover Song Identification]\u000a*[http://www.music-ir.org/mirex/wiki/Real-time_Audio_to_Score_Alignment_(a.k.a_Score_Following) Real-time Audio to Score Alignment (a.k.a Score Following)]\u000a*[http://www.music-ir.org/mirex/wiki/Query_by_Singing/Humming Query by Singing/Humming]\u000a*[http://www.music-ir.org/mirex/wiki/Multiple_Fundamental_Frequency_Estimation_&_Tracking Multiple Fundamental Frequency Estimation & Tracking]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Chord_Estimation Audio Chord Estimation]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Melody_Extraction Audio Melody Extraction]\u000a*[http://www.music-ir.org/mirex/wiki/Query_by_Tapping Query by Tapping]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Beat_Tracking Audio Beat Tracking]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Music_Similarity_and_Retrieval Audio Music Similarity and Retrieval]\u000a*[http://www.music-ir.org/mirex/wiki/Symbolic_Melodic_Similarity Symbolic Melodic Similarity]\u000a*[http://www.music-ir.org/mirex/wiki/Structural_Segmentation Structural Segmentation]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Drum_Detection Audio Drum Detection]\u000a*[http://www.music-ir.org/mirex/wiki/Audio_Tempo_Extraction Audio Tempo Extraction]\u000a\u000a==See also==\u000a* [[International Conference on Digital Audio Effects]]\u000a* [[Music Technology]]\u000a* [[Sound and music computing|Sound and Music Computing]]\u000a* [[Sound and Music Computing Conference]]\u000a\u000a==Notes==\u000a{{Reflist}}\u000a\u000a[[Category:Music technology]]\u000a[[Category:Multimedia]]\u000a[[Category:Information retrieval]]
p298
sg6
S'International Society for Music Information Retrieval'
p299
ssI239
(dp300
g2
S'http://en.wikipedia.org/wiki/Songza'
p301
sg4
S'{{use mdy dates|date=July 2014}}\n{{use American English|date=July 2014}}\n{{Infobox website\n|name           = Songza\n|logo           = [[File:Songza Logo.jpg|frameless|150px]]\n|screenshot     = \n|caption        = \n|url            = {{URL|songza.com}}\n|alexa          = {{Loss}} 9,279 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/songza.com |title= Songza.com Site Info | publisher= [[Alexa Internet]] |accessdate= April 1, 2014 }}</ref><!--Updated monthly by OKBot.-->\n|commercial     = \n|type           = Free [[internet radio]]\n|language       = [[English language|English]]\n|location       = [[Long Island City, New York|Long Island City]], [[Queens]], [[New York City]], [[New York]], United States\n|registration   = \n|owner          = [[Google Inc.]]\n|author         = [[Aza Raskin]] and Scott Robbin\n|launch date    = {{start date and age|2007|11|08|paren=yes}}\n|current status = Active\n|revenue        = \n|slogan         = Good music makes good times.<ref>{{cite web|url= http://songza.com |title= Songza.com Site Info | publisher= Songza Media, Inc. |accessdate= August 15, 2012 }}</ref>\n}}\n\n\'\'\'Songza\'\'\' is a free [[music streaming]] and [[Recommender system|recommendation]] service for Internet users in the United States and Canada. \n\nStating that its playlists are made by music experts, the service recommends various playlists based on time of day and mood or activity.<ref name="The New York Times">{{cite news| first= Ben| last= Sisaro| work = [[The New York Times]] |title= Pandora Faces Rivals for Ears and Ads| accessdate = June 20, 2012| url= http://www.nytimes.com/2012/06/21/business/songza-and-spotify-challenge-pandora-for-ears-and-ads.html?_r=3| date= June 20, 2012}}</ref><ref name=PandoDaily>{{cite web| first= Erin|last= Griffith| publisher= [[PandoDaily]]|title= Songza\'s Founders Realized They Weren\'t Thinking Radically Enough{{spaced ndash}} Here\'s How They Changed That| accessdate = August 15, 2012|url= http://pandodaily.com/2012/08/15/songzas-founders-realized-they-werent-thinking-radically-enough-heres-how-they-changed-that/}}</ref> Songza offers playlists for activities such as waking up, working out, commuting, concentrating, unwinding, entertaining, and sleeping.<ref name="The Washington Post" >{{cite news| first= Hayley| last= Tsukayama| work = [[The Washington Post]] |title=TechBits: Songza adapts the music to your mood| accessdate = June 23, 2012| url = http://www.washingtonpost.com/techbits-songza-adapts-the-music-to-your-mood/2012/06/23/gJQAYRzKyV_story.html| date= June 25, 2012}}</ref>  Users can vote songs up or down, and the service will adapt to the user\'s personal music preferences.<ref name="The Washington Post" /> Users can find playlists not just based on artists, songs, or genres, but also based on themes, interests, and eras, such as "[[List of 1990s one-hit wonders in the United States|90s One-Hit Wonders]]", or "Music of [[Fashion Week]]".<ref name=SongzaAbout>{{cite web| publisher= Songza|title= About Us| accessdate = March 25, 2011| url = http://songza.com/page/about/}}</ref>\n\nSongza is headquartered in the [[Long Island City]] neighborhood of the [[Queens]] [[borough (New York City)|borough]] of [[New York City]], [[New York]].<ref name="NY Daily News">{{cite news| first= Clare | last= Trapasso| work = [[Daily News (New York)|Daily News]] |title= Songza music service streams for success| accessdate = July 27, 2012| url= http://articles.nydailynews.com/2012-07-27/news/32874462_1_spotify-apps-music-download}}</ref>\n\n== History ==\n[[Amie Street]] acquired Songza, a product created by [[Aza Raskin]] and Scott Robbin, in October 2008.<ref>{{cite web| first= Kristen| last=Nicole| publisher= bub.blicio.us |title= Interview with Amie Street: Why Keep Acquisition of Songza a Secret?| accessdate = March 25, 2011| url = http://bub.blicio.us/interview-with-amie-street-why-keep-acquisition-of-songza-a-secret/}}</ref> In August 2010, Amie Street was sold to Amazon for an undisclosed amount.<ref>{{cite web| first= Michael | last= Arrington| publisher= [[TechCrunch]]|title= Amazon Acquires Amie Street, But Not in a Good Way| accessdate = September 8, 2010| url= http://techcrunch.com/2010/09/08/amazon-acquires-amie-street-but-not-in-a-good-way/}}</ref>  Shortly after this the co-founders{{spaced ndash}} CEO Elias Roman, COO Peter Asbill, CPO Elliott Breece and CCO Eric Davich{{spaced ndash}} refocused their efforts on Songza.<ref name="The New York Times" /><ref>{{cite web| publisher= [[Internships.com]]|title= 5 in 5! with Eric Davich, Chief Content Officer and Co-Founder of Songza| accessdate = August 6, 2012| url= http://www.internships.com/eyeoftheintern/applying-2/employers-applying-2/5-5-eric-davichchief-content-officer-cofounder-songza/?cid=SO_ST_TW_080612_5IN5_SONGZA}}</ref>  The team discontinued the original version and relaunched a new alpha version of Songza, keeping nothing of the original product but the name.<ref name=Upstart>{{cite news| first= Michael| last= del Castillo| work =  [[American City Business Journals|Upstart Business Journal]] |title= Downtime: The birth of Songza| accessdate = June 15, 2012| url= http://upstart.bizjournals.com/entrepreneurs/hot-shots/2012/06/15/songza-minigolfs-to-no-1-app.html?page=2}}</ref>\n\nOver the next year the founders experimented with various iterations, when the app originally launched in 2010 "it was like a pre-Turntable.fm.  A function called Social Radio allowed users to be DJs for their friends" stated PandoDaily.<ref name="PandoDaily" />  This version of the app allowed it to be social and crowdsourced; the problem with it was that the service as it stood was not sufficiently differentiated from other services on the market and the quality of the crowd sourced playlists was low.<ref name=PandoDaily/>  Following a year of testing various iterations of the alpha version of the app, Songza relaunched in beta on iPhone and Android apps on September 13, 2011, armed with a team of 25 expert music curators.<ref name="The New York Times" /><ref name="PandoDaily" /><ref name=TechCrunch>{{cite web| first= Rip| last= Empson| publisher= [[TechCrunch]]|title= Songza Raises Seven Figure Round; Launches Mobile, Sharable Music Collections in the Cloud| accessdate = September 13, 2011| url= http://techcrunch.com/2011/09/13/songza-raises-seven-figure-round-launches-mobile-sharable-music-collections-in-the-cloud/}}</ref><ref>[http://www.ad60.com/2011/09/19/songza-launches-iphone-android-apps-digitize-mix-tape/ "Songza launches iPhone and Android apps to digitize the mix tape"].</ref>\n\nIn March 2012, Songza released its Music Concierge feature, on iPhone and the web.<ref name="The New York Times" /><ref name = TechCrunch>{{cite web| first= Jordan| last= Crook| publisher= [[TechCrunch.com]]|title= Songza, the Music Streaming Service That Does All Work for You, Launches an iPad App| accessdate = June 7, 2012| url= http://techcrunch.com/2012/06/07/songza-the-music-streaming-service-that-does-all-work-for-you-launches-an-ipad-app/}}</ref>  The concierge presents users with up to six situations based on time of day, with filters for whatever mood they might be in.  For example, on a Wednesday morning a user might be presented with situations for "Waking Up", "Singing in the Shower", "Working Out" and so on.  This feature was rolled out to iPad on June 7, 2012; during the first ten days following the iPad app launch, Songza saw over 1.15 million downloads.<ref>{{cite news| first= Stephanie| last= Mlot| work = [[PC Magazine]]|title= Songza Hits 1.15 Million iOS Downloads in 10 Days| accessdate = June 18, 2012| url= http://www.pcmag.com/article2/0,2817,2405952,00.asp}}</ref>\n\nOn June 12, 2012, Songza was listed as the top free app on iTunes for the iPad and the number two free app for the iPhone.<ref>{{cite web| first= Glenn| last= Peoples| publisher= [[Billboard.biz]]|title= Songza Reaches One Million iOs Downloads in Ten Days, But Is It the Next Big Thing?| accessdate = June 19, 2012| url= http://www.billboard.biz/bbbiz/others/songza-reaches-one-million-ios-downloads-1007360352.story}}</ref>  Concierge was released on Android on July 10, 2012, and for Android tablets on August 14, 2012.<ref>{{cite web| first= Andrew| last= Kameka| publisher= Androinica.com |title= Songza re-ups with expert Music Concierge playlists, lockscreen controls, and new Holo-like design| accessdate = July 10, 2012| url= http://androinica.com/2012/07/songza-android-app/}}</ref><ref>{{cite news| first= Stephanie| last= Mlot| work = [[PC Magazine]]|title= Songza App Now Available on Android Tablets| accessdate = August 14, 2012| url= http://www.pcmag.com/article2/0,2817,2408435,00.asp}}</ref>  The app expanded to Canada on August 7, 2012, and became the number-one overall free app in Canada on August 13, 2012.<ref name=PandoDaily/><ref>{{cite web| first= Anand| last= Ram| publisher= o.canada.com |title= Songza\'s Elias Roman wants to provide the music for every mood | accessdate = August 7, 2012| url= http://o.canada.com/2012/08/05/songzas-elias-roman-wants-to-provide-the-music-for-every-mood/}}</ref> Within the week of Microsoft\'s Build developer event in June 2013, Songza snuck in its official Windows 8 App.<ref>[http://www.wpcentral.com/songza-sneaks-windows-store-wins-our-hearts]. WP Central. June 27, 2013.</ref>\n\nSongza launched in Canada on August 7, 2012, and reached the one million download mark after 70 days.<ref>Dobby, Christine (August 23, 2012).  [http://business.financialpost.com/2012/08/23/songza-startup-singing-a-canadian-tune/ "Songza startup singing a Canadian tune"].  \'\'[[Financial Post]]\'\'. August 23, 2012.</ref><ref>Crook, Jordan (October 18, 2012). [http://techcrunch.com/2012/10/18/songzas-canada-launch-nabs-1-million-new-users-in-70-days/ "Songza\'s Canada Launch Nabs 1 Million New Users in 70 Days"]. [[TechCrunch]].</ref>\n\nStarting October 2013, Songza began inserting pop-up audio/video ads when initiating a playlist so it is no longer "audio-ad free". Songza reported having 5.5 million regular users at the end of 2013.<ref>{{Cite news|url = http://www.nytimes.com/2014/07/02/business/media/google-buys-songza-a-playlist-app-for-any-occasion.html|title = Google in Deal for Songza, a Music Playlist Service|last = Sisario|first = Ben|date = July 1, 2014|work = New York Times|accessdate = }}</ref>\n\nSongza was acquired by Google on July 1, 2014.<ref>{{cite web | url=http://techcrunch.com/2014/07/01/google-buys-songza/ | title=Google Buys Songza | publisher= [[TechCrunch]] | accessdate= July 1, 2014}}</ref> No terms were disclosed but speculation put the price at somewhere between $15 million and $39 million. Both companies issued statements saying they were "thrilled" to be doing the deal.<ref name="GoogleSongza">{{cite news|title=Google acquires music app start-up Songza|url=http://www.businesssun.com/index.php/sid/223470673/scat/3a8a80d6f705f8cc/ht/Google-acquires-music-app-start-up-Songza|accessdate= July 3, 2014|publisher=\'\'Business Sun\'\'}}</ref> In October 2014, following the acquisition, the [[Google Play Music|Google Play Music All Access]] service was updated to include functionality adapted from Songza\'s Concierge system.<ref name=verge-songzagpm>{{cite web|title=Google brings Songza\'s best feature to Play Music|url=http://www.theverge.com/2014/10/21/7027707/google-brings-best-songza-feature-to-play-music|website=The Verge|accessdate=21 October 2014}}</ref>\n\n==Similar organizations==\n{{div col|colwidth=30em}}\n* [[8tracks]]\n* [[AccuRadio]]\n* [[Deezer]]\n* [[Digitally Imported]]\n* [[FIT Radio]]\n* [[Google Play Music]]\n* [[Grooveshark]]\n* [[Guvera]]\n* [[iHeartRadio]]\n* [[Live365]]\n* [[MOG (online music)|MOG]]\n* [[Musicovery]]\n* [[Pandora Radio]]\n* [[Rara.com]]\n* [[Rdio]]\n* [[Rhapsody (online music service)|Rhapsody]]\n* [[Slacker Radio]]\n* [[Spotify]]\n* [[Soundtracker (music streaming)]]\n* [[WiMP]]\n* [[Xbox Music]]\n{{div col end}}\n\n\n{{portal|Companies|Music}}\n\n==References==\n{{reflist|30em}}\n==External links==\n*{{official website|songza.com}}\n\n{{Digital distribution platforms}}\n{{Google Inc.}}\n\n[[Category:American companies established in 2007]]\n[[Category:Community websites]]\n[[Category:Companies based in Queens, New York]]\n[[Category:Domain-specific search engines]]\n[[Category:Free music]]\n[[Category:Google acquisitions]]\n[[Category:Internet advertising]]\n[[Category:Internet companies of the United States]]\n[[Category:Internet properties established in 2007]]\n[[Category:Internet radio in the United States]]\n[[Category:Long Island City]]\n[[Category:Media companies based in New York City]]\n[[Category:Music companies of the United States]]\n[[Category:Music search engines]]\n[[Category:Recommender systems]]\n[[Category:Technology companies established in 2007]]'
p302
sg6
S'Songza'
p303
ssI113
(dp304
g2
S'http://en.wikipedia.org/wiki/Personalized search'
p305
sg4
V{{essay-like|date=January 2015}}\u000a{{original research|date=January 2015}}\u000a'''Personalized search''' refers to search experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to personalizing search results, one involving modifying the user\u2019s query and the other re-ranking search results.<ref>{{cite journal|last=Pitokow|first=James|author2=Hinrich Schütze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM (CACM)|year=2002|volume=45|issue=9|pages=50\u201355|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}</ref>\u000a\u000a==History==\u000a\u000aGoogle introduced Personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search set up for not just those who have a Google account but everyone as well. There is not very much information on how exactly Google personalizes their searches, however, it is believed that they use user language, location, and web history.<ref>http://personalization.ccs.neu.edu/paper.pdf</ref>\u000a\u000aEarly search engines, like [[Yahoo!]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by [[Google]], has become far more complex with the goal to "understand exactly what you mean and give you exactly what you want."<ref>{{citation | last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Using mathematical algorithms, search engines are now able to return results based on the number of links to an from sites; the more links a site has, the higher it is placed on the page.<ref>{{cite AV media|last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.<ref>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969\u2013982}}</ref> If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results  indicating that those near the top are more relevant to a user's wants than those below.<ref>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969\u2013982}}</ref>\u000a\u000aWhile many [[search engines]] take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.<ref>{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}</ref><ref>{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237\u2013251|doi=10.1002/asi.20477}}</ref> But user supplied information can be hard to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.<ref>{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287\u2013296}}</ref><ref>{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581\u2013590}}</ref><ref>{{cite journal|last=Shen|first=X.|coauthors=Tan, B. and Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824\u2013831}}</ref><ref>{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675\u2013684}}</ref><ref>{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415\u2013422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}</ref>\u000a\u000aThere are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]'s search result personalization<ref>{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}</ref>). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. The more you keep going the same site through a search result from Google, it believes that you like that page. So when you do certain searches, Google's personalized search algorithm gives the page a boost, moving it up through the ranks. Even if you're signed out, Google may personalize your results because it keeps a 180 day record of what a particular web browser has searched for, linked to a cookie in that browser.<ref>{{cite web|last=Sullivan|first=Danny|title=Of "Magic Keywords" and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}</ref>\u000a\u000aIn order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University set out to answer this question. By comparing an aggregate set of searches from logged in users against a control group, the research team found that 11.7% of results show differences due to personalization, however this varies widely by search query and result ranking position.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref> Of various factors tested, the two that had measurable impact were being logged in with a Google account and the IP address of the searching users. It should also be noted that results with high degress of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if you searched for "used car sales", Google may churn out results of local car dealerships in your area. On the other hand, queries with the least amount of personalization include factual queries ("what is") and health.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref>\u000a\u000aWhen measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when you perform a search and follow it with a subsequent search, the results of the second search is influenced by the first search. An interesting point to note is that the top ranked URLs are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization, based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.<ref>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|accessdate=21 April 2014}}</ref>\u000a\u000a==The Filter Bubble==\u000a{{Main|Filter bubble}}\u000a\u000aSeveral concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by biasing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the "filter bubble" by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they've collected on individuals. This can isolate users in their own worlds or "filter bubbles" where they only see information that they want to, such a consequence of "The Friendly World Syndrome." As a result people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aThe methods of personalization, and how useful it is to \u201cpromote\u201d certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the Filter Bubble happens. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.<ref>{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal Of Pattern Recognition & Artificial Intelligence |year=2007|pages=183\u2013205}}</ref>\u000a\u000aAn area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information. This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref> While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.<ref>http://{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|accessdate= 27 April 2014}}</ref>\u000a\u000aMany search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in 'apple' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user's interests based on which links they click on in a results list, while downgrading unclicked links.<ref>{{cite journal|last=Wai-Tin|first=Kenneth|coauthors=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEE transaction on knowledge and data engineering|year=2010|volume=22|issue=7|pages=969\u2013982|doi=10.1109/tkde.2009.144}}</ref>\u000a\u000aThe feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.<ref>[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html "Google Personalized Results Could Be Bad for Search"]. ''Network World''. Retrieved July 12, 2010.</ref> An example of this is found in Eli Pariser's, The Filter Bubble, where he had two friends type in "BP" into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aSome have noted that personalized search results not only serve to customize a user's search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].<ref>{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}</ref>\u000a\u000a==The Case of Google==\u000a{{Main|Google Personalized Search}}\u000a\u000aThe perfect example of search personalization is [[Google]]. Google is not just a search engine, but a corporation that is entering every facet of our lives. Personalization with Google has gone far beyond just search. There are a host of new applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.<ref>{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=20\u201331}}</ref> that keeps track of all the information directly under one\u2019s name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the [[New York Times]]. The high level of personalization that was available with Google played a significant part in helping remain the world\u2019s most favorite search engine.\u000a\u000aOne large example of Google\u2019s ability to personalized search is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed as interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information you want. The concern, however, is that the very important information can be held back because it does not match with the criteria that the program sets for the particular user. This can create the \u201c[[filter bubble]]\u201d as described earlier.<ref>{{cite book|last=Pariser|first=Eli|title=The Filter Bubble|year=2011}}</ref>\u000a\u000aAn interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff, is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.\u000aGoogle\u2019s popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although many people would say that having Google personalize your search results based on what you searched previously would be a good thing, there are negatives that come with it.<ref>{{cite web|last=Jackson|first=Mark|title=The Future of Google's Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}</ref><ref>{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}</ref>\u000aWith the power from this information, Google has chosen to bully its way into other sectors it owned such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.\u000a\u000aUsing Search Personalization, Google has doubled its video market share to about eighty percent. The legal definition of a monopoly is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.\u000a\u000aThe analytical firm Experian Hitwise stated that since two thousand and seven, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from thirty one percent in two thousand and seven to ten percent in two thousand and ten. Even Yahoo Images has gone from twelve percent to seven percent. It becomes very apparent that the decline of these companies has come because of Google\u2019s increase in market share from forty three percent in two thousand and seven to about fifty five percent in two thousand and nine.\u000a\u000aIt might be easy to say that all of this has come from Google being more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Google\u2019s bullying because their numbers went from one point three million unique visitors to eleven point nine unique visitors in one month. That kind of growth can only come with the change of a process.\u000a\u000aIn the end, there are two things in common theme with all of these graphs. The first is that Google\u2019s market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around two thousand and seven, which is around the time that Google began to use its \u201cUniversal Search\u201d method.<ref>{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}</ref>\u000a\u000a==Benefits==\u000a\u000aOne of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, human\u2019s capability of processing information has not expanded much.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref> When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show positive correlation between personalized search and the quality of consumers\u2019 decisions.\u000a\u000aThe first study was conducted by Kristin Diehl from University of South Carolina. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that \u2018consumers make worse choices because lower search costs cause them to consider inferior options.\u2019 It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref> The study by Gerald Haubl from University of Alberta and Benedict G.C. Dellaert from Maastricht University mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers\u2019 decision quality and reduced the number of products inspected.<ref>Diehl, K. (2003). Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making. Advances In Consumer Research, 30(1), 166-169.</ref>\u000a\u000a==Models==\u000a\u000aPersonalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.<ref>Coyle, M., & Smyth, B. (2007). Information recovery and discovery in collaborative web search. Advances in Information Retrieval (pp. 356\u2013367).</ref> Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.\u000a\u000aThe first model available is based on the users\u2019 historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.\u000a\u000aThere is another way to personalize search results. In Bracha Shapira and Boaz Zabar\u2019s \u201cPersonalized Search: Integrating Collaboration and Social Networks\u201d, Shapira and Zabar focused on a model that utilizes a recommendation system.<ref>Shapira, B., & Zabar, B. (2011). Personalized search: Integrating collaboration and social networks. Journal Of The American Society For Information Science & Technology, 62(1), 146-160. doi:10.1002/asi.21446</ref> This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.\u000a\u000a==Disadvantages==\u000a\u000aWhile there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users\u2019 search engine results to material that aligns with the users\u2019 interests and history. It limits the users\u2019 ability to become exposed to material that would be relevant to the user\u2019s search query but due to the fact that some of this material differs from the user\u2019s interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. \u201cObjectivity matters little when you know what you are looking for, but its lack is problematic when you do not\u201d.<ref>{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426\u2013445|doi=10.1111/j.1467-9973.2012.01759.x}}</ref>  One of the main functions of the internet is the collection and sharing of information. This is the criticism of search personalization. It limits a core function of the web. It helps prevent users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to user\u2019s search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue. The user\u2019s search results will reflect that. The user not be displayed both sides of the issue if the user\u2019s interests lean to one side or another. The user may be missing out on information that could be important. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users even though each user entered the same search query. \u201cWhen I further distilled the results, I saw that only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on\u201d.<ref>{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|volume=35.6}}</ref> If search personalization was not active, all the results in theory should have been the same stories in an identical order.\u000a\u000aAnother disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling your internet interests and histories to other companies. This raises a privacy issue. The issue is if people are content with companies gather and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.\u000a\u000a==Sites that use Personalized Search==\u000a\u000aE. Pariser author of the Filter Bubble explains how there are differences that search personalization has on both Facebook and Google. Facebook implements personalization when it comes to the amount of things we share and also what pages we \u201clike\u201d. It also takes into consideration our social interactions, whose profile we visit the most, who we message or chat with are all indicators that are used when Facebook uses personalization. Rather than what we share being an indicator of what is filtered out, but Google takes into consideration what we \u201cclick\u201d to filter out what comes up in our searches. In addition Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and we share what other people want to see. Even while tagging photographs, Facebook uses personalization and recognition that will automatically assign a name to face for you without you having to tag them. In terms of Google we are provided similar websites and resources based on what we initially click on. This doesn't just affect Google and Facebook. There are even other websites that use the filter tactic to better adhere to user preferences. For example, Netflix also judges from the users search history to suggest movies that they may be interested in for the future. There are cites like Amazon and personal shopping cites also use other peoples history in order to serve their interests better. Twitter also uses personalization by \u201csuggesting\u201d other people to follow. In addition, based on who we \u201cfollow\u201d and who we \u201ctweet\u201d and \u201cretweet\u201d at Twitter filters out to peoples best interest for us.  Mark Zuckerberg, founder of Facebook, believed that we only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isn\u2019t true. Although personalized search may seem helpful it is not a very accurate representation of who we are as people. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles in order to make themselves look better. Search personalization is not an ideal representation of any person. There are so many cites used for different purposes and that does not make up one person\u2019s identity at all that, but are in fact false representations of ourselves.<ref>http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf</ref>\u000a\u000a==Personalized Search and Online Shopping==\u000a\u000aSearch engines, such as Google and Yahoo!, utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individual\u2019s web clicks, search engines can use personalized search to put forth advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in brick-and-mortar stores. These types of products and services are called long tail items.<ref>Badke, William. \u201cPersonalization and Information Literacy\u201d. Online, 47. Feb. 2012.</ref> Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.\u000a\u000aAside from aiding consumers and businesses in finding one-another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.<ref>Inside Google. "Traffic Report: How Google Is Squeezing Out Competitors and Muscling Into New Markets." Consumer Watchdog. http://www.consumerwatchdog.org, 2 June 2010. Web.</ref> In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word \u201cshoes\u201d using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumer\u2019s queries.\u000a\u000a==References==\u000a{{reflist|30em}}\u000a\u000a{{DEFAULTSORT:Personalized search}}\u000a[[Category:Information retrieval]]\u000a[[Category:Internet search engines| ]]\u000a[[Category:Internet terminology]]\u000a[[Category:Personalized search|*]]
p306
sg6
S'Personalized search'
p307
ssI116
(dp308
g2
S'http://en.wikipedia.org/wiki/Online search'
p309
sg4
V'''Online search''' is the process of interactively searching for and retrieving requested information via a computer from [[database]]s that are [[online]].<ref name="whatis?">{{cite journal|last1=Hawkins|first1= Donald T.|last2= Brown|first2= Carolyn P.|date=Jan 1980|title=What Is an Online Search?|journal=Online|volume=4|issue=1|pages=12\u201318|id=Eric:EJ214713| accessdate=2011-04-04}}</ref> Interactive searches became possible in the 1980s with the advent of faster databases and [[smart terminal]]s.<ref name="whatis?"/> In contrast, [[computerized batch searching]] was prevalent in the 1960s and 1970s.<ref name="whatis?"/> Today, searches through [[web search engine]]s constitute the majority of online searches.\u000a\u000aOnline searches often supplement reference transactions.\u000a\u000a==References==\u000a{{reflist}}\u000a\u000a{{Internet search}}\u000a\u000a[[Category:Internet terminology]]\u000a[[Category:Information retrieval]]\u000a\u000a\u000a{{web-stub}}
p310
sg6
S'Online search'
p311
ssI119
(dp312
g2
S'http://en.wikipedia.org/wiki/Personalization'
p313
sg4
V{{cleanup-reorganize|date=June 2008}}\u000a\u000a'''Personalization''', also known as '''customization''', involves using technology to accommodate the differences between individuals.\u000a\u000a==Web pages==\u000a{{see also|Web pages|Adaptive hypermedia}}  \u000a[[Web page]]s are personalized based on the characteristics (interests, social category, context, ...) of an individual. Personalization implies that the changes are based on implicit data, such as items purchased or pages viewed. The term ''customization'' is used instead when the site only uses explicit data such as ratings or preferences. \u000a\u000aOn an [[intranet]] or [[B2E]] [[Web portal#Enterprise Web portals|Enterprise Web portals]], personalization is often based on user attributes such as department, functional area, or role.  The term '''customization''' in this context refers to the ability of users to modify the page layout or specify what content should be displayed.\u000a\u000aThere are three categories of personalization:\u000a# Profile / Group based\u000a# Behaviour based (also known as Wisdom of the Crowds)\u000a# Collaboration based\u000a\u000a\u000a\u000aThere are three broad methods of personalization:\u000a# Implicit\u000a# Explicit\u000a# Hybrid\u000a\u000aWith implicit personalization the personalization is performed by the web page (or information system) based on the different categories mentioned above. It can also be learned from interactions with the user directly.<ref>{{cite web|last1=Flynn|first1=Lawrence|title=5 Things To Know About Siri And Google Now's Growing Intelligence|url=http://www.forbes.com/sites/parmyolson/2014/07/08/5-things-to-know-about-siri-and-google-nows-growing-intelligence/|website=Forbes}}</ref> With explicit personalization, the web page (or information system) is changed by the user using the features provided by the system.\u000aHybrid personalization combines the above two approaches to leverage the ''best of both worlds''.\u000a\u000aMany companies offer services for web recommendation and email recommendation that are based on personalization or anonymously collected user behaviors.<ref name=behaviors>[http://online.wsj.com/article/SB10001424052748703294904575385532109190198.html?mod=googlenews_wsj ''Wall Street Journal'', \u201cOn the Web's Cutting Edge, Anonymity in Name Only\u201d], August 4, 2010</ref>  \u000a\u000aWeb personalization is closely linked to the notion of '''[[Adaptive hypermedia]]''' (AH). The main difference is that the former would usually work on what is considered an Open Corpus Hypermedia, whilst the latter would traditionally work on Closed Corpus Hypermedia. However, recent research directions in the AH domain take both closed and open corpus into account. Thus, the two fields are closely inter-related.\u000a\u000aPersonalization is also being considered for use in less overtly commercial applications to improve the user experience online.<ref>[[Jonathan Bowen|Bowen, J.P.]] and Filippini-Fantoni, S., [http://www.archimuse.com/mw2004/papers/bowen/bowen.html Personalization and the Web from a Museum Perspective]. In [[David Bearman]] and Jennifer Trant (eds.), ''[[Museums and the Web]] 2004: Selected Papers from an International Conference'', Arlington, Virginia, USA, 31 March \u2013 3 April 2004. Archives & Museum Informatics, pages 63\u201378, 2004.</ref> [[Remote control]] manufacturer [[Ruwido]] developed an [[interactive]] [[IPTV]] platform in 2010 called Voco Media, which controls [[digital media]] in the [[living room]] using web personalization. It uses personalization as a tool that supports modern forms of [[TV]] usage, by allowing users to create different profiles for each family member, personalized menu structures and [[fingerprint recognition]].<ref>[http://www.digitaltveurope.net/news_articles/mar_10/23_mar_10/ruwido_wins_virgin_media_contract,_announces_new_voco_apps Ruwido Wins Virgin Media Contract, Announces New Voco App]{{dead link|date=January 2013}}</ref>\u000a\u000aInternet activist [[Eli Pariser]] has documented that search engines like Google and Yahoo News give different results to different people (even when logged out).  He also points out social media site Facebook changes user's friend feeds based on what it thinks they want to see.  Pariser warns that these algorithms can create a "[[filter bubble]]" that prevents people from encountering a diversity of viewpoints beyond their own, or which only presents facts which confirm their existing views.\u000a\u000a==Digital media==\u000aAnother aspect of personalization is the increasing prevalence of [[open data]] on the Web. Many companies make their data available on the Web via [[API]]s, web services, and [[open data]] standards.<ref>{{cite news| url=http://www.guardian.co.uk/news/datablog/2010/apr/02/ordnance-survey-open-data | location=London | work=The Guardian | first1=Chris | last1=Thorpe | first2=Simon | last2=Rogers | title=Ordnance Survey opendata maps: what does it actually include? | date=2 April 2010}}</ref> Ordnance Survey Open Data This data is structured to allow it to be inter-connected and re-used by third parties.<ref>{{cite web|url=http://www.cio.com/article/372363/Google_Opens_Up_Data_Center_For_Third_Party_Web_Applications |title=Google Opens Up Data Centre for Third Party Web Applications |publisher=Cio.com |date=2008-05-28 |accessdate=2013-01-16}}</ref>\u000a\u000aData available from a user\u2019s personal [[social graph]] can be accessed by third-party [[application software]] to be suited to fit the personalized [[web page]] or [[information appliance]].\u000a\u000aCurrent [[open data]] standards on the Web include:\u000a# [[Attention Profiling Mark-up Language]] (APML)\u000a# [[DataPortability]]\u000a# [[OpenID]]\u000a# [[OpenSocial]]\u000a\u000a== Mobile phones ==\u000a\u000aOver time mobile phones have seen an increased emphasis placed on user personalization. Far from the black and white screens and monophonic ringtones of the past, phones now offer interactive wallpapers and MP3 TruTones. In the UK and Asia, WeeMees have become popular. WeeMees are three-dimensional characters that are used as wallpaper and respond to the tendencies of the user. Video Graphics Array (VGA) picture quality allows people to change their background with ease without sacrificing quality. All of these services are downloaded through the provider with the goal to make the user feel connected to the phone.<ref>May, Harvey, and Greg Hearn. "The Mobile Phone as Media." International Journal of Cultural Studies 8.2 (2005): 195-211. Print.</ref>\u000a\u000a==Print media==\u000a{{main|Mail merge}}\u000a\u000aIn print media, ranging from [[magazine]]s to [[admail|promotional publication]]s, personalization uses databases of individual recipients\u2019 information. Not only does the written document address itself by name to the reader, but the advertising is targeted to the recipient\u2019s demographics or interests using fields within the database, such as "first name", "last name", "company", etc. \u000a\u000aThe term "personalization" should not be confused with variable data, which is a much more granular method of marketing that leverages both images and text with the medium, not just fields within a database. Although personalized children's books are created by companies who are using and leveraging all the strengths of [[variable data printing| variable data printing (VDP)]]. This allows for full image and text variability within a printed book.\u000aWith the advent of online 3D printing services such as Shapeways and Ponoko we are seeing personalization enter into the realms of product design.\u000a\u000a== Promotional merchandise ==\u000aPromotional items ([[mug]]s, [[T-shirt]]s, [[keychain]]s, [[ball]]s etc.) are regularly personalized. Personalized children\u2019s storybooks \u2014 wherein the child becomes the [[protagonist]], with the name and image of the child personalized \u2014 are also popular. Personalized CDs for children also exist. With the advent of [[digital printing]], personalized calendars that start in any month, birthday cards, cards, e-cards, posters and photo books can also be obtained. In addition, with the advent of [[3D printing]], personalised apparel and accessories, such as jewellery made by [[StyleRocks]], is also increasing in popularity.<ref>{{cite web|url=http://www.jewellermagazine.com/Article.aspx?id=2167&h=New-jewellery-website-targets-|title=New jewellery website targets 'customisers'|last=Weinman|first=Aaron|date=21 February 2012|publisher=Jeweller Magazine|language=|accessdate=6 January 2015|quote=StyleRocks founder and CEO, Pascale Helyar-Moray, said the site offers women\u2019s and men\u2019s rings, necklaces, bracelets, earrings and cufflinks. Working alongside an Australian jewellery wholesaler, Helyar-Moray said customers have access to a variety of different styles and designs in an attempt to widen the site\u2019s ability to personalise pieces.}}</ref><ref>{{cite web|url=http://www.ragtrader.com.au/news/style-first|title=Style first|date=15 August 2014|publisher=Ragtrader|language=|accessdate=6 January 2015|quote=Online retailer StyleRocks is about to introduce an Australian first for the jewellery sector. The customisable fine jewellery retailer has introduced 3D printing in conjunction with the launch of a new website.}}</ref>\u000a\u000a== Mass personalization ==\u000a\u000a{{tone|section|date=January 2011}}\u000a\u000aMass personalization is defined as custom tailoring by a company in accordance with its end users tastes and preferences.<ref>{{cite web|url=http://www.answers.com/personalization&r=67 |title=personalize: Definition, Synonyms from |publisher=Answers.com |date= |accessdate=2013-01-16}}</ref> From collaborative engineering perspective, mass customization can be viewed as collaborative efforts between customers and manufacturers, who have different sets of priorities and need to jointly search for solutions that best match customers\u2019 individual specific needs with manufacturers\u2019 customization capabilities. <ref>	Chen, S., Y. Wang and M. M. Tseng. 2009. Mass Customization as a Collaborative Engineering Effort. International Journal of Collaborative Engineering, 1(2): 152-167</ref> The main difference between mass customization and mass personalization is that customization is the ability for a company to give its customers an opportunity to create and choose product to certain specifications, but does have limits.<ref>Haag et al., ''Management Information Systems for the Information Age'', 3rd edition, 2006, page 331.</ref> Clothing industry has also adopted the mass customization paradigm and some footwear retailers are producing mass customized shoes.<ref>{{cite web|url=http://www.botisto.com/how.php?language=EN |title=Botisto |publisher=Botisto |date= |accessdate=2013-01-16}}</ref><ref>[http://www.promoline1.com/Custom-T-Shirts-s/1814.htm Clothing ]</ref> The gaming market is seeing personalization in the new custom controller industry. A new, and notable, company called "Experience Custom" gives customers the opportunity to order personalized gaming controllers.<ref>{{cite web|url=http://www.experiencecustom.com/|title=Custom Controllers |publisher=ExperienceCustom.com |date= |accessdate=2014-11-20}}</ref> \u000a\u000aA website knowing a user's location, and buying habits, will present offers and suggestions tailored to the user's demographics; this is an example of mass personalization. The personalization is not individual but rather the user is first classified and then the personalization is based on the group they belong to.<ref>{{cite news| url=http://www.telegraph.co.uk/foodanddrink/9808015/How-supermarkets-prop-up-our-class-system.html | location=London | work=The Daily Telegraph | first=Harry | last=Wallop | title=How supermarkets prop up our class system | date=2013-01-18}}</ref>\u000a\u000a[[Behavioral targeting]] represents a concept that is similar to mass personalization.\u000a\u000a== Predictive personalization ==\u000a\u000aPredictive personalization is defined as the ability to predict customer behavior, needs or wants - and tailor offers and communications very precisely.<ref>{{cite web|url=http://www.slideshare.net/jwtintelligence/jwt-10-trends-for-2013-executive-summary|title=10 Trends for 2013 Executive Summary: Definition, Projected Trends |publisher=JWTIntelligence.com |date= |accessdate=2012-12-04}}</ref>  Social data is one source of providing this predictive analysis, particularly social data that is structured.  Predictive personalization is a much more recent means of personalization and can be used well to augment current personalization offerings.\u000a\u000a==See also==\u000a* [[Adaptation (computer science)]]\u000a* [[Mass customization]]\u000a* [[Adaptive hypermedia]]\u000a* [[Behavioral targeting]]\u000a* [[Bespoke]]\u000a* [[Collaborative filtering]]\u000a* [[Configurator]]\u000a* [[Personalized learning]]\u000a* [[Preorder economy]]\u000a* [[Real-time marketing]]\u000a* [[Recommendation system]]\u000a* [[User modeling]]\u000a\u000a==References==\u000a{{reflist|2}}\u000a\u000a==External links==\u000a* [http://www.iimcp.org International Institute on Mass Customization & Personalization which organizes MCP, a biannual conference on customization and personalization]\u000a* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] ''The Journal of Personalization Research''\u000a\u000a[[Category:Human\u2013computer interaction]]\u000a[[Category:World Wide Web]]\u000a[[Category:User interface techniques]]\u000a[[Category:Usability|Personas]]\u000a[[Category:Types of marketing]]\u000a[[Category:Information retrieval]]
p314
sg6
S'Personalization'
p315
ssI122
(dp316
g2
S'http://en.wikipedia.org/wiki/Search engine indexing'
p317
sg4
V{{Too many see alsos|date=December 2012}}\u000a'''Search engine indexing''' collects, parses, and stores [[data (computing)|data]] to facilitate fast and accurate [[information retrieval]]. Index design incorporates interdisciplinary concepts from linguistics, cognitive psychology, mathematics, [[Information technology|informatics]], and computer science.  An alternate name for the process in the context of [[search engine]]s designed to find web pages on the Internet is ''[[web indexing]]''.\u000a\u000aPopular engines focus on the full-text indexing of online, natural language documents.<ref>Clarke, C., Cormack, G.: Dynamic Inverted Indexes for a Distributed Full-Text Retrieval System. TechRep MT-95-01, University of Waterloo, February 1995.</ref> [[Multimedia|Media types]] such as video and audio<ref>http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf</ref> and graphics<ref>Charles E. Jacobs, Adam Finkelstein, David H. Salesin. [http://grail.cs.washington.edu/projects/query/mrquery.pdf Fast Multiresolution Image Querying]. Department of Computer Science and Engineering, University of Washington. 1995. Verified Dec 2006</ref> are also searchable.\u000a\u000a[[Metasearch engine|Meta search engines]] reuse the indices of other services and do not store a local index, whereas cache-based search engines permanently store the index along with the  [[text corpus|corpus]]. Unlike full-text indices, partial-text services restrict the depth indexed to reduce index size. Larger services typically perform indexing at a predetermined time interval due to the required time and processing costs, while [[Intelligent agent|agent]]-based search engines index in [[Real time business intelligence|real time]].\u000a\u000a==Indexing==\u000aThe purpose of storing an index is to optimize speed and performance in finding relevant documents for a search query. Without an index, the search engine would [[Lexical analysis|scan]] every document in the corpus, which would require considerable time and computing power.  For example, while an index of 10,000 documents can be queried within milliseconds, a sequential scan of every word in 10,000 large documents could take hours. The additional computer storage required to store the index, as well as the considerable increase in the time required for an update to take place, are traded off for the time saved during information retrieval.\u000a\u000a===Index design factors===\u000aMajor factors in designing a search engine's architecture include:\u000a\u000a; Merge factors : How data enters the index, or how words or subject features are added to the index during text corpus traversal, and whether multiple indexers can work asynchronously. The indexer must first check whether it is updating old content or adding new content. Traversal typically correlates to the [[Web crawling|data collection]] policy. Search engine index merging is similar in concept to the [[Merge (SQL)|SQL Merge]] command and other merge algorithms.<ref>Brown, E.W.: Execution Performance Issues in Full-Text Information Retrieval. Computer Science Department, University of Massachusetts Amherst, Technical Report 95-81, October 1995.</ref>\u000a; Storage techniques : How to store the index [[data]], that is, whether information should be data compressed or filtered.\u000a; Index size : How much computer storage is required to support the index.\u000a; Lookup speed : How quickly a word can be found in the inverted index. The speed of finding an entry in a data structure, compared with how quickly it can be updated or removed, is a central focus of computer science.\u000a; Maintenance : How the index is maintained over time.<ref>Cutting, D., Pedersen, J.: Optimizations for dynamic inverted index maintenance. Proceedings of SIGIR, 405-411, 1990.</ref>\u000a;Fault tolerance : How important it is for the service to be reliable. Issues include dealing with index corruption, determining whether bad data can be treated in isolation, dealing with bad hardware, [[partition (database)|partitioning]], and schemes such as [[hash function|hash-based]] or composite partitioning,<ref>[http://dev.mysql.com/doc/refman/5.1/en/partitioning-linear-hash.html Linear Hash Partitioning]. MySQL 5.1 Reference Manual. Verified Dec 2006</ref> as well as [[Replication (computer science)|replication]].\u000a\u000a===Index data structures===\u000aSearch engine architectures vary in the way indexing is performed and in methods of index storage to meet the various design factors.\u000a\u000a;[[Suffix tree]] : Figuratively structured like a tree, supports linear time lookup. Built by storing the suffixes of words. The suffix tree is a type of [[trie]]. Tries support extendable hashing, which is important for search engine indexing.<ref>[http://www.nist.gov/dads/HTML/trie.html trie], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology].</ref> Used for searching for patterns in [[DNA]] sequences and clustering. A major drawback is that storing a word in the tree may require space beyond that required to store the word itself.<ref name="Gus97">{{cite book\u000a | last = Gusfield\u000a | first = Dan\u000a | origyear = 1997\u000a | year = 1999\u000a | title = Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology\u000a | publisher = Cambridge University Press\u000a | location = USA\u000a | isbn = 0-521-58519-8}}.\u000a</ref> An alternate representation is a [[suffix array]], which is considered to require less virtual memory and supports data compression such as the [[Burrows-Wheeler transform|BWT]] algorithm.\u000a\u000a;[[Inverted index]] : Stores a list of occurrences of each atomic search criterion,<ref>Black, Paul E., [http://www.nist.gov/dads/HTML/invertedIndex.html inverted index], [http://www.nist.gov/dads Dictionary of Algorithms and Data Structures], [http://www.nist.gov U.S. National Institute of Standards and Technology] Oct 2006. Verified Dec 2006.</ref> typically in the form of a [[hash table]] or [[binary tree]].<ref>C. C. Foster, Information retrieval: information storage and retrieval using AVL trees, Proceedings of the 1965 20th national conference, p.192-205, August 24\u201326, 1965, Cleveland, Ohio, United States</ref><ref>Landauer, W. I.: The balanced tree and its utilization in information retrieval. IEEE Trans. on Electronic Computers, Vol. EC-12, No. 6, December 1963.</ref>\u000a\u000a;[[Citation index]] : Stores citations or hyperlinks between documents to support citation analysis, a subject of [[Bibliometrics]].\u000a;[[N-gram|Ngram index]] : Stores sequences of length of data to support other types of retrieval or text mining.<ref>[http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13 Google Ngram Datasets] for sale at [http://www.ldc.upenn.edu/ LDC] Catalog</ref>\u000a;[[Document-term matrix]] : Used in latent semantic analysis, stores the occurrences of words in documents in a two-dimensional [[sparse matrix]].\u000a\u000a===Challenges in parallelism===\u000aA major challenge in the design of search engines is the management of serial computing processes. There are many opportunities for [[race conditions]] and coherent faults. For example, a new document is added to the corpus and the index must be updated, but the index simultaneously needs to continue responding to search queries. This is a collision between two competing tasks. Consider that authors are producers of information, and a web crawler is the consumer of this information, grabbing the text and storing it in a cache (or [[Text corpus|corpus]]). The forward index is the consumer of the information produced by the corpus, and the inverted index is the consumer of information produced by the forward index. This is commonly referred to as a '''producer-consumer model'''. The indexer is the producer of searchable information and users are the consumers that need to search.  The challenge is magnified when working with distributed storage and distributed processing. In an effort to scale with larger amounts of indexed information, the search engine's architecture may involve [[distributed computing]], where the search engine consists of several machines operating in unison. This increases the possibilities for incoherency and makes it more difficult to maintain a fully synchronized, distributed, parallel architecture.<ref>Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. Google, Inc. OSDI. 2004.</ref>\u000a\u000a===Inverted indices===\u000aMany search engines incorporate an [[inverted index]] when evaluating a [[search query]] to quickly locate documents containing the words in a query and then rank these documents by relevance. Because the inverted index stores a list of the documents containing each word, the search engine can use direct [[random access|access]] to find the documents associated with each word in the query in order to retrieve the matching documents quickly. The following is a simplified illustration of an inverted index:\u000a\u000a{| align="center" class="wikitable"\u000a|+ Inverted Index\u000a|-\u000a! Word !! Documents\u000a|-\u000a| the || Document 1, Document 3, Document 4, Document 5, Document 7\u000a|-\u000a| cow || Document 2, Document 3, Document 4\u000a|-\u000a| says || Document 5\u000a|-\u000a| moo || Document 7\u000a|}\u000a\u000aThis index can only determine whether a word exists within a particular document, since it stores no information regarding the frequency and position of the word; it is therefore considered to be a [[boolean datatype|boolean]] index. Such an index determines which documents match a query but does not rank matched documents. In some designs the index includes additional information such as the frequency of each word in each document or the positions of a word in each document.<ref>Grossman, Frieder, Goharian. [http://www.cs.clemson.edu/~juan/CPSC862/Concept-50/IR-Basics-of-Inverted-Index.pdf IR Basics of Inverted Index]. 2002. Verified Aug 2011.</ref> Position information enables the search algorithm to identify word proximity to support searching for phrases; frequency can be used to help in ranking the relevance of documents to the query. Such topics are the central research focus of [[information retrieval]].\u000a\u000aThe inverted index is a [[sparse matrix]], since not all words are present in each document. To reduce computer storage memory requirements, it is stored differently from a two dimensional [[Array data structure|array]]. The index is similar to the [[document-term matrix|term document matrices]] employed by [[latent semantic analysis]]. The inverted index can be considered a form of a hash table. In some cases the index is a form of a [[binary tree]], which requires additional storage but may reduce the lookup time. In larger indices the architecture is typically a [[distributed hash table]].<ref>Tang, Hunqiang. [[Sandhya Dwarkadas|Dwarkadas, Sandhya]]. "Hybrid Global Local Indexing for Efficient\u000aPeer to Peer Information Retrieval". University of Rochester. Pg 1. http://www.cs.rochester.edu/u/sandhya/papers/nsdi04.ps</ref>\u000a\u000a===Index merging===\u000aThe inverted index is filled via a merge or rebuild. A rebuild is similar to a merge but first deletes the contents of the inverted index. The architecture may be designed to support incremental indexing,<ref>Tomasic, A., et al.: Incremental Updates of Inverted Lists for Text Document Retrieval. Short Version of Stanford University Computer Science Technical Note STAN-CS-TN-93-1, December, 1993.</ref> where a merge identifies the document or documents to be added or updated and then parses each document into words. For technical accuracy, a merge conflates newly indexed documents, typically residing in virtual memory, with the index cache residing on one or more computer hard drives.\u000a\u000aAfter parsing, the indexer adds the referenced document to the document list for the appropriate words. In a larger search engine, the process of finding each word in the inverted index (in order to report that it occurred within a document) may be too time consuming, and so this process is commonly split up into two parts, the development of a forward index and a process which sorts the contents of the forward index into the inverted index. The inverted index is  so named because it is an inversion of the forward index.\u000a\u000a===The forward index===\u000aThe forward index stores a list of words for each document. The following is a simplified form of the forward index:\u000a\u000a{| align="center" class="wikitable"\u000a|+ Forward Index\u000a|-\u000a! Document !! Words\u000a|-\u000a| Document 1 || the,cow,says,moo\u000a|-\u000a| Document 2 || the,cat,and,the,hat\u000a|-\u000a| Document 3 || the,dish,ran,away,with,the,spoon\u000a|}\u000a\u000aThe rationale behind developing a forward index is that as documents are parsing, it is better to immediately store the words per document.  The delineation enables Asynchronous system processing, which partially circumvents the inverted index update [[wikt:bottleneck|bottleneck]].<ref>Sergey Brin and Lawrence Page. [http://infolab.stanford.edu/~backrub/google.html The Anatomy of a Large-Scale Hypertextual Web Search Engine]. [[Stanford University]]. 1998. Verified Dec 2006.</ref> The forward index is [[Sorting algorithm|sorted]] to transform it to an inverted index. The forward index is essentially a list of pairs consisting of a document and a word, collated by the document. Converting the forward index to an inverted index is only a matter of sorting the pairs by the words. In this regard, the inverted index is a word-sorted forward index.\u000a\u000a===Compression===\u000aGenerating or maintaining a large-scale search engine index represents a significant storage and processing challenge. Many search engines utilize a form of compression to reduce the size of the indices on [[computer storage|disk]].<ref>H.S. Heaps. Storage analysis of a compression coding for a document database. 1NFOR, I0(i):47-61, February 1972.</ref> Consider the following scenario for a full text, Internet search engine.\u000a\u000a* It takes 8 bits (or 1 [[byte]]) to store a single character. Some [[character encoding|encodings]] use 2 bytes per character<ref>[http://www.unicode.org/faq/basic_q.html#15 The Unicode Standard - Frequently Asked Questions]. Verified Dec 2006.</ref><ref>[http://www.uplink.freeuk.com/data.html Storage estimates]. Verified Dec 2006.</ref>\u000a* The average number of characters in any given word on a page may be estimated at 5 ([[Wikipedia:Size comparisons]])\u000a\u000aGiven this scenario, an uncompressed index (assuming a non-[[conflation|conflated]], simple, index) for 2 billion web pages would need to store 500 billion word entries. At 1 byte per character, or 5 bytes per word, this would require 2500 gigabytes of storage space alone. This space requirement may be even larger for a fault-tolerant distributed storage architecture. Depending on the compression technique chosen, the index can be reduced to a fraction of this size. The tradeoff is the time and processing power required to perform compression and decompression.\u000a\u000aNotably, large scale search engine designs incorporate the cost of storage as well as the costs of electricity to power the storage. Thus compression is a measure of cost.\u000a\u000a==Document parsing==\u000aDocument parsing breaks apart the components (words) of a document or other form of media for insertion into the forward and inverted indices. The words found are called ''tokens'', and so, in the context of search engine indexing and [[natural language processing]], parsing is more commonly referred to as [[Tokenization (lexical analysis)|tokenization]]. It is also sometimes called [[word boundary disambiguation]], [[Part-of-speech tagging|tagging]], [[text segmentation]], [[content analysis]], text analysis, [[text mining]], [[Agreement (linguistics)|concordance]] generation, [[speech segmentation]], [[Lexical analysis|lexing]], or [[lexical analysis]]. The terms 'indexing', 'parsing', and 'tokenization' are used interchangeably in corporate slang.\u000a\u000aNatural language processing, as of 2006, is the subject of continuous research and technological improvement. Tokenization presents many challenges in extracting the necessary information from documents for indexing to support quality searching. Tokenization for indexing involves multiple technologies, the implementation of which are commonly kept as corporate secrets.\u000a\u000a=== Challenges in natural language processing ===\u000a; Word Boundary Ambiguity : Native [[English language|English]] speakers may at first consider tokenization to be a straightforward task, but this is not the case with designing a [[multilingual]] indexer.  In digital form, the texts of other languages such as [[Chinese language|Chinese]], [[Japanese language|Japanese]] or [[Arabic language|Arabic]] represent a greater challenge, as words are not clearly delineated by [[Whitespace (computer science)|whitespace]]. The goal during tokenization is to identify words for which users will search. Language-specific logic is employed to properly identify the boundaries of words, which is often the rationale for designing a parser for each language supported (or for groups of languages with similar boundary markers and syntax).\u000a\u000a; Language Ambiguity : To assist with properly ranking matching documents, many search engines collect additional information about each word, such as its [[language]] or [[lexical category]] ([[part of speech]]). These techniques are language-dependent, as the syntax varies among languages. Documents do not always clearly identify the language of the document or represent it accurately. In tokenizing the document, some search engines attempt to automatically identify the language of the document.\u000a\u000a; Diverse File Formats : In order to correctly identify which bytes of a document represent characters, the file format must be correctly handled. Search engines which support multiple file formats must be able to correctly open and access the document and be able to tokenize the characters of the document.\u000a\u000a; Faulty Storage : The quality of the natural language data may not always be perfect.  An unspecified number of documents, particular on the Internet, do not closely obey proper file protocol.  [[Binary data|Binary]] characters may be mistakenly encoded into various parts of a document. Without recognition of these characters and appropriate handling, the index quality or indexer performance could degrade.\u000a\u000a=== Tokenization ===\u000aUnlike [[literacy|literate]] humans, computers do not understand the structure of a natural language document and cannot automatically recognize words and sentences. To a computer, a document is only a sequence of bytes. Computers do not 'know' that a space character separates words in a document. Instead, humans must program the computer to identify what constitutes an individual or distinct word, referred to as a token. Such a program is commonly called a [[tokenizer]] or [[parser]] or [[Lexical analysis|lexer]]. Many search engines, as well as other natural language processing software, incorporate [[Comparison of parser generators|specialized programs]] for parsing, such as [[YACC]] or [[Lex programming tool|Lex]].\u000a\u000aDuring tokenization, the parser identifies sequences of characters which represent words and other elements, such as punctuation, which are represented by numeric codes, some of which are non-printing control characters. The parser can also identify [[Entity extraction|entities]] such as [[email]] addresses, phone numbers, and [[Uniform Resource Locator|URL]]s. When identifying each token, several characteristics may be stored, such as the token's case (upper, lower, mixed, proper), language or encoding, lexical category (part of speech, like 'noun' or 'verb'), position, sentence number, sentence position, length, and line number.\u000a\u000a=== Language recognition ===\u000aIf the search engine supports multiple languages, a common initial step during tokenization is to identify each document's language; many of the subsequent steps are language dependent (such as [[stemming]] and [[part of speech]] tagging). [[Language identification|Language recognition]] is the process by which a computer program attempts to automatically identify, or categorize, the [[language]] of a document. Other names for language recognition include language classification, language analysis, language identification, and language tagging. Automated language recognition is the subject of ongoing research in [[natural language processing]]. Finding which language the words belongs to may involve the use of a [[language recognition chart]].\u000a\u000a=== Format analysis ===\u000aIf the search engine supports multiple [[File format|document formats]], documents must be prepared for tokenization. The challenge is that many document formats contain formatting information in addition to textual content.  For example, [[HTML]] documents contain HTML tags, which specify formatting information such as new line starts, '''bold''' emphasis, and [[font]] size or [[Font family|style]].  If the search engine were to ignore the difference between content and 'markup', extraneous information would be included in the index, leading to poor search results. Format analysis is the identification and handling of the formatting content embedded within documents which controls the way the document is rendered on a computer screen or interpreted by a software program. Format analysis is also referred to as structure analysis, format parsing, tag stripping, format stripping, text normalization, text cleaning, and text preparation. The challenge of format analysis is further complicated by the intricacies of various file formats. Certain file formats are proprietary with very little information disclosed, while others are well documented. Common, well-documented file formats that many search engines support include:\u000a\u000a* [[HTML]]\u000a* [[ASCII]] text files (a text document without specific computer readable formatting)\u000a* [[Adobe Systems|Adobe]]'s Portable Document Format ([[PDF]])\u000a* [[PostScript]] (PS)\u000a* [[LaTeX]]\u000a* [[UseNet]] netnews server formats\u000a* [[XML]] and derivatives like [[RSS]]\u000a* [[SGML]]\u000a* [[Multimedia]] [[meta data]] formats like [[ID3]]\u000a* [[Microsoft Word]]\u000a* [[Microsoft Excel]]\u000a* [[Microsoft PowerPoint]]\u000a* IBM [[Lotus Notes]]\u000aOptions for dealing with various formats include using a publicly available commercial parsing tool that is offered by the organization which developed, maintains, or owns the format, and writing a custom [[parser]].\u000a\u000aSome search engines support inspection of files that are stored in a [[Compressor (software)|compressed]] or encrypted file format.  When working with a compressed format, the indexer first decompresses the document; this step may result in one or more files, each of which must be indexed separately. Commonly supported [[list of archive formats|compressed file format]]s include:\u000a\u000a* [[ZIP (file format)|ZIP]] - Zip archive file\u000a* [[RAR]] - Roshal ARchive file\u000a* [[Cabinet (file format)|CAB]] - [[Microsoft Windows]] Cabinet File\u000a* [[Gzip]] - File compressed with gzip\u000a* [[Bzip2|BZIP]] - File compressed using bzip2\u000a* [[Tar (file format)|Tape ARchive (TAR)]], [[Unix]] archive file, not (itself) compressed\u000a* TAR.Z, TAR.GZ or TAR.BZ2 - [[Unix]] archive files compressed with Compress, GZIP or BZIP2\u000a\u000aFormat analysis can involve quality improvement methods to avoid including 'bad information' in the index.  Content can manipulate the formatting information to include additional content. Examples of abusing document formatting for [[spamdexing]]:\u000a\u000a* Including hundreds or thousands of words in a section which is hidden from view on the computer screen, but visible to the indexer, by use of formatting (e.g. hidden [[Span and div|"div" tag]] in [[HTML]], which may incorporate the use of [[CSS]] or [[JavaScript]] to do so).\u000a* Setting the foreground font color of words to the same as the background color, making words hidden on the computer screen to a person viewing the document, but not hidden to the indexer.\u000a\u000a=== Section recognition ===\u000aSome search engines incorporate section recognition, the identification of major parts of a document, prior to tokenization. Not all the documents in a corpus read like a well-written book, divided into organized chapters and pages.  Many documents on the [[Internet|web]], such as newsletters and corporate reports, contain erroneous content and side-sections which do not contain primary material (that which the document is about). For example, this article displays a side menu with links to other web pages. Some file formats, like HTML or PDF, allow for content to be displayed in columns. Even though the content is displayed, or rendered, in different areas of the view, the raw markup content may store this information sequentially. Words that appear sequentially in the raw source content are indexed sequentially, even though these sentences and paragraphs are rendered in different parts of the computer screen. If search engines index this content as if it were normal content, the quality of the index and search quality may be degraded due to the mixed content and improper word proximity. Two primary problems are noted:\u000a\u000a* Content in different sections is treated as related in the index, when in reality it is not\u000a* Organizational 'side bar' content is included in the index, but the side bar content does not contribute to the meaning of the document, and the index is filled with a poor representation of its documents.\u000a\u000aSection analysis may require the search engine to implement the rendering logic of each document, essentially an abstract representation of the actual document, and then index the representation instead. For example, some content on the Internet is rendered via JavaScript. If the search engine does not render the page and evaluate the JavaScript within the page, it would not 'see' this content in the same way and would index the document incorrectly. Given that some search engines do not bother with rendering issues, many web page designers avoid displaying content via JavaScript or use the [[Noscript tag]] to ensure that the web page is indexed properly.  At the same time, this fact can also be [[spamdexing|exploited]] to cause the search engine indexer to 'see' different content than the viewer.\u000a\u000a=== HTML Priority System ===\u000a{{Section OR|date=November 2013}}\u000aIndexing often has to recognize the [[HTML]] tags to organize priority. Indexing low priority to high margin to labels like ''strong'' and ''link'' to optimize the order of priority if those labels are at the beginning of the text could not prove to be relevant. Some indexers like [[Google]] and [[Bing]] ensure that the [[search engine]] does not take the large texts as relevant source due to[[ strong type system]] compatibility.<ref>Google Webmaster Tools, "Hypertext Markup Language 5", Conference for SEO January 2012.</ref>\u000a\u000a=== Meta tag indexing ===\u000aSpecific documents often contain embedded meta information such as author, keywords, description, and language. For HTML pages, the [[meta tag]] contains keywords which are also included in the index. Earlier Internet [[search engine technology]] would only index the keywords in the meta tags for the forward index; the full document would not be parsed. At that time full-text indexing was not as well established, nor was [[computer hardware]] able to support such technology.  The design of the HTML markup language initially included support for meta tags for the very purpose of being properly and easily indexed, without requiring tokenization.<ref>Berners-Lee, T., "Hypertext Markup Language - 2.0", RFC 1866, Network Working Group, November 1995.</ref>\u000a\u000aAs the Internet grew through the 1990s, many [[brick and mortar business|brick-and-mortar corporations]] went 'online' and established corporate websites. The keywords used to describe webpages (many of which were corporate-oriented webpages similar to product brochures) changed from descriptive to marketing-oriented keywords designed to drive sales by placing the webpage high in the search results for specific search queries. The fact that these keywords were subjectively specified was leading to [[spamdexing]], which drove many search engines to adopt full-text indexing technologies in the 1990s. Search engine designers and companies could only place so many 'marketing keywords' into the content of a webpage before draining it of all interesting and useful information.  Given that conflict of interest with the business goal of designing user-oriented websites which were 'sticky', the [[customer lifetime value]] equation was changed to incorporate more useful content into the website in hopes of retaining the visitor. In this sense, full-text indexing was more objective and increased the quality of search engine results, as it was one more step away from subjective control of search engine result placement, which in turn furthered research of full-text indexing technologies.\u000a\u000aIn [[Desktop search]], many solutions incorporate meta tags to provide a way for authors to further customize how the search engine will index content from various files that is not evident from the file content. Desktop search is more under the control of the user, while Internet search engines must focus more on the full text index.\u000a\u000a== See also ==\u000a{{div col|colwidth=25em}}\u000a* [[Compound term processing]]\u000a* [[Concordance (publishing)|Concordance]]\u000a* [[Content analysis]]\u000a* [[Controlled vocabulary]]\u000a* [[Desktop search]]\u000a* [[Documentation]]\u000a* [[Document retrieval|Document Retrieval]]\u000a* [[Full text search]]\u000a* [[Index (database)]]\u000a* [[Information extraction]]\u000a* [[Information retrieval]]\u000a* [[Key Word in Context|Keyword In Context Indexing]]\u000a* [[Latent semantic indexing]]\u000a* [[List of search engines]]\u000a* [[Natural language processing]]\u000a* [[Search engine]]\u000a* [[Selection-based search]]\u000a* [[Semantic Web]]\u000a* [[Site map]]\u000a* [[Text mining]]\u000a* [[Text retrieval|Text Retrieval]]\u000a* [[Vertical search]]\u000a* [[Web crawler]]\u000a* [[Web indexing]]\u000a* [[Website Parse Template]]\u000a*[[Windows indexing service]]<ref>Krishna Nareddy. [http://msdn2.microsoft.com/en-us/library/ms951558.aspx Indexing with Microsoft Index Server]. MSDN Library. Microsoft Corporation. January 30, 1998. Verified Dec 2006. Note that this is a commercial, external link.</ref>\u000a{{div col end}}\u000a\u000a== References ==\u000a<references/>\u000a\u000a==Further reading==\u000a*R. Bayer and E. McCreight. Organization and maintenance of large ordered indices. Acta Informatica, 173-189, 1972.\u000a*[[Donald E. Knuth]]. The art of computer programming, volume 1 (3rd ed.): fundamental algorithms, Addison Wesley Longman Publishing Co. Redwood City, CA, 1997.\u000a*[[Donald E. Knuth]]. The art of computer programming, volume 3: (2nd ed.) sorting and searching, Addison Wesley Longman Publishing Co. Redwood City, CA, 1998.\u000a*[[Gerald Salton]]. Automatic text processing, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1988.\u000a*[[Gerard Salton]]. Michael J. McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986.\u000a*[[Gerard Salton]]. Lesk, M.E.: Computer evaluation of indexing and text processing. Journal of the ACM. January 1968.\u000a*[[Gerard Salton]]. The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall Inc., Englewood Cliffs, 1971.\u000a*[[Gerard Salton]]. The Transformation, Analysis, and Retrieval of Information by Computer, Addison-Wesley, Reading, Mass., 1989.\u000a*Baeza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. Chapter 8. ACM Press 1999.\u000a*G. K. Zipf. Human Behavior and the Principle of Least Effort. Addison-Wesley, 1949.\u000a*Adelson-Velskii, G.M., Landis, E. M.: An information organization algorithm. DANSSSR, 146, 263-266 (1962).\u000a*[[Edward H. Sussenguth Jr.]], Use of tree structures for processing files, Communications of the ACM, v.6 n.5, p.&nbsp;272-279, May 1963\u000a*Harman, D.K., et al.: Inverted files. In Information Retrieval: Data Structures and Algorithms, Prentice-Hall, pp 28\u201343, 1992.\u000a*Lim, L., et al.: Characterizing Web Document Change, LNCS 2118, 133\u2013146, 2001.\u000a*Lim, L., et al.: Dynamic Maintenance of Web Indexes Using Landmarks. Proc. of the 12th W3 Conference, 2003.\u000a*Moffat, A., Zobel, J.: Self-Indexing Inverted Files for Fast Text Retrieval. ACM TIS, 349\u2013379, October 1996, Volume 14, Number 4.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]]: Data Structures and Efficient Algorithms, Springer Verlag, EATCS Monographs, 1984.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]], [[Mark Overmars|Overmars, M.H.]]: Optimal Dynamization of Decomposable Searching Problems. IPL 12, 93\u201398, 1981.\u000a*[[Kurt Mehlhorn|Mehlhorn, K.]]: Lower Bounds on the Efficiency of Transforming Static Data Structures into Dynamic Data Structures. Math. Systems Theory 15, 1\u201316, 1981.\u000a*Koster, M.: ALIWEB: Archie-Like indexing in the Web. Computer Networks and ISDN Systems, Vol. 27, No. 2 (1994) 175-182 (also see Proc. First Int'l World Wide Web Conf., Elsevier Science, Amsterdam, 1994, pp.&nbsp;175\u2013182)\u000a*[[Serge Abiteboul]] and [[Victor Vianu]]. [http://dbpubs.stanford.edu:8090/pub/showDoc.Fulltext?lang=en&doc=1996-20&format=text&compression=&name=1996-20.text Queries and Computation on the Web]. Proceedings of the International Conference on Database Theory. Delphi, Greece 1997.\u000a*Ian H Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images. New York: Van Nostrand Reinhold, 1994.\u000a*A. Emtage and P. Deutsch, "Archie--An Electronic Directory Service for the Internet." Proc. Usenix Winter 1992 Tech. Conf., Usenix Assoc., Berkeley, Calif., 1992, pp.&nbsp;93\u2013110.\u000a*M. Gray, [http://www.mit.edu/people/mkgray/net/ World Wide Web Wanderer].\u000a*D. Cutting and J. Pedersen. "Optimizations for Dynamic Inverted Index Maintenance." Proceedings of the 13th International Conference on Research and Development in Information Retrieval, pp.&nbsp;405\u2013411, September 1990.\u000a*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.\u000a\u000a{{Internet search}}\u000a\u000a{{DEFAULTSORT:Index (Search Engine)}}\u000a[[Category:Information retrieval]]\u000a[[Category:Searching]]\u000a[[Category:Indexing]]\u000a[[Category:Internet search algorithms]]
p318
sg6
S'Search engine indexing'
p319
ssI125
(dp320
g2
S'http://en.wikipedia.org/wiki/Type-1 OWA operators'
p321
sg4
VThe [[Ordered weighted averaging aggregation operator|Yager's OWA (ordered weighted averaging) operators]]<ref name="yagerOWA">{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183\u2013190|doi=10.1109/21.87068}}</ref>  have been widely used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decisin making, multi-criteria multi-expert decision making).<ref>{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}</ref><ref>{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}</ref> It is widely accepted that fuzzy sets<ref>{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338\u2013353|doi=10.1016/S0019-9958(65)90241-X}}</ref> are more suitable for representing preferences of criteria in decision making. But fuzzy sets are not crisp values, how can we aggregate fuzzy sets in OWA mechanism? \u000a\u000aThe type-1 OWA operators<ref name="fssT1OWA">{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=3281\u20133296|doi=10.1016/j.fss.2008.06.018}}</ref><ref name="kdeT1OWA">{{cite journal|last=Zhou|first=S. M.|coauthors=F. Chiclana, R. I. John and J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=1455\u20131468|doi=10.1109/TKDE.2010.191}}</ref>  have been proposed for this purpose. So the type-1 OWA operators provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\u000a\u000aFirst, there are two definitions for type-1 OWA operators, one is based on Zadeh's Extension Principle, the other is based on <math>\u005calpha</math>-cuts of fuzzy sets. The two definitions lead to equivalent results.\u000a\u000a==Definitions==\u000a\u000a'''Definition 1.<ref name="fssT1OWA" /> '''\u000aLet <math>F(X)</math> be the set of fuzzy sets with domain of discourse <math>X</math>, a type-1 OWA operator is defined as follows:\u000a\u000aGiven n linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i = 1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,1]</math>, a type-1 OWA operator is a mapping, <math>\u005cPhi</math>,\u000a\u000a:<math>\u005cPhi \u005ccolon F(X)\u005ctimes \u005ccdots \u005ctimes F(X)  \u005clongrightarrow  F(X)</math>\u000a:<math>(A^1 , \u005ccdots ,A^n)  \u005cmapsto   Y</math>\u000a\u000asuch that\u000a\u000a:<math>\u005cmu _{Y} (y) =\u005cdisplaystyle \u005csup_{\u005cdisplaystyle \u005csum_{k =1}^n \u005cbar {w}_i a_{\u005csigma (i)}  = y }\u005cleft({\u005cbegin{array}{*{1}l}\u005cmu _{W^1 } (w_1 )\u005cwedge \u005ccdots \u005cwedge \u005cmu_{W^n } (w_n )\u005cwedge \u005cmu _{A^1 } (a_1 )\u005cwedge \u005ccdots \u005cwedge \u005cmu _{A^n } (a_n )\u005cend{array}}\u005cright)</math>\u000a\u000awhere <math>\u005cbar {w}_i = \u005cfrac{w_i }{\u005csum_{i = 1}^n {w_i } }</math>,and <math>\u005csigma \u005ccolon \u005c{1, \u005ccdots ,n\u005c} \u005clongrightarrow \u005c{1, \u005ccdots ,n\u005c}</math> is a permutation function such that <math>a_{\u005csigma (i)} \u005cgeq a_{\u005csigma (i + 1)},\u005c \u005cforall i = 1, \u005ccdots ,n - 1</math>, i.e., <math>a_{\u005csigma(i)} </math> is the <math>i</math>th highest element in the set <math>\u005cleft\u005c{ {a_1 , \u005ccdots ,a_n } \u005cright\u005c}</math>.\u000a\u000a'''Definition 2.<ref name="kdeT1OWA" /> '''\u000a\u000aThe definition below is based on the alpha-cuts of fuzzy sets:\u000a\u000aGiven the n linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\u005c;\u005c;1]</math>, then for each <math>\u005calpha \u005cin [0,\u005c;1]</math>, an <math>\u005calpha </math>-level type-1 OWA operator with <math>\u005calpha </math>-level sets <math>\u005cleft\u005c{ {W_\u005calpha ^i } \u005cright\u005c}_{i = 1}^n </math> to aggregate the <math>\u005calpha </math>-cuts of fuzzy sets <math>\u005cleft\u005c{ {A^i} \u005cright\u005c}_{i =1}^n </math> is given as\u000a\u000a: <math>\u000a\u005cPhi_\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005cldots ,A_\u005calpha ^n } \u005cright) =\u005cleft\u005c{ {\u005cfrac{\u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} } }{\u005csum\u005climits_{i = 1}^n {w_i } }\u005cleft| {w_i \u005cin W_\u005calpha ^i ,\u005c;a_i } \u005cright. \u005cin A_\u005calpha ^i ,\u005c;i = 1, \u005cldots ,n} \u005cright\u005c}</math>\u000a\u000awhere  <math>W_\u005calpha ^i= \u005c{w| \u005cmu_{W_i }(w) \u005cgeq \u005calpha \u005c}, A_\u005calpha ^i=\u005c{ x| \u005cmu _{A_i }(x)\u005cgeq \u005calpha \u005c}</math>, and <math>\u005csigma :\u005c{\u005c;1, \u005ccdots ,n\u005c;\u005c} \u005cto \u005c{\u005c;1, \u005ccdots ,n\u005c;\u005c}</math> is a permutation function such that <math>a_{\u005csigma (i)} \u005cge a_{\u005csigma (i + 1)} ,\u005c;\u005cforall \u005c;i = 1, \u005ccdots ,n - 1</math>, i.e., <math>a_{\u005csigma (i)} </math> is the <math>i</math>th largest\u000aelement in the set <math>\u005cleft\u005c{ {a_1 , \u005ccdots ,a_n } \u005cright\u005c}</math>.\u000a\u000a== Representation theorem of Type-1 OWA operators<ref name="kdeT1OWA" />==\u000a\u000aGiven the ''n'' linguistic weights <math>\u005cleft\u005c{ {W^i} \u005cright\u005c}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\u005c;\u005c;1]</math>, and the fuzzy sets <math>A^1, \u005ccdots ,A^n</math>, then we have that<ref name="kdeT1OWA" />\u000a:<math>Y=G</math>\u000a\u000awhere <math>Y</math> is the aggregation result obtained by Definition 1, and <math>G</math> is the result obtained by in Definition 2.\u000a\u000a==Programming problems for Type-1 OWA operators==\u000a\u000aAccording to the '''''Representation Theorem of Type-1 OWA Operators''''',a general type-1 OWA operator can be decomposed into a series of <math>\u005calpha</math>-level type-1 OWA operators. In practice, these series of  <math>\u005calpha</math>-level type-1 OWA operators are used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals <math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots ,A_\u005calpha ^n } \u005cright)</math>. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:\u000a\u000a:<math>\u005cmu _{G} (x) = \u005cmathop \u005cvee \u005climits_{\u005calpha :x \u005cin \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots\u000a,A_\u005calpha ^n } \u005cright)_\u005calpha } \u005calpha </math>\u000a\u000aFor the left end-points, we need to solve the following programming problem:\u000a:<math> \u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots ,A_\u005calpha ^n } \u005cright)_{-} = \u005cmathop {\u005cmin }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i = 1}^n {w_i } } </math>\u000a\u000awhile for the right end-points, we need to solve the following programming problem:\u000a:<math>\u005cPhi _\u005calpha \u005cleft( {A_\u005calpha ^1 , \u005ccdots , A_\u005calpha ^n } \u005cright)_{+} = \u005cmathop {\u005cmax }\u005climits_{\u005cbegin{array}{l} W_{\u005calpha - }^i \u005cle w_i \u005cle W_{\u005calpha + }^i  A_{\u005calpha - }^i \u005cle a_i \u005cle A_{\u005calpha + }^i  \u005cend{array}} \u005csum\u005climits_{i = 1}^n {w_i a_{\u005csigma (i)} / \u005csum\u005climits_{i =\u000a1}^n {w_i } } </math>\u000a\u000aA fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.<ref name="kdeT1OWA" />\u000a\u000a== Alpha-level approach to Type-1 OWA operation<ref name="kdeT1OWA" />==\u000a* '''Step 1'''.To set up the <math>\u005calpha </math>- level resolution in [0, 1].\u000a* '''Step 2'''. For each <math>\u005calpha \u005cin [0,1]</math>,\u000a''Step 2.1.'' To calculate <math>\u005crho _{\u005calpha +} ^{i_0^\u005cast } </math>\u000a# Let <math>i_0 = 1</math>;\u000a# If <math>\u005crho _{\u005calpha +} ^{i_0 } \u005cge A_{\u005calpha + }^{\u005csigma (i_0 )} </math>, stop, <math>\u005crho _{\u005calpha +} ^{i_0 } </math> is the solution; otherwise go to ''Step 2.1-3''.\u000a# <math>i_0 \u005cleftarrow i_0 + 1</math>, go to ''Step 2.1-2''.\u000a\u000a''Step 2.2.'' To calculate<math>\u005crho _{\u005calpha -} ^{i_0^\u005cast } </math>\u000a# Let <math>i_0 = 1</math>;\u000a# If <math>\u005crho _{\u005calpha -} ^{i_0 } \u005cge A_{\u005calpha - }^{\u005csigma (i_0 )} </math>, stop, <math>\u005crho _{\u005calpha -} ^{i_0 } </math> is the solution; otherwise go to ''Step 2.2-3.''\u000a#<math>i_0 \u005cleftarrow i_0 + 1</math>, go to step ''Step 2.2-2.''\u000a\u000a'''Step 3.'''To construct the aggregation resulting fuzzy set <math>G</math> based on all the available intervals <math>\u005cleft[ {\u005crho _{\u005calpha -} ^{i_0^\u005cast } ,\u005c;\u005crho _{\u005calpha +} ^{i_0^\u005cast } } \u005cright]</math>: \u000a\u000a:<math>\u005cmu _{G} (x) = \u005cmathop \u005cvee \u005climits_{\u005calpha :x \u005cin \u005cleft[ {\u005crho _{\u005calpha -} ^{i_0^\u005cast } ,\u005c;\u005crho _{\u005calpha +} ^{i_0^\u005cast } } \u005cright]} \u005calpha </math>\u000a\u000a==Special cases of Type-1 OWA operators==\u000a* Any OWA operators, like maximum, minimum, mean operators;<ref name="yagerOWA" />\u000a* Join operators of (type-1) fuzzy sets,<ref name="MT">{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=312\u201340|doi=10.1016/s0019-9958(76)80011-3}}</ref><ref name="zadehJ">{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199\u2013249|doi=10.1016/0020-0255(75)90036-5}}</ref> i.e., fuzzy maximum operators;\u000a* Meet operators of (type-1) fuzzy sets,<ref name="MT"/><ref name="zadehJ"/> i.e., fuzzy minimum operators;\u000a* Join-like operators of (type-1) fuzzy sets;<ref name="kdeT1OWA"/><ref name="bookT1OWA">{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91\u2013109|doi=10.1007/978-3-642-17910-5_5}}</ref>\u000a* Meet-like operators of (type-1) fuzzy sets.<ref name="kdeT1OWA"/><ref name="bookT1OWA"/>\u000a\u000a==Generalizations==\u000aType-2 OWA operators<ref>{{cite journal|last=Zhou|first=S.M.|coauthors=R. I. John, F. Chiclana and J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540\u2013558|doi=10.1002/int.20420}}</ref> have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.\u000a\u000a== References ==\u000a{{reflist}}\u000a\u000a[[Category:Artificial intelligence]]\u000a[[Category:Logic in computer science]]\u000a[[Category:Fuzzy logic]]\u000a[[Category:Information retrieval]]
p322
sg6
S'Type-1 OWA operators'
p323
ss.